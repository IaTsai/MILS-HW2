#!/usr/bin/env python3
"""
å®Œæ•´çµ±ä¸€å¤šä»»å‹™æ¨¡å‹æ¸¬è©¦è…³æœ¬
æ¸¬è©¦éª¨å¹¹ç¶²è·¯ + é ¸éƒ¨ç¶²è·¯ + å¤šä»»å‹™é ­éƒ¨çš„å®Œæ•´æ¨¡å‹
"""
import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.backbone import create_backbone
from src.models.neck import create_neck
from src.models.head import create_multitask_head
from src.models.unified_model import create_unified_model, UnifiedMultiTaskModel


def test_model_components():
    """æ¸¬è©¦æ¨¡å‹å„çµ„ä»¶åƒæ•¸é‡"""
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹å„çµ„ä»¶åƒæ•¸é‡...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. æ¸¬è©¦éª¨å¹¹ç¶²è·¯
    backbone = create_backbone('mobilenetv3_small', pretrained=False)
    backbone_params = backbone.get_parameter_count()
    print(f"ğŸ”§ éª¨å¹¹ç¶²è·¯: {backbone_params['total_parameters']:,} åƒæ•¸ ({backbone_params['total_parameters']/1e6:.2f}M)")
    
    # 2. æ¸¬è©¦é ¸éƒ¨ç¶²è·¯
    neck = create_neck('fpn', in_channels_list=[16, 24, 48, 96], out_channels=128)
    neck_params = neck.get_parameter_count()
    print(f"ğŸ”— é ¸éƒ¨ç¶²è·¯: {neck_params['total_parameters']:,} åƒæ•¸ ({neck_params['total_parameters']/1e6:.2f}M)")
    
    # 3. æ¸¬è©¦é ­éƒ¨ç¶²è·¯
    head = create_multitask_head(
        'unified',
        in_channels=128,
        num_det_classes=10,
        num_seg_classes=21,
        num_cls_classes=10,
        shared_channels=256
    )
    head_params = head.get_parameter_count()
    print(f"ğŸ¯ é ­éƒ¨ç¶²è·¯: {head_params['total_parameters']:,} åƒæ•¸ ({head_params['total_parameters']/1e6:.2f}M)")
    
    # ç¸½è¨ˆ
    total_estimated = backbone_params['total_parameters'] + neck_params['total_parameters'] + head_params['total_parameters']
    print(f"ğŸ“Š é ä¼°ç¸½è¨ˆ: {total_estimated:,} åƒæ•¸ ({total_estimated/1e6:.2f}M)")
    
    return {
        'backbone': backbone_params['total_parameters'],
        'neck': neck_params['total_parameters'],
        'head': head_params['total_parameters'],
        'total_estimated': total_estimated
    }


def test_unified_model_creation():
    """æ¸¬è©¦çµ±ä¸€æ¨¡å‹å‰µå»º"""
    print("\nğŸ—ï¸ æ¸¬è©¦çµ±ä¸€æ¨¡å‹å‰µå»º...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¸¬è©¦ä¸åŒé…ç½®
    configs = ['default', 'lightweight']
    
    results = {}
    
    for config_name in configs:
        print(f"\nğŸ“‹ æ¸¬è©¦é…ç½®: {config_name}")
        
        try:
            model = create_unified_model(config_name)
            model = model.to(device)
            
            # ç²å–åƒæ•¸çµ±è¨ˆ
            param_info = model.get_total_parameters()
            model_info = model.get_model_info()
            
            print(f"  âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")
            print(f"  ğŸ“Š ç¸½åƒæ•¸: {param_info['total_parameters']:,} ({param_info['total_parameters']/1e6:.2f}M)")
            print(f"  ğŸ¯ é ç®—ä½¿ç”¨ç‡: {model_info['parameter_budget']['utilization_rate']:.1%}")
            
            # æª¢æŸ¥åƒæ•¸é ç®—
            budget_ok = param_info['total_parameters'] <= 8_000_000
            print(f"  {'âœ…' if budget_ok else 'âŒ'} åƒæ•¸é ç®—: {budget_ok}")
            
            results[config_name] = {
                'success': True,
                'parameters': param_info['total_parameters'],
                'budget_ok': budget_ok,
                'model_info': model_info
            }
            
        except Exception as e:
            print(f"  âŒ æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
            results[config_name] = {
                'success': False,
                'error': str(e)
            }
    
    return results


def test_model_forward_pass():
    """æ¸¬è©¦æ¨¡å‹å‰å‘å‚³æ’­"""
    print("\nğŸš€ æ¸¬è©¦æ¨¡å‹å‰å‘å‚³æ’­...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºæ¨¡å‹
    model = create_unified_model('default')
    model = model.to(device)
    model.eval()
    
    # æ¸¬è©¦ä¸åŒè¼¸å…¥å°ºå¯¸
    test_cases = [
        (1, 224, 224),
        (2, 320, 320),
        (4, 512, 512)
    ]
    
    results = {}
    
    for batch_size, height, width in test_cases:
        print(f"\nğŸ§ª æ¸¬è©¦è¼¸å…¥: ({batch_size}, 3, {height}, {width})")
        
        try:
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            test_input = torch.randn(batch_size, 3, height, width).to(device)
            
            # æ¸¬è©¦å®Œæ•´æ¨ç†
            with torch.no_grad():
                start_time = time.time()
                outputs = model(test_input, task_type='all')
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                end_time = time.time()
            
            inference_time = (end_time - start_time) * 1000  # ms
            
            print(f"  âœ… å‰å‘å‚³æ’­æˆåŠŸ")
            print(f"  â±ï¸ æ¨ç†æ™‚é–“: {inference_time:.2f}ms")
            print(f"  ğŸ” è¼¸å‡ºå½¢ç‹€:")
            
            for task, output in outputs.items():
                print(f"    {task}: {output.shape}")
            
            # æ¸¬è©¦å–®ä»»å‹™æ¨ç†
            single_task_times = {}
            for task in ['detection', 'segmentation', 'classification']:
                with torch.no_grad():
                    start_time = time.time()
                    single_output = model(test_input, task_type=task)
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    end_time = time.time()
                
                single_task_times[task] = (end_time - start_time) * 1000
            
            print(f"  ğŸ¯ å–®ä»»å‹™æ¨ç†æ™‚é–“:")
            for task, task_time in single_task_times.items():
                print(f"    {task}: {task_time:.2f}ms")
            
            results[f"{batch_size}x{height}x{width}"] = {
                'success': True,
                'inference_time_ms': inference_time,
                'single_task_times': single_task_times,
                'output_shapes': {task: list(output.shape) for task, output in outputs.items()},
                'meets_latency': inference_time < 150.0
            }
            
        except Exception as e:
            print(f"  âŒ å‰å‘å‚³æ’­å¤±æ•—: {e}")
            results[f"{batch_size}x{height}x{width}"] = {
                'success': False,
                'error': str(e)
            }
    
    return results


def test_model_benchmark():
    """æ¸¬è©¦æ¨¡å‹æ€§èƒ½åŸºæº–"""
    print("\nâ±ï¸ åŸ·è¡Œæ¨¡å‹æ€§èƒ½åŸºæº–æ¸¬è©¦...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºæ¨¡å‹
    model = create_unified_model('default')
    model = model.to(device)
    
    # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
    try:
        benchmark_results = model.inference_benchmark(
            input_size=(512, 512),
            batch_size=1,
            num_warmup=10,
            num_runs=50
        )
        
        print("âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦æˆåŠŸï¼")
        print("ğŸ“ˆ æ¸¬è©¦çµæœ:")
        
        for key, value in benchmark_results.items():
            if isinstance(value, (int, float)):
                if 'time' in key or 'fps' in key:
                    print(f"  {key}: {value:.3f}")
                elif 'memory' in key:
                    print(f"  {key}: {value:.2f}")
                else:
                    print(f"  {key}: {value}")
            else:
                print(f"  {key}: {value}")
        
        return benchmark_results
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        return {'success': False, 'error': str(e)}


def test_model_save_load():
    """æ¸¬è©¦æ¨¡å‹ä¿å­˜å’ŒåŠ è¼‰"""
    print("\nğŸ’¾ æ¸¬è©¦æ¨¡å‹ä¿å­˜å’ŒåŠ è¼‰...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        # å‰µå»ºåŸå§‹æ¨¡å‹
        original_model = create_unified_model('default')
        original_model = original_model.to(device)
        
        # ä¿å­˜æ¨¡å‹
        save_path = '/tmp/test_unified_model.pth'
        original_model.save_model(save_path, epoch=0, test_mode=True)
        
        # åŠ è¼‰æ¨¡å‹
        loaded_model, loaded_info = UnifiedMultiTaskModel.load_model(save_path, device=device)
        
        # é©—è­‰åŠ è¼‰çš„æ¨¡å‹
        test_input = torch.randn(1, 3, 224, 224).to(device)
        
        with torch.no_grad():
            original_output = original_model(test_input)
            loaded_output = loaded_model(test_input)
        
        # æª¢æŸ¥è¼¸å‡ºæ˜¯å¦ä¸€è‡´
        outputs_match = True
        for task in original_output:
            if not torch.allclose(original_output[task], loaded_output[task], atol=1e-5):
                outputs_match = False
                break
        
        print(f"âœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è¼‰æˆåŠŸï¼")
        print(f"ğŸ” è¼¸å‡ºä¸€è‡´æ€§: {'âœ… ä¸€è‡´' if outputs_match else 'âŒ ä¸ä¸€è‡´'}")
        
        # æ¸…ç†
        os.remove(save_path)
        
        return {'success': True, 'outputs_match': outputs_match}
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜/åŠ è¼‰å¤±æ•—: {e}")
        return {'success': False, 'error': str(e)}


def run_comprehensive_test():
    """é‹è¡Œå…¨é¢æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹çµ±ä¸€å¤šä»»å‹™æ¨¡å‹å…¨é¢æ¸¬è©¦...")
    print("=" * 70)
    
    results = {}
    
    # 1. çµ„ä»¶åƒæ•¸æ¸¬è©¦
    try:
        component_results = test_model_components()
        results['components'] = component_results
    except Exception as e:
        print(f"âŒ çµ„ä»¶æ¸¬è©¦å¤±æ•—: {e}")
        results['components'] = {'error': str(e)}
    
    # 2. æ¨¡å‹å‰µå»ºæ¸¬è©¦
    try:
        creation_results = test_unified_model_creation()
        results['creation'] = creation_results
    except Exception as e:
        print(f"âŒ æ¨¡å‹å‰µå»ºæ¸¬è©¦å¤±æ•—: {e}")
        results['creation'] = {'error': str(e)}
    
    # 3. å‰å‘å‚³æ’­æ¸¬è©¦
    try:
        forward_results = test_model_forward_pass()
        results['forward'] = forward_results
    except Exception as e:
        print(f"âŒ å‰å‘å‚³æ’­æ¸¬è©¦å¤±æ•—: {e}")
        results['forward'] = {'error': str(e)}
    
    # 4. æ€§èƒ½åŸºæº–æ¸¬è©¦
    try:
        benchmark_results = test_model_benchmark()
        results['benchmark'] = benchmark_results
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        results['benchmark'] = {'error': str(e)}
    
    # 5. ä¿å­˜/åŠ è¼‰æ¸¬è©¦
    try:
        save_load_results = test_model_save_load()
        results['save_load'] = save_load_results
    except Exception as e:
        print(f"âŒ ä¿å­˜/åŠ è¼‰æ¸¬è©¦å¤±æ•—: {e}")
        results['save_load'] = {'error': str(e)}
    
    return results


def print_final_summary(results):
    """æ‰“å°æœ€çµ‚æ¸¬è©¦ç¸½çµ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ çµ±ä¸€å¤šä»»å‹™æ¨¡å‹æ¸¬è©¦ç¸½çµ")
    print("=" * 70)
    
    # æˆåŠŸç‡çµ±è¨ˆ
    total_tests = len(results)
    successful_tests = sum(1 for r in results.values() if isinstance(r, dict) and 'error' not in r)
    
    print(f"âœ… æ¸¬è©¦é€šé: {successful_tests}/{total_tests}")
    
    # é—œéµæŒ‡æ¨™ç¸½çµ
    if 'creation' in results and 'default' in results['creation']:
        default_info = results['creation']['default']
        if 'model_info' in default_info:
            model_info = default_info['model_info']
            params = model_info['parameters']
            budget = model_info['parameter_budget']
            
            print(f"\nğŸ—ï¸ é»˜èªé…ç½®æ¨¡å‹:")
            print(f"  ç¸½åƒæ•¸: {params['total_parameters']:,} ({params['total_parameters']/1e6:.2f}M)")
            print(f"  é ç®—ä½¿ç”¨: {budget['utilization_rate']:.1%}")
            print(f"  é ç®—æª¢æŸ¥: {'âœ… é€šé' if budget['used_parameters'] <= budget['total_budget'] else 'âŒ è¶…å‡º'}")
    
    # æ€§èƒ½æŒ‡æ¨™
    if 'benchmark' in results and 'avg_inference_time_ms' in results['benchmark']:
        benchmark = results['benchmark']
        print(f"\nâ±ï¸ æ€§èƒ½æŒ‡æ¨™ (512x512):")
        print(f"  æ¨ç†æ™‚é–“: {benchmark['avg_inference_time_ms']:.2f}ms")
        print(f"  FPS: {benchmark['fps']:.1f}")
        print(f"  å»¶é²è¦æ±‚: {'âœ… æ»¿è¶³' if benchmark['meets_latency_requirement'] else 'âŒ ä¸æ»¿è¶³'} (<150ms)")
        
        if 'avg_memory_usage_mb' in benchmark:
            print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {benchmark['avg_memory_usage_mb']:.1f}MB")
    
    # è¼¸å‡ºæ ¼å¼é©—è­‰
    if 'forward' in results:
        forward_results = results['forward']
        successful_forward = sum(1 for r in forward_results.values() if r.get('success', False))
        print(f"\nğŸ” å‰å‘å‚³æ’­æ¸¬è©¦: {successful_forward}/{len(forward_results)} æˆåŠŸ")
        
        # æª¢æŸ¥è¼¸å‡ºæ ¼å¼
        for size, result in forward_results.items():
            if result.get('success', False):
                shapes = result['output_shapes']
                print(f"  {size}:")
                for task, shape in shapes.items():
                    print(f"    {task}: {shape}")
                break
    
    # æœ€çµ‚çµè«–
    print(f"\nğŸ¯ æœ€çµ‚çµè«–:")
    
    # æª¢æŸ¥é—œéµè¦æ±‚
    requirements_met = []
    
    # 1. åƒæ•¸é‡æª¢æŸ¥
    param_ok = False
    if 'creation' in results and 'default' in results['creation']:
        param_ok = results['creation']['default'].get('budget_ok', False)
    requirements_met.append(('åƒæ•¸é‡ < 8M', param_ok))
    
    # 2. æ¨ç†é€Ÿåº¦æª¢æŸ¥
    speed_ok = False
    if 'benchmark' in results:
        speed_ok = results['benchmark'].get('meets_latency_requirement', False)
    requirements_met.append(('æ¨ç†é€Ÿåº¦ < 150ms', speed_ok))
    
    # 3. å‰å‘å‚³æ’­æª¢æŸ¥
    forward_ok = False
    if 'forward' in results:
        forward_ok = any(r.get('success', False) for r in results['forward'].values())
    requirements_met.append(('å‰å‘å‚³æ’­æ­£å¸¸', forward_ok))
    
    # 4. æ¨¡å‹ä¿å­˜/åŠ è¼‰æª¢æŸ¥
    save_load_ok = False
    if 'save_load' in results:
        save_load_ok = results['save_load'].get('success', False)
    requirements_met.append(('æ¨¡å‹ä¿å­˜/åŠ è¼‰', save_load_ok))
    
    for requirement, met in requirements_met:
        print(f"  {'âœ…' if met else 'âŒ'} {requirement}")
    
    all_requirements_met = all(met for _, met in requirements_met)
    
    if all_requirements_met:
        print(f"\nğŸ‰ æ‰€æœ‰è¦æ±‚éƒ½å·²æ»¿è¶³ï¼æ¨¡å‹æº–å‚™å°±ç·’ã€‚")
        print(f"âœ… Phase 2 å®Œæˆï¼æº–å‚™é€²å…¥ Phase 3: é˜²éºå¿˜ç­–ç•¥å¯¦ç¾")
        return True
    else:
        failed_count = sum(1 for _, met in requirements_met if not met)
        print(f"\nâš ï¸ {failed_count} å€‹è¦æ±‚æœªæ»¿è¶³ï¼Œéœ€è¦ä¿®å¾©ã€‚")
        return False


if __name__ == "__main__":
    print("ğŸ—ï¸ çµ±ä¸€å¤šä»»å‹™æ¨¡å‹æ¸¬è©¦è…³æœ¬")
    print(f"ğŸ“… æ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    print("\n" + "=" * 70)
    
    # é‹è¡Œæ¸¬è©¦
    test_results = run_comprehensive_test()
    
    # æ‰“å°ç¸½çµ
    success = print_final_summary(test_results)
    
    # é€€å‡ºç¢¼
    sys.exit(0 if success else 1)