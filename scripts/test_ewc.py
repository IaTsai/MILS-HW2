#!/usr/bin/env python3
"""
EWC (Elastic Weight Consolidation) æ¸¬è©¦è…³æœ¬
é©—è­‰ EWC ç®—æ³•çš„åŠŸèƒ½ã€æ•ˆç‡å’Œé˜²éºå¿˜æ•ˆæœ
"""
import os
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.ewc import EWC, ewc_loss, create_ewc_handler
from src.models.unified_model import create_unified_model


class SimpleModel(nn.Module):
    """ç°¡å–®æ¸¬è©¦æ¨¡å‹"""
    def __init__(self, input_dim=10, hidden_dim=50, output_dim=5):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x


def create_synthetic_data(num_samples=1000, input_dim=10, num_classes=5, task_shift=0.0):
    """
    å‰µå»ºåˆæˆæ•¸æ“šé›†
    
    Args:
        num_samples: æ¨£æœ¬æ•¸é‡
        input_dim: è¼¸å…¥ç¶­åº¦
        num_classes: é¡åˆ¥æ•¸é‡
        task_shift: ä»»å‹™åç§» (ç”¨æ–¼æ¨¡æ“¬ä»»å‹™å·®ç•°)
    
    Returns:
        dataset: æ•¸æ“šé›†
    """
    # ç”Ÿæˆéš¨æ©Ÿæ•¸æ“š
    data = torch.randn(num_samples, input_dim)
    
    # æ·»åŠ ä»»å‹™ç‰¹å®šçš„åç§»
    if task_shift != 0.0:
        data += task_shift
    
    # ç”Ÿæˆæ¨™ç±¤ (åŸºæ–¼æ•¸æ“šçš„ç·šæ€§çµ„åˆ)
    weights = torch.randn(input_dim)
    linear_combination = torch.matmul(data, weights)
    labels = (linear_combination > 0).long()
    
    # æ“´å±•åˆ°å¤šé¡åˆ¥
    if num_classes > 2:
        # ä½¿ç”¨é‡åŒ–ä¾†å‰µå»ºå¤šé¡åˆ¥
        quantiles = torch.quantile(linear_combination, torch.linspace(0, 1, num_classes + 1))
        labels = torch.bucketize(linear_combination, quantiles[1:-1])
    
    return TensorDataset(data, labels)


def test_ewc_basic_functionality():
    """æ¸¬è©¦ EWC åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ EWC åŸºæœ¬åŠŸèƒ½...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºæ¨¡å‹
    model = SimpleModel().to(device)
    
    # å‰µå»º EWC è™•ç†å™¨
    ewc = create_ewc_handler(model, importance=1000.0)
    
    # å‰µå»ºæ•¸æ“š
    dataset = create_synthetic_data(num_samples=200)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    print("âœ… EWC è™•ç†å™¨å‰µå»ºæˆåŠŸ")
    
    # æ¸¬è©¦ Fisher çŸ©é™£è¨ˆç®—
    print("  ğŸ§® æ¸¬è©¦ Fisher çŸ©é™£è¨ˆç®—...")
    fisher_matrix = ewc.compute_fisher_matrix(dataloader, verbose=False)
    
    assert len(fisher_matrix) > 0, "Fisher çŸ©é™£ç‚ºç©º"
    assert all(isinstance(f, torch.Tensor) for f in fisher_matrix.values()), "Fisher çŸ©é™£é¡å‹éŒ¯èª¤"
    print("  âœ… Fisher çŸ©é™£è¨ˆç®—æˆåŠŸ")
    
    # æ¸¬è©¦åƒæ•¸å­˜å„²
    print("  ğŸ“¥ æ¸¬è©¦åƒæ•¸å­˜å„²...")
    ewc.store_optimal_params(task_id=0)
    assert 0 in ewc.optimal_params, "åƒæ•¸å­˜å„²å¤±æ•—"
    print("  âœ… åƒæ•¸å­˜å„²æˆåŠŸ")
    
    # æ¸¬è©¦æ‡²ç½°é …è¨ˆç®—
    print("  ğŸ’° æ¸¬è©¦æ‡²ç½°é …è¨ˆç®—...")
    penalty = ewc.penalty()
    assert isinstance(penalty, torch.Tensor), "æ‡²ç½°é …é¡å‹éŒ¯èª¤"
    assert penalty.requires_grad, "æ‡²ç½°é …ä¸æ”¯æŒæ¢¯åº¦"
    print(f"  âœ… æ‡²ç½°é …è¨ˆç®—æˆåŠŸ: {penalty.item():.6f}")
    
    # æ¸¬è©¦ EWC æå¤±
    print("  ğŸ”— æ¸¬è©¦ EWC æå¤±æ•´åˆ...")
    
    # ç¨å¾®ä¿®æ”¹æ¨¡å‹åƒæ•¸ä¾†ç”¢ç”Ÿéé›¶æ‡²ç½°é …
    with torch.no_grad():
        for param in model.parameters():
            param.data += 0.1 * torch.randn_like(param.data)
    
    dummy_input = torch.randn(10, 10).to(device)
    output = model(dummy_input)
    dummy_labels = torch.randint(0, 5, (10,)).to(device)
    base_loss = F.cross_entropy(output, dummy_labels)
    
    total_loss, ewc_penalty = ewc_loss(base_loss, ewc)
    
    # æª¢æŸ¥æ‡²ç½°é …æ˜¯å¦éé›¶ä¸”ç¸½æå¤±æ­£ç¢º
    assert ewc_penalty.item() > 0, f"EWC æ‡²ç½°é …æ‡‰è©²å¤§æ–¼0ï¼Œå¯¦éš›å€¼: {ewc_penalty.item()}"
    assert torch.allclose(total_loss, base_loss + ewc_penalty, atol=1e-6), "ç¸½æå¤±è¨ˆç®—éŒ¯èª¤"
    print(f"  âœ… EWC æå¤±æ•´åˆæˆåŠŸ: åŸºç¤={base_loss.item():.4f}, EWC={ewc_penalty.item():.4f}, ç¸½è¨ˆ={total_loss.item():.4f}")
    
    return True


def test_ewc_multitask_integration():
    """æ¸¬è©¦ EWC èˆ‡å¤šä»»å‹™æ¨¡å‹çš„æ•´åˆ"""
    print("\nğŸ§ª æ¸¬è©¦ EWC èˆ‡å¤šä»»å‹™æ¨¡å‹æ•´åˆ...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºçµ±ä¸€å¤šä»»å‹™æ¨¡å‹
    model = create_unified_model('lightweight')  # ä½¿ç”¨è¼•é‡åŒ–é…ç½®ç¯€çœæ™‚é–“
    model = model.to(device)
    
    # å‰µå»º EWC è™•ç†å™¨
    ewc = create_ewc_handler(model, importance=500.0)
    
    # å‰µå»ºæ¨¡æ“¬æ•¸æ“š (æ¨¡æ“¬çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨æ ¼å¼)
    def create_multitask_data(num_samples=50):
        images = torch.randn(num_samples, 3, 224, 224)
        task_types = ['classification'] * num_samples  # ç°¡åŒ–ç‚ºåˆ†é¡ä»»å‹™
        targets = [torch.randint(0, 10, (1,)) for _ in range(num_samples)]
        
        batch_data = []
        for i in range(0, num_samples, 10):  # æ‰¹æ¬¡å¤§å° 10
            batch_images = images[i:i+10]
            batch_targets = targets[i:i+10]
            batch_data.append({
                'images': batch_images,
                'task_types': task_types[i:i+10],
                'targets': batch_targets
            })
        
        return batch_data
    
    # å‰µå»ºæ•¸æ“š
    multitask_data = create_multitask_data(num_samples=30)  # æ¸›å°‘æ¨£æœ¬æ•¸ä»¥åŠ å¿«æ¸¬è©¦
    
    print("  ğŸ§® è¨ˆç®—å¤šä»»å‹™æ¨¡å‹çš„ Fisher çŸ©é™£...")
    
    # æ‰‹å‹•è™•ç†æ‰¹æ¬¡æ•¸æ“š
    sample_count = 0
    fisher_matrix = {}
    
    # åˆå§‹åŒ– Fisher çŸ©é™£
    for name, param in model.named_parameters():
        if param.requires_grad:
            fisher_matrix[name] = torch.zeros_like(param.data)
    
    model.eval()
    for batch in multitask_data:
        images = batch['images'].to(device)
        batch_size = images.size(0)
        
        # å‰å‘å‚³æ’­
        outputs = model(images, task_type='classification')
        
        # è¨ˆç®— Fisher ä¿¡æ¯ (ç°¡åŒ–ç‰ˆæœ¬)
        if 'classification' in outputs:
            class_output = outputs['classification']
            log_prob = F.log_softmax(class_output, dim=1)
            prob = F.softmax(class_output, dim=1)
            
            for class_idx in range(prob.size(1)):
                class_prob = prob[:, class_idx].sum()
                if class_prob.item() > 1e-8:
                    class_log_prob = log_prob[:, class_idx].sum()
                    
                    # è¨ˆç®—æ¢¯åº¦
                    model.zero_grad()
                    class_log_prob.backward(retain_graph=True)
                    
                    # ç´¯ç© Fisher ä¿¡æ¯
                    for name, param in model.named_parameters():
                        if param.requires_grad and param.grad is not None:
                            fisher_matrix[name] += class_prob.item() * (param.grad.data ** 2)
        
        sample_count += batch_size
        if sample_count >= 30:  # é™åˆ¶æ¨£æœ¬æ•¸
            break
    
    # æ­£è¦åŒ–
    for name in fisher_matrix:
        fisher_matrix[name] = fisher_matrix[name] / sample_count
    
    # æ‰‹å‹•è¨­ç½® Fisher çŸ©é™£
    ewc.fisher_matrices[0] = fisher_matrix
    ewc.store_optimal_params(task_id=0)
    
    print("  âœ… å¤šä»»å‹™æ¨¡å‹ Fisher çŸ©é™£è¨ˆç®—å®Œæˆ")
    
    # æ¸¬è©¦æ‡²ç½°é …è¨ˆç®—
    penalty = ewc.penalty()
    print(f"  ğŸ’° å¤šä»»å‹™æ¨¡å‹ EWC æ‡²ç½°é …: {penalty.item():.6f}")
    
    # æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨
    memory_info = ewc.get_memory_usage()
    print(f"  ğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨: {memory_info['total_mb']:.2f} MB")
    
    return True


def test_ewc_efficiency():
    """æ¸¬è©¦ EWC è¨ˆç®—æ•ˆç‡"""
    print("\nâ±ï¸ æ¸¬è©¦ EWC è¨ˆç®—æ•ˆç‡...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¸¬è©¦ä¸åŒæ¨¡å‹å¤§å°
    model_configs = [
        {'input_dim': 10, 'hidden_dim': 50, 'output_dim': 5, 'name': 'Small'},
        {'input_dim': 50, 'hidden_dim': 200, 'output_dim': 10, 'name': 'Medium'},
        {'input_dim': 100, 'hidden_dim': 500, 'output_dim': 20, 'name': 'Large'}
    ]
    
    results = {}
    
    for config in model_configs:
        print(f"  ğŸ“Š æ¸¬è©¦ {config['name']} æ¨¡å‹...")
        
        # å‰µå»ºæ¨¡å‹
        model = SimpleModel(
            input_dim=config['input_dim'],
            hidden_dim=config['hidden_dim'],
            output_dim=config['output_dim']
        ).to(device)
        
        # è¨ˆç®—æ¨¡å‹åƒæ•¸é‡
        total_params = sum(p.numel() for p in model.parameters())
        
        # å‰µå»º EWC è™•ç†å™¨
        ewc = create_ewc_handler(model, importance=1000.0)
        
        # å‰µå»ºæ•¸æ“š
        dataset = create_synthetic_data(
            num_samples=500, 
            input_dim=config['input_dim'], 
            num_classes=config['output_dim']
        )
        dataloader = DataLoader(dataset, batch_size=50, shuffle=True)
        
        # æ¸¬è©¦ Fisher çŸ©é™£è¨ˆç®—æ™‚é–“
        start_time = time.time()
        fisher_matrix = ewc.compute_fisher_matrix(dataloader, verbose=False)
        fisher_time = time.time() - start_time
        
        # å­˜å„²åƒæ•¸
        ewc.store_optimal_params()
        
        # æ¸¬è©¦æ‡²ç½°é …è¨ˆç®—æ™‚é–“
        start_time = time.time()
        for _ in range(100):  # å¤šæ¬¡è¨ˆç®—æ±‚å¹³å‡
            penalty = ewc.penalty()
        penalty_time = (time.time() - start_time) / 100
        
        # è¨˜æ†¶é«”ä½¿ç”¨
        memory_info = ewc.get_memory_usage()
        
        results[config['name']] = {
            'params': total_params,
            'fisher_time': fisher_time,
            'penalty_time': penalty_time * 1000,  # è½‰æ›ç‚ºæ¯«ç§’
            'memory_mb': memory_info['total_mb'],
            'penalty_value': penalty.item()
        }
        
        print(f"    åƒæ•¸é‡: {total_params:,}")
        print(f"    Fisher è¨ˆç®—æ™‚é–“: {fisher_time:.3f}s")
        print(f"    æ‡²ç½°é …è¨ˆç®—æ™‚é–“: {penalty_time*1000:.3f}ms")
        print(f"    è¨˜æ†¶é«”ä½¿ç”¨: {memory_info['total_mb']:.2f}MB")
    
    # æ‰“å°æ•ˆç‡ç¸½çµ
    print("\nğŸ“ˆ æ•ˆç‡æ¸¬è©¦ç¸½çµ:")
    print("æ¨¡å‹å¤§å° | åƒæ•¸é‡ | Fisheræ™‚é–“ | æ‡²ç½°é …æ™‚é–“ | è¨˜æ†¶é«”ä½¿ç”¨")
    print("-" * 60)
    for name, stats in results.items():
        print(f"{name:8} | {stats['params']:7,} | {stats['fisher_time']:8.3f}s | {stats['penalty_time']:8.3f}ms | {stats['memory_mb']:7.2f}MB")
    
    return results


def test_catastrophic_forgetting_prevention():
    """æ¸¬è©¦ç½é›£æ€§éºå¿˜é˜²æ­¢æ•ˆæœ"""
    print("\nğŸ§  æ¸¬è©¦ç½é›£æ€§éºå¿˜é˜²æ­¢æ•ˆæœ...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºæ¨¡å‹
    model = SimpleModel(input_dim=20, hidden_dim=100, output_dim=5).to(device)
    
    # ä»»å‹™ 1: å­¸ç¿’ç¬¬ä¸€å€‹ä»»å‹™
    print("  ğŸ“š å­¸ç¿’ä»»å‹™ 1...")
    task1_dataset = create_synthetic_data(num_samples=500, input_dim=20, num_classes=5, task_shift=0.0)
    task1_loader = DataLoader(task1_dataset, batch_size=32, shuffle=True)
    
    # è¨“ç·´ä»»å‹™ 1
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.train()
    
    task1_losses = []
    for epoch in range(10):  # ç°¡åŒ–è¨“ç·´
        epoch_loss = 0
        for batch_data, batch_labels in task1_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_data)
            loss = F.cross_entropy(outputs, batch_labels)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(task1_loader)
        task1_losses.append(avg_loss)
        if epoch % 5 == 0:
            print(f"    Epoch {epoch}: æå¤± = {avg_loss:.4f}")
    
    # æ¸¬è©¦ä»»å‹™ 1 æ€§èƒ½
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for batch_data, batch_labels in task1_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            outputs = model(batch_data)
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        task1_accuracy = 100 * correct / total
        print(f"  âœ… ä»»å‹™ 1 è¨“ç·´å®Œæˆï¼Œæº–ç¢ºç‡: {task1_accuracy:.2f}%")
    
    # è¨­ç½® EWC
    print("  ğŸ”§ è¨­ç½® EWC...")
    ewc = create_ewc_handler(model, importance=1000.0)
    ewc.finish_task(task1_loader, task_id=0, verbose=False)
    
    # ä»»å‹™ 2: å­¸ç¿’ç¬¬äºŒå€‹ä»»å‹™ (æœ‰ EWC ä¿è­·)
    print("  ğŸ“š å­¸ç¿’ä»»å‹™ 2 (æœ‰ EWC ä¿è­·)...")
    task2_dataset = create_synthetic_data(num_samples=500, input_dim=20, num_classes=5, task_shift=2.0)
    task2_loader = DataLoader(task2_dataset, batch_size=32, shuffle=True)
    
    model.train()
    task2_losses = []
    ewc_penalties = []
    
    for epoch in range(10):
        epoch_loss = 0
        epoch_ewc = 0
        for batch_data, batch_labels in task2_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_data)
            base_loss = F.cross_entropy(outputs, batch_labels)
            
            # æ·»åŠ  EWC æ‡²ç½°é …
            total_loss, ewc_penalty = ewc_loss(base_loss, ewc)
            
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += base_loss.item()
            epoch_ewc += ewc_penalty.item()
        
        avg_loss = epoch_loss / len(task2_loader)
        avg_ewc = epoch_ewc / len(task2_loader)
        task2_losses.append(avg_loss)
        ewc_penalties.append(avg_ewc)
        
        if epoch % 5 == 0:
            print(f"    Epoch {epoch}: åŸºç¤æå¤± = {avg_loss:.4f}, EWC æ‡²ç½° = {avg_ewc:.4f}")
    
    # æ¸¬è©¦ä»»å‹™ 2 æ€§èƒ½
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for batch_data, batch_labels in task2_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            outputs = model(batch_data)
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        task2_accuracy = 100 * correct / total
        print(f"  âœ… ä»»å‹™ 2 è¨“ç·´å®Œæˆï¼Œæº–ç¢ºç‡: {task2_accuracy:.2f}%")
    
    # é‡æ–°æ¸¬è©¦ä»»å‹™ 1 æ€§èƒ½ (æª¢æŸ¥éºå¿˜)
    print("  ğŸ” æª¢æŸ¥ä»»å‹™ 1 éºå¿˜æƒ…æ³...")
    with torch.no_grad():
        correct = 0
        total = 0
        for batch_data, batch_labels in task1_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            outputs = model(batch_data)
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        task1_final_accuracy = 100 * correct / total
        forgetting_rate = task1_accuracy - task1_final_accuracy
        
        print(f"  ğŸ“Š ä»»å‹™ 1 æœ€çµ‚æº–ç¢ºç‡: {task1_final_accuracy:.2f}%")
        print(f"  ğŸ“‰ éºå¿˜ç‡: {forgetting_rate:.2f}%")
    
    # èˆ‡ç„¡ EWC çš„æƒ…æ³å°æ¯”
    print("\n  ğŸ†š å°æ¯”ï¼šç„¡ EWC ä¿è­·çš„æƒ…æ³...")
    
    # é‡æ–°å‰µå»ºæ¨¡å‹é€²è¡Œå°æ¯”
    model_no_ewc = SimpleModel(input_dim=20, hidden_dim=100, output_dim=5).to(device)
    optimizer_no_ewc = optim.Adam(model_no_ewc.parameters(), lr=0.001)
    
    # è¨“ç·´ä»»å‹™ 1 (ç„¡ EWC)
    model_no_ewc.train()
    for epoch in range(10):
        for batch_data, batch_labels in task1_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            optimizer_no_ewc.zero_grad()
            outputs = model_no_ewc(batch_data)
            loss = F.cross_entropy(outputs, batch_labels)
            loss.backward()
            optimizer_no_ewc.step()
    
    # è¨“ç·´ä»»å‹™ 2 (ç„¡ EWC)
    for epoch in range(10):
        for batch_data, batch_labels in task2_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            optimizer_no_ewc.zero_grad()
            outputs = model_no_ewc(batch_data)
            loss = F.cross_entropy(outputs, batch_labels)
            loss.backward()
            optimizer_no_ewc.step()
    
    # æ¸¬è©¦ç„¡ EWC çš„ä»»å‹™ 1 æ€§èƒ½
    model_no_ewc.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for batch_data, batch_labels in task1_loader:
            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
            outputs = model_no_ewc(batch_data)
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        task1_no_ewc_accuracy = 100 * correct / total
        no_ewc_forgetting = task1_accuracy - task1_no_ewc_accuracy
    
    print(f"    ç„¡ EWC ä»»å‹™ 1 æœ€çµ‚æº–ç¢ºç‡: {task1_no_ewc_accuracy:.2f}%")
    print(f"    ç„¡ EWC éºå¿˜ç‡: {no_ewc_forgetting:.2f}%")
    
    # EWC æ•ˆæœè©•ä¼°
    ewc_improvement = no_ewc_forgetting - forgetting_rate
    print(f"\n  ğŸ¯ EWC æ•ˆæœè©•ä¼°:")
    print(f"    EWC éºå¿˜ç‡: {forgetting_rate:.2f}%")
    print(f"    ç„¡ EWC éºå¿˜ç‡: {no_ewc_forgetting:.2f}%")
    print(f"    EWC æ”¹å–„: {ewc_improvement:.2f}%")
    
    if ewc_improvement > 0:
        print("  âœ… EWC æœ‰æ•ˆæ¸›å°‘äº†ç½é›£æ€§éºå¿˜ï¼")
    else:
        print("  âš ï¸ EWC æ•ˆæœä¸æ˜é¡¯ï¼Œå¯èƒ½éœ€è¦èª¿æ•´åƒæ•¸")
    
    return {
        'ewc_forgetting': forgetting_rate,
        'no_ewc_forgetting': no_ewc_forgetting,
        'improvement': ewc_improvement,
        'task1_accuracy': task1_accuracy,
        'task2_accuracy': task2_accuracy,
        'task1_final_accuracy': task1_final_accuracy
    }


def run_comprehensive_ewc_test():
    """é‹è¡Œå…¨é¢çš„ EWC æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹ EWC å…¨é¢æ¸¬è©¦...")
    print("=" * 70)
    
    results = {}
    
    # 1. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
    try:
        basic_success = test_ewc_basic_functionality()
        results['basic_functionality'] = basic_success
        print("âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {e}")
        results['basic_functionality'] = False
        return results
    
    # 2. å¤šä»»å‹™æ•´åˆæ¸¬è©¦
    try:
        multitask_success = test_ewc_multitask_integration()
        results['multitask_integration'] = multitask_success
        print("âœ… å¤šä»»å‹™æ•´åˆæ¸¬è©¦é€šé")
    except Exception as e:
        print(f"âŒ å¤šä»»å‹™æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        results['multitask_integration'] = False
    
    # 3. æ•ˆç‡æ¸¬è©¦
    try:
        efficiency_results = test_ewc_efficiency()
        results['efficiency'] = efficiency_results
        print("âœ… æ•ˆç‡æ¸¬è©¦é€šé")
    except Exception as e:
        print(f"âŒ æ•ˆç‡æ¸¬è©¦å¤±æ•—: {e}")
        results['efficiency'] = False
    
    # 4. éºå¿˜é˜²æ­¢æ¸¬è©¦
    try:
        forgetting_results = test_catastrophic_forgetting_prevention()
        results['forgetting_prevention'] = forgetting_results
        print("âœ… éºå¿˜é˜²æ­¢æ¸¬è©¦é€šé")
    except Exception as e:
        print(f"âŒ éºå¿˜é˜²æ­¢æ¸¬è©¦å¤±æ•—: {e}")
        results['forgetting_prevention'] = False
    
    return results


def print_final_summary(results):
    """æ‰“å°æœ€çµ‚æ¸¬è©¦ç¸½çµ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ EWC æ¸¬è©¦ç¸½çµ")
    print("=" * 70)
    
    # æˆåŠŸç‡çµ±è¨ˆ
    successful_tests = sum(1 for r in results.values() if r not in [False, None])
    total_tests = len(results)
    
    print(f"âœ… æ¸¬è©¦é€šé: {successful_tests}/{total_tests}")
    
    # è©³ç´°çµæœ
    for test_name, result in results.items():
        if test_name == 'basic_functionality':
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"ğŸ§ª åŸºæœ¬åŠŸèƒ½: {status}")
        
        elif test_name == 'multitask_integration':
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"ğŸ”— å¤šä»»å‹™æ•´åˆ: {status}")
        
        elif test_name == 'efficiency':
            if result:
                print(f"â±ï¸ æ•ˆç‡æ¸¬è©¦: âœ… é€šé")
                # é¡¯ç¤ºæ•ˆç‡ç¸½çµ
                large_model = result.get('Large', {})
                if large_model:
                    print(f"   å¤§æ¨¡å‹ Fisher è¨ˆç®—: {large_model.get('fisher_time', 0):.3f}s")
                    print(f"   æ‡²ç½°é …è¨ˆç®—: {large_model.get('penalty_time', 0):.3f}ms")
                    print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {large_model.get('memory_mb', 0):.2f}MB")
            else:
                print(f"â±ï¸ æ•ˆç‡æ¸¬è©¦: âŒ å¤±æ•—")
        
        elif test_name == 'forgetting_prevention':
            if result:
                print(f"ğŸ§  éºå¿˜é˜²æ­¢: âœ… é€šé")
                print(f"   EWC éºå¿˜ç‡: {result.get('ewc_forgetting', 0):.2f}%")
                print(f"   ç„¡ EWC éºå¿˜ç‡: {result.get('no_ewc_forgetting', 0):.2f}%")
                print(f"   æ”¹å–„ç¨‹åº¦: {result.get('improvement', 0):.2f}%")
            else:
                print(f"ğŸ§  éºå¿˜é˜²æ­¢: âŒ å¤±æ•—")
    
    # æœ€çµ‚çµè«–
    print(f"\nğŸ¯ æœ€çµ‚çµè«–:")
    if successful_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰ EWC æ¸¬è©¦é€šéï¼ç®—æ³•å¯¦ç¾æ­£ç¢ºä¸”æœ‰æ•ˆã€‚")
        return True
    else:
        failed_tests = total_tests - successful_tests
        print(f"âš ï¸ {failed_tests} å€‹æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦ä¿®å¾©ã€‚")
        return False


if __name__ == "__main__":
    print("ğŸ§® EWC (Elastic Weight Consolidation) æ¸¬è©¦è…³æœ¬")
    print(f"ğŸ“… æ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    print("\n" + "=" * 70)
    
    # é‹è¡Œæ¸¬è©¦
    test_results = run_comprehensive_ewc_test()
    
    # æ‰“å°ç¸½çµ
    success = print_final_summary(test_results)
    
    if success:
        print("\nâœ… EWC ç®—æ³•å¯¦ç¾å®Œæˆï¼")
    
    # é€€å‡ºç¢¼
    sys.exit(0 if success else 1)