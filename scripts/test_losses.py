#!/usr/bin/env python3
"""
æå¤±å‡½æ•¸æ¸¬è©¦è…³æœ¬
æ¸¬è©¦æª¢æ¸¬ã€åˆ†å‰²ã€åˆ†é¡å’Œå¤šä»»å‹™æå¤±å‡½æ•¸çš„åŠŸèƒ½ã€æ¢¯åº¦å›å‚³å’Œæ•¸å€¼ç©©å®šæ€§
"""
import os
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.losses.detection_loss import create_detection_loss
from src.losses.segmentation_loss import create_segmentation_loss
from src.losses.classification_loss import create_classification_loss
from src.losses.multitask_loss import create_multitask_loss


def test_detection_loss():
    """æ¸¬è©¦æª¢æ¸¬æå¤±å‡½æ•¸"""
    print("ğŸ¯ æ¸¬è©¦æª¢æ¸¬æå¤±å‡½æ•¸...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºæª¢æ¸¬æå¤±
    det_loss = create_detection_loss(num_classes=10, iou_loss_type='giou')
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 4
    num_predictions = 100
    
    # é æ¸¬: (B, H*W, 6) - (cx, cy, w, h, centerness, class)
    predictions = torch.randn(batch_size, num_predictions, 6).to(device)
    predictions[..., :4] = torch.sigmoid(predictions[..., :4])  # æ­¸ä¸€åŒ–åæ¨™
    predictions[..., 5] = torch.randint(0, 10, (batch_size, num_predictions)).float()  # é¡åˆ¥
    
    # ç›®æ¨™
    targets = []
    for b in range(batch_size):
        num_objects = torch.randint(1, 5, (1,)).item()
        target = {
            'boxes': torch.rand(num_objects, 4).to(device),  # (cx, cy, w, h)
            'labels': torch.randint(0, 10, (num_objects,)).to(device)
        }
        targets.append(target)
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    total_loss, loss_dict = det_loss(predictions, targets)
    
    print(f"  âœ… æª¢æ¸¬æå¤±è¨ˆç®—æˆåŠŸ")
    print(f"  ğŸ“Š ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"  ğŸ” æå¤±çµ„æˆ: {list(loss_dict.keys())}")
    
    # æ¸¬è©¦æ¢¯åº¦å›å‚³
    predictions.requires_grad_(True)
    total_loss.backward()
    
    if predictions.grad is not None:
        grad_norm = predictions.grad.norm().item()
        print(f"  ğŸ“ˆ æ¢¯åº¦ç¯„æ•¸: {grad_norm:.6f}")
        print(f"  âœ… æ¢¯åº¦å›å‚³æ­£å¸¸")
    else:
        print(f"  âŒ æ¢¯åº¦å›å‚³å¤±æ•—")
    
    # æ¸¬è©¦ä¸åŒIoUæå¤±é¡å‹
    print(f"  ğŸ§ª æ¸¬è©¦ä¸åŒIoUæå¤±é¡å‹:")
    iou_types = ['iou', 'giou', 'diou', 'ciou']
    
    for iou_type in iou_types:
        try:
            test_loss = create_detection_loss(num_classes=10, iou_loss_type=iou_type)
            test_total_loss, _ = test_loss(predictions.detach(), targets)
            print(f"    {iou_type}: {test_total_loss.item():.4f}")
        except Exception as e:
            print(f"    {iou_type}: éŒ¯èª¤ - {e}")
    
    return True


def test_segmentation_loss():
    """æ¸¬è©¦åˆ†å‰²æå¤±å‡½æ•¸"""
    print("\nğŸ¨ æ¸¬è©¦åˆ†å‰²æå¤±å‡½æ•¸...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºåˆ†å‰²æå¤±
    seg_loss = create_segmentation_loss(num_classes=21, loss_type='combined')
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 4
    height, width = 128, 128
    num_classes = 21
    
    # é æ¸¬: (B, C, H, W)
    predictions = torch.randn(batch_size, num_classes, height, width).to(device)
    
    # ç›®æ¨™: (B, H, W)
    targets = torch.randint(0, num_classes, (batch_size, height, width)).to(device)
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    total_loss, loss_dict = seg_loss(predictions, targets)
    
    print(f"  âœ… åˆ†å‰²æå¤±è¨ˆç®—æˆåŠŸ")
    print(f"  ğŸ“Š ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"  ğŸ” æå¤±çµ„æˆ: {list(loss_dict.keys())}")
    
    # æ¸¬è©¦æ¢¯åº¦å›å‚³
    predictions.requires_grad_(True)
    total_loss.backward()
    
    if predictions.grad is not None:
        grad_norm = predictions.grad.norm().item()
        print(f"  ğŸ“ˆ æ¢¯åº¦ç¯„æ•¸: {grad_norm:.6f}")
        print(f"  âœ… æ¢¯åº¦å›å‚³æ­£å¸¸")
    else:
        print(f"  âŒ æ¢¯åº¦å›å‚³å¤±æ•—")
    
    # æ¸¬è©¦ä¸åŒæå¤±é¡å‹
    print(f"  ğŸ§ª æ¸¬è©¦ä¸åŒæå¤±é¡å‹:")
    loss_types = ['ce', 'dice', 'focal', 'combined', 'advanced']
    
    for loss_type in loss_types:
        try:
            test_loss = create_segmentation_loss(num_classes=21, loss_type=loss_type)
            test_total_loss, _ = test_loss(predictions.detach(), targets)
            print(f"    {loss_type}: {test_total_loss.item():.4f}")
        except Exception as e:
            print(f"    {loss_type}: éŒ¯èª¤ - {e}")
    
    return True


def test_classification_loss():
    """æ¸¬è©¦åˆ†é¡æå¤±å‡½æ•¸"""
    print("\nğŸ“Š æ¸¬è©¦åˆ†é¡æå¤±å‡½æ•¸...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºåˆ†é¡æå¤±
    cls_loss = create_classification_loss(num_classes=10, loss_type='combined')
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 16
    num_classes = 10
    feature_dim = 128
    
    # é æ¸¬: (B, C)
    predictions = torch.randn(batch_size, num_classes).to(device)
    
    # ç›®æ¨™: (B,)
    targets = torch.randint(0, num_classes, (batch_size,)).to(device)
    
    # ç‰¹å¾µ: (B, D) - ç”¨æ–¼å°æ¯”å­¸ç¿’
    features = torch.randn(batch_size, feature_dim).to(device)
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    total_loss, loss_dict = cls_loss(predictions, targets, features=features)
    
    print(f"  âœ… åˆ†é¡æå¤±è¨ˆç®—æˆåŠŸ")
    print(f"  ğŸ“Š ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"  ğŸ” æå¤±çµ„æˆ: {list(loss_dict.keys())}")
    
    # æ¸¬è©¦æ¢¯åº¦å›å‚³
    predictions.requires_grad_(True)
    total_loss.backward()
    
    if predictions.grad is not None:
        grad_norm = predictions.grad.norm().item()
        print(f"  ğŸ“ˆ æ¢¯åº¦ç¯„æ•¸: {grad_norm:.6f}")
        print(f"  âœ… æ¢¯åº¦å›å‚³æ­£å¸¸")
    else:
        print(f"  âŒ æ¢¯åº¦å›å‚³å¤±æ•—")
    
    # æ¸¬è©¦ä¸åŒæå¤±é¡å‹
    print(f"  ğŸ§ª æ¸¬è©¦ä¸åŒæå¤±é¡å‹:")
    loss_types = ['ce', 'focal', 'temperature', 'contrastive', 'combined']
    
    for loss_type in loss_types:
        try:
            test_loss = create_classification_loss(num_classes=10, loss_type=loss_type)
            
            if loss_type == 'contrastive':
                test_total_loss, _ = test_loss(predictions.detach(), targets, features=features)
            else:
                test_total_loss, _ = test_loss(predictions.detach(), targets)
            
            print(f"    {loss_type}: {test_total_loss.item():.4f}")
        except Exception as e:
            print(f"    {loss_type}: éŒ¯èª¤ - {e}")
    
    # æ¸¬è©¦ç‰¹æ®ŠåŠŸèƒ½
    print(f"  ğŸ­ æ¸¬è©¦ Mixup:")
    targets_b = torch.randint(0, num_classes, (batch_size,)).to(device)
    mixup_params = {
        'target_a': targets,
        'target_b': targets_b,
        'lambda': 0.7
    }
    
    mixup_loss, _ = cls_loss(predictions.detach(), targets, mixup_params=mixup_params)
    print(f"    Mixup æå¤±: {mixup_loss.item():.4f}")
    
    # æ¸¬è©¦çŸ¥è­˜è’¸é¤¾
    print(f"  ğŸ“ æ¸¬è©¦çŸ¥è­˜è’¸é¤¾:")
    teacher_logits = torch.randn(batch_size, num_classes).to(device)
    kd_loss = create_classification_loss(num_classes=10, loss_type='distillation')
    
    kd_total_loss, _ = kd_loss(predictions.detach(), targets, teacher_logits=teacher_logits)
    print(f"    çŸ¥è­˜è’¸é¤¾æå¤±: {kd_total_loss.item():.4f}")
    
    return True


def test_multitask_loss():
    """æ¸¬è©¦å¤šä»»å‹™æå¤±å‡½æ•¸"""
    print("\nğŸ”— æ¸¬è©¦å¤šä»»å‹™æå¤±å‡½æ•¸...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºå¤šä»»å‹™æå¤±
    multitask_loss = create_multitask_loss(
        task_weights={'detection': 1.0, 'segmentation': 1.0, 'classification': 1.0},
        weighting_strategy='uncertainty'
    )
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 4
    predictions = {
        'detection': torch.randn(batch_size, 50, 6).to(device),  # (B, H*W, 6)
        'segmentation': torch.randn(batch_size, 21, 128, 128).to(device),  # (B, C, H, W)
        'classification': torch.randn(batch_size, 10).to(device)  # (B, C)
    }
    
    # ç›®æ¨™æ•¸æ“š
    targets = {
        'detection': [
            {
                'boxes': torch.rand(2, 4).to(device),
                'labels': torch.randint(0, 10, (2,)).to(device)
            } for _ in range(batch_size)
        ],
        'segmentation': torch.randint(0, 21, (batch_size, 128, 128)).to(device),
        'classification': torch.randint(0, 10, (batch_size,)).to(device)
    }
    
    # å…±äº«ç‰¹å¾µ
    features = torch.randn(batch_size, 128).to(device)
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    total_loss, loss_info = multitask_loss(predictions, targets, features=features)
    
    print(f"  âœ… å¤šä»»å‹™æå¤±è¨ˆç®—æˆåŠŸ")
    print(f"  ğŸ“Š ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"  ğŸ¯ ä»»å‹™æå¤±: {[(k, v.item()) for k, v in loss_info['task_losses'].items()]}")
    print(f"  âš–ï¸ ä»»å‹™æ¬Šé‡: {loss_info['task_weights']}")
    
    if 'uncertainties' in loss_info:
        print(f"  ğŸ”® ä¸ç¢ºå®šæ€§: {loss_info['uncertainties']}")
    
    # æ¸¬è©¦æ¢¯åº¦å›å‚³
    for task_pred in predictions.values():
        if task_pred.requires_grad:
            task_pred.requires_grad_(True)
    
    total_loss.backward()
    
    print(f"  âœ… æ¢¯åº¦å›å‚³æ­£å¸¸")
    
    # æ¸¬è©¦ä¸åŒæ¬Šé‡ç­–ç•¥
    print(f"  ğŸ§ª æ¸¬è©¦ä¸åŒæ¬Šé‡ç­–ç•¥:")
    strategies = ['fixed', 'uncertainty', 'dynamic']
    
    for strategy in strategies:
        try:
            test_loss = create_multitask_loss(
                weighting_strategy=strategy,
                task_weights={'detection': 1.0, 'segmentation': 1.0, 'classification': 1.0}
            )
            
            test_total_loss, test_info = test_loss(predictions, targets, features=features)
            print(f"    {strategy}: {test_total_loss.item():.4f}, æ¬Šé‡: {test_info['task_weights']}")
            
        except Exception as e:
            print(f"    {strategy}: éŒ¯èª¤ - {e}")
    
    # æ¸¬è©¦æ¬Šé‡èª¿æ•´
    print(f"  ğŸ”§ æ¸¬è©¦æ¬Šé‡èª¿æ•´:")
    current_weights = multitask_loss.get_task_weights()
    print(f"    ç•¶å‰æ¬Šé‡: {current_weights}")
    
    new_weights = {'detection': 2.0, 'segmentation': 0.5, 'classification': 1.5}
    multitask_loss.set_task_weights(new_weights)
    updated_weights = multitask_loss.get_task_weights()
    print(f"    æ›´æ–°å¾Œæ¬Šé‡: {updated_weights}")
    
    return True


def test_loss_numerical_stability():
    """æ¸¬è©¦æå¤±å‡½æ•¸çš„æ•¸å€¼ç©©å®šæ€§"""
    print("\nğŸ”¬ æ¸¬è©¦æå¤±å‡½æ•¸æ•¸å€¼ç©©å®šæ€§...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¸¬è©¦æ¥µç«¯æƒ…æ³
    test_cases = [
        "æ­£å¸¸æƒ…æ³",
        "æ¥µå°å€¼",
        "æ¥µå¤§å€¼",
        "é›¶å€¼",
        "NaNè¼¸å…¥"
    ]
    
    results = {}
    
    for i, case_name in enumerate(test_cases):
        print(f"  ğŸ§ª æ¸¬è©¦ {case_name}:")
        
        try:
            # ç”Ÿæˆä¸åŒçš„æ¸¬è©¦æ•¸æ“š
            if case_name == "æ­£å¸¸æƒ…æ³":
                pred = torch.randn(4, 10).to(device)
                target = torch.randint(0, 10, (4,)).to(device)
            elif case_name == "æ¥µå°å€¼":
                pred = torch.full((4, 10), -100.0).to(device)
                target = torch.randint(0, 10, (4,)).to(device)
            elif case_name == "æ¥µå¤§å€¼":
                pred = torch.full((4, 10), 100.0).to(device)
                target = torch.randint(0, 10, (4,)).to(device)
            elif case_name == "é›¶å€¼":
                pred = torch.zeros(4, 10).to(device)
                target = torch.randint(0, 10, (4,)).to(device)
            else:  # NaNè¼¸å…¥
                pred = torch.full((4, 10), float('nan')).to(device)
                target = torch.randint(0, 10, (4,)).to(device)
            
            # æ¸¬è©¦åˆ†é¡æå¤±
            cls_loss = create_classification_loss(num_classes=10, loss_type='ce')
            loss, _ = cls_loss(pred, target)
            
            if torch.isnan(loss) or torch.isinf(loss):
                results[case_name] = f"âŒ æ•¸å€¼ä¸ç©©å®š: {loss.item()}"
            else:
                results[case_name] = f"âœ… ç©©å®š: {loss.item():.4f}"
            
        except Exception as e:
            results[case_name] = f"âŒ éŒ¯èª¤: {str(e)[:50]}"
        
        print(f"    {results[case_name]}")
    
    return results


def test_loss_convergence():
    """æ¸¬è©¦æå¤±å‡½æ•¸æ”¶æ–‚æ€§"""
    print("\nğŸ“ˆ æ¸¬è©¦æå¤±å‡½æ•¸æ”¶æ–‚æ€§...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºç°¡å–®çš„å„ªåŒ–ä»»å‹™
    batch_size = 16
    num_classes = 10
    
    # ç”Ÿæˆå›ºå®šç›®æ¨™
    targets = torch.randint(0, num_classes, (batch_size,)).to(device)
    
    # å‰µå»ºå¯å­¸ç¿’åƒæ•¸
    logits = nn.Parameter(torch.randn(batch_size, num_classes).to(device))
    
    # å‰µå»ºæå¤±å‡½æ•¸
    cls_loss = create_classification_loss(num_classes=10, loss_type='ce')
    
    # å„ªåŒ–å™¨
    optimizer = torch.optim.Adam([logits], lr=0.01)
    
    # è¨˜éŒ„æå¤±æ­·å²
    loss_history = []
    
    print("  ğŸƒ é–‹å§‹å„ªåŒ–...")
    for step in range(100):
        optimizer.zero_grad()
        
        loss, _ = cls_loss(logits, targets)
        loss.backward()
        optimizer.step()
        
        loss_history.append(loss.item())
        
        if step % 20 == 0:
            print(f"    Step {step}: æå¤± = {loss.item():.4f}")
    
    # æª¢æŸ¥æ”¶æ–‚æ€§
    initial_loss = loss_history[0]
    final_loss = loss_history[-1]
    improvement = (initial_loss - final_loss) / initial_loss * 100
    
    print(f"  ğŸ“Š å„ªåŒ–çµæœ:")
    print(f"    åˆå§‹æå¤±: {initial_loss:.4f}")
    print(f"    æœ€çµ‚æå¤±: {final_loss:.4f}")
    print(f"    æ”¹å–„ç¨‹åº¦: {improvement:.2f}%")
    
    if improvement > 50:
        print(f"  âœ… æ”¶æ–‚æ€§è‰¯å¥½")
        return True
    else:
        print(f"  âš ï¸ æ”¶æ–‚æ€§è¼ƒå·®")
        return False


def test_loss_gradient_flow():
    """æ¸¬è©¦æå¤±å‡½æ•¸æ¢¯åº¦æµ"""
    print("\nğŸŒŠ æ¸¬è©¦æå¤±å‡½æ•¸æ¢¯åº¦æµ...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºå¤šä»»å‹™æå¤±
    multitask_loss = create_multitask_loss(weighting_strategy='uncertainty')
    
    # æ¸¬è©¦æ•¸æ“š
    batch_size = 4
    predictions = {
        'detection': torch.randn(batch_size, 20, 6, requires_grad=True).to(device),
        'segmentation': torch.randn(batch_size, 21, 64, 64, requires_grad=True).to(device),
        'classification': torch.randn(batch_size, 10, requires_grad=True).to(device)
    }
    
    targets = {
        'detection': [
            {
                'boxes': torch.rand(1, 4).to(device),
                'labels': torch.randint(0, 10, (1,)).to(device)
            } for _ in range(batch_size)
        ],
        'segmentation': torch.randint(0, 21, (batch_size, 64, 64)).to(device),
        'classification': torch.randint(0, 10, (batch_size,)).to(device)
    }
    
    # å‰å‘å‚³æ’­
    total_loss, loss_info = multitask_loss(predictions, targets)
    
    # åå‘å‚³æ’­
    total_loss.backward()
    
    # æª¢æŸ¥æ¢¯åº¦
    gradient_stats = {}
    
    for task, pred in predictions.items():
        if pred.grad is not None:
            grad_norm = pred.grad.norm().item()
            grad_mean = pred.grad.mean().item()
            grad_std = pred.grad.std().item()
            
            gradient_stats[task] = {
                'norm': grad_norm,
                'mean': grad_mean,
                'std': grad_std,
                'has_nan': torch.isnan(pred.grad).any().item(),
                'has_inf': torch.isinf(pred.grad).any().item()
            }
        else:
            gradient_stats[task] = {'error': 'ç„¡æ¢¯åº¦'}
    
    print("  ğŸ“Š æ¢¯åº¦çµ±è¨ˆ:")
    for task, stats in gradient_stats.items():
        if 'error' in stats:
            print(f"    {task}: âŒ {stats['error']}")
        elif stats['has_nan'] or stats['has_inf']:
            print(f"    {task}: âŒ æ¢¯åº¦ç•°å¸¸ (NaN/Inf)")
        else:
            print(f"    {task}: âœ… æ­£å¸¸ (ç¯„æ•¸: {stats['norm']:.4f})")
    
    return all('error' not in stats and not stats.get('has_nan', False) and not stats.get('has_inf', False) 
              for stats in gradient_stats.values())


def run_comprehensive_loss_tests():
    """é‹è¡Œå…¨é¢çš„æå¤±å‡½æ•¸æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹æå¤±å‡½æ•¸å…¨é¢æ¸¬è©¦...")
    print("=" * 70)
    
    results = {}
    
    # 1. æª¢æ¸¬æå¤±æ¸¬è©¦
    try:
        results['detection'] = test_detection_loss()
        print("âœ… æª¢æ¸¬æå¤±æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ æª¢æ¸¬æå¤±æ¸¬è©¦å¤±æ•—: {e}")
        results['detection'] = False
    
    # 2. åˆ†å‰²æå¤±æ¸¬è©¦
    try:
        results['segmentation'] = test_segmentation_loss()
        print("âœ… åˆ†å‰²æå¤±æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ åˆ†å‰²æå¤±æ¸¬è©¦å¤±æ•—: {e}")
        results['segmentation'] = False
    
    # 3. åˆ†é¡æå¤±æ¸¬è©¦
    try:
        results['classification'] = test_classification_loss()
        print("âœ… åˆ†é¡æå¤±æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ åˆ†é¡æå¤±æ¸¬è©¦å¤±æ•—: {e}")
        results['classification'] = False
    
    # 4. å¤šä»»å‹™æå¤±æ¸¬è©¦
    try:
        results['multitask'] = test_multitask_loss()
        print("âœ… å¤šä»»å‹™æå¤±æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ å¤šä»»å‹™æå¤±æ¸¬è©¦å¤±æ•—: {e}")
        results['multitask'] = False
    
    # 5. æ•¸å€¼ç©©å®šæ€§æ¸¬è©¦
    try:
        stability_results = test_loss_numerical_stability()
        results['numerical_stability'] = stability_results
        print("âœ… æ•¸å€¼ç©©å®šæ€§æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ•¸å€¼ç©©å®šæ€§æ¸¬è©¦å¤±æ•—: {e}")
        results['numerical_stability'] = False
    
    # 6. æ”¶æ–‚æ€§æ¸¬è©¦
    try:
        results['convergence'] = test_loss_convergence()
        print("âœ… æ”¶æ–‚æ€§æ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ”¶æ–‚æ€§æ¸¬è©¦å¤±æ•—: {e}")
        results['convergence'] = False
    
    # 7. æ¢¯åº¦æµæ¸¬è©¦
    try:
        results['gradient_flow'] = test_loss_gradient_flow()
        print("âœ… æ¢¯åº¦æµæ¸¬è©¦å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµæ¸¬è©¦å¤±æ•—: {e}")
        results['gradient_flow'] = False
    
    return results


def print_final_summary(results):
    """æ‰“å°æœ€çµ‚æ¸¬è©¦ç¸½çµ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ æå¤±å‡½æ•¸æ¸¬è©¦ç¸½çµ")
    print("=" * 70)
    
    # æˆåŠŸç‡çµ±è¨ˆ
    boolean_results = {k: v for k, v in results.items() if isinstance(v, bool)}
    successful_tests = sum(boolean_results.values())
    total_tests = len(boolean_results)
    
    print(f"âœ… æ¸¬è©¦é€šé: {successful_tests}/{total_tests}")
    
    # è©³ç´°çµæœ
    test_names = {
        'detection': 'ğŸ¯ æª¢æ¸¬æå¤±',
        'segmentation': 'ğŸ¨ åˆ†å‰²æå¤±', 
        'classification': 'ğŸ“Š åˆ†é¡æå¤±',
        'multitask': 'ğŸ”— å¤šä»»å‹™æå¤±',
        'convergence': 'ğŸ“ˆ æ”¶æ–‚æ€§',
        'gradient_flow': 'ğŸŒŠ æ¢¯åº¦æµ'
    }
    
    for test_key, test_name in test_names.items():
        if test_key in results:
            result = results[test_key]
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"{test_name}: {status}")
    
    # æ•¸å€¼ç©©å®šæ€§è©³ç´°çµæœ
    if 'numerical_stability' in results and isinstance(results['numerical_stability'], dict):
        print("ğŸ”¬ æ•¸å€¼ç©©å®šæ€§:")
        for case, result in results['numerical_stability'].items():
            print(f"  {case}: {result}")
    
    # æœ€çµ‚çµè«–
    print(f"\nğŸ¯ æœ€çµ‚çµè«–:")
    if successful_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æå¤±å‡½æ•¸æ¸¬è©¦é€šéï¼å¯¦ç¾æ­£ç¢ºä¸”ç©©å®šã€‚")
        return True
    else:
        failed_tests = total_tests - successful_tests
        print(f"âš ï¸ {failed_tests} å€‹æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦ä¿®å¾©ã€‚")
        return False


if __name__ == "__main__":
    print("ğŸ”¥ æå¤±å‡½æ•¸å…¨é¢æ¸¬è©¦è…³æœ¬")
    print(f"ğŸ“… æ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    print("\n" + "=" * 70)
    
    # é‹è¡Œæ¸¬è©¦
    test_results = run_comprehensive_loss_tests()
    
    # æ‰“å°ç¸½çµ
    success = print_final_summary(test_results)
    
    if success:
        print("\nâœ… æå¤±å‡½æ•¸å¯¦ç¾å®Œæˆï¼")
    
    # é€€å‡ºç¢¼
    sys.exit(0 if success else 1)