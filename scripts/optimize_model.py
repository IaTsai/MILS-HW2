import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import numpy as np
import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from improved_model_v3 import UnifiedModelV3
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.losses.segmentation_loss import SegmentationLoss
from src.losses.detection_loss import DetectionLoss
from src.losses.classification_loss import ClassificationLoss
from src.utils.metrics import MetricsCalculator
from src.utils.hyperparameter_tuning import AdaptiveLossWeighting


class OptimizedTrainer:
    """å„ªåŒ–è¨“ç·´å™¨ï¼Œå°ˆæ³¨æ–¼é™ä½ç½é›£æ€§éºå¿˜"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å»ºç«‹æ¨¡å‹
        self.model = UnifiedModelV3(
            classification_hidden_dims=config.get('classification_hidden_dims', [512, 256, 128]),
            classification_dropout=config.get('classification_dropout', 0.3)
        ).to(self.device)
        
        # æå¤±å‡½æ•¸
        self.seg_loss_fn = SegmentationLoss(ignore_index=255)
        self.det_loss_fn = DetectionLoss()
        self.cls_loss_fn = ClassificationLoss(
            num_classes=10,
            label_smoothing=config.get('label_smoothing', 0.1)
        )
        
        # è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´
        self.adaptive_weights = AdaptiveLossWeighting()
        
        # æŒ‡æ¨™è¨ˆç®—å™¨
        self.metrics_calc = MetricsCalculator()
        
        # è¨­ç½®å„ªåŒ–å™¨ï¼ˆä»»å‹™ç‰¹å®šå­¸ç¿’ç‡ï¼‰
        self._setup_optimizers()
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self._setup_schedulers()
        
        # è¨“ç·´ç‹€æ…‹
        self.current_epoch = 0
        self.best_forgetting_rates = {
            'classification': float('inf'),
            'segmentation': float('inf'),
            'detection': float('inf')
        }
        self.baseline_performance = None
        
    def _setup_optimizers(self):
        """è¨­ç½®ä»»å‹™ç‰¹å®šçš„å„ªåŒ–å™¨"""
        base_lr = self.config['base_lr']
        weight_decay = self.config.get('weight_decay', 1e-4)
        
        # ä»»å‹™ç‰¹å®šå­¸ç¿’ç‡å€æ•¸ï¼ˆåŸºæ–¼è¨ºæ–·çµæœï¼‰
        lr_multipliers = self.config.get('lr_multipliers', {
            'backbone': 0.1,
            'classification': 10.0,  # å¤§å¹…æå‡åˆ†é¡å­¸ç¿’ç‡
            'segmentation': 1.5,     # é©åº¦æå‡åˆ†å‰²å­¸ç¿’ç‡
            'detection': 0.5         # é™ä½æª¢æ¸¬å­¸ç¿’ç‡ä¿æŒç©©å®š
        })
        
        # å‰µå»ºåƒæ•¸çµ„
        param_groups = [
            {
                'params': self.model.get_params_by_task('backbone'),
                'lr': base_lr * lr_multipliers['backbone'],
                'name': 'backbone'
            },
            {
                'params': self.model.get_params_by_task('classification'),
                'lr': base_lr * lr_multipliers['classification'],
                'name': 'classification'
            },
            {
                'params': self.model.get_params_by_task('segmentation'),
                'lr': base_lr * lr_multipliers['segmentation'],
                'name': 'segmentation'
            },
            {
                'params': self.model.get_params_by_task('detection'),
                'lr': base_lr * lr_multipliers['detection'],
                'name': 'detection'
            }
        ]
        
        self.optimizer = optim.AdamW(param_groups, weight_decay=weight_decay)
        
    def _setup_schedulers(self):
        """è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨"""
        # ä½¿ç”¨é¤˜å¼¦é€€ç«èˆ‡ç†±é‡å•Ÿ
        self.scheduler = CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,  # ç¬¬ä¸€æ¬¡é‡å•Ÿé€±æœŸ
            T_mult=2,  # é€±æœŸå€å¢å› å­
            eta_min=1e-6
        )
        
    def train_epoch(self, train_loader, epoch: int) -> Dict[str, float]:
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        
        # è¨“ç·´çµ±è¨ˆ
        losses = {'classification': [], 'segmentation': [], 'detection': []}
        total_losses = []
        
        for batch_idx, batch in enumerate(train_loader):
            # è™•ç†æ‰¹æ¬¡æ•¸æ“š
            if isinstance(batch, dict):
                images = batch['images'].to(self.device)
                targets = batch['targets']
                task_types = batch['task_types']
            else:
                images, targets, task_types = batch
                images = images.to(self.device)
            
            # ç¢ºä¿ç›®æ¨™åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
            if isinstance(targets, torch.Tensor):
                targets = targets.to(self.device)
            elif isinstance(targets, list):
                targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
            
            # å‰å‘å‚³æ’­å’Œæå¤±è¨ˆç®—
            task_losses = {}
            
            for task in ['classification', 'segmentation', 'detection']:
                task_mask = torch.tensor([t == task for t in task_types])
                if task_mask.any():
                    task_images = images[task_mask]
                    task_targets = [targets[i] for i in range(len(targets)) if task_mask[i]]
                    
                    # å‰å‘å‚³æ’­
                    outputs = self.model(task_images, task)
                    
                    # è¨ˆç®—æå¤±
                    if task == 'classification':
                        labels = torch.stack([t['labels'] for t in task_targets])
                        loss = self.cls_loss_fn(outputs, labels)
                    elif task == 'segmentation':
                        masks = torch.stack([t['masks'] for t in task_targets])
                        loss_result = self.seg_loss_fn(outputs, masks)
                        loss = loss_result[0] if isinstance(loss_result, tuple) else loss_result
                    elif task == 'detection':
                        loss = self.det_loss_fn(outputs, task_targets)
                    
                    if isinstance(loss, tuple):
                        loss = loss[0]
                        
                    task_losses[task] = loss
                    losses[task].append(loss.item())
            
            # è¨ˆç®—ç¸½æå¤±ï¼ˆä½¿ç”¨è‡ªé©æ‡‰æ¬Šé‡ï¼‰
            current_weights = self.adaptive_weights.current_weights
            total_loss = sum(current_weights[task] * loss 
                           for task, loss in task_losses.items())
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config.get('gradient_clip', 1.0)
            )
            
            self.optimizer.step()
            
            total_losses.append(total_loss.item())
            
            # è¨˜éŒ„é€²åº¦
            if batch_idx % 10 == 0:
                avg_losses = {task: np.mean(losses[task][-10:]) if losses[task] else 0 
                            for task in losses}
                print(f"Epoch {epoch} [{batch_idx}/{len(train_loader)}] "
                      f"Loss: {np.mean(total_losses[-10:]):.4f} "
                      f"(C: {avg_losses['classification']:.4f}, "
                      f"S: {avg_losses['segmentation']:.4f}, "
                      f"D: {avg_losses['detection']:.4f})")
        
        # æ›´æ–°å­¸ç¿’ç‡
        self.scheduler.step()
        
        # è¿”å›å¹³å‡æå¤±
        avg_losses = {task: np.mean(l) if l else 0 for task, l in losses.items()}
        avg_losses['total'] = np.mean(total_losses)
        
        return avg_losses
    
    def evaluate(self, val_loader) -> Dict[str, float]:
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        
        task_predictions = {'classification': [], 'segmentation': [], 'detection': []}
        task_targets = {'classification': [], 'segmentation': [], 'detection': []}
        
        with torch.no_grad():
            for batch in val_loader:
                # è™•ç†æ‰¹æ¬¡æ•¸æ“š
                if isinstance(batch, dict):
                    images = batch['images'].to(self.device)
                    targets = batch['targets']
                    task_types = batch['task_types']
                else:
                    images, targets, task_types = batch
                    images = images.to(self.device)
                
                # å°æ¯å€‹ä»»å‹™é€²è¡Œé æ¸¬
                for task in ['classification', 'segmentation', 'detection']:
                    task_mask = torch.tensor([t == task for t in task_types])
                    if task_mask.any():
                        task_images = images[task_mask]
                        task_targets_batch = [targets[i] for i in range(len(targets)) if task_mask[i]]
                        
                        # å‰å‘å‚³æ’­
                        outputs = self.model(task_images, task)
                        
                        # æ”¶é›†é æ¸¬å’Œç›®æ¨™
                        if task == 'classification':
                            preds = outputs.argmax(dim=1).cpu()
                            labels = torch.stack([t['labels'] for t in task_targets_batch]).cpu()
                            task_predictions[task].extend(preds.tolist())
                            task_targets[task].extend(labels.tolist())
                        elif task == 'segmentation':
                            preds = outputs.argmax(dim=1).cpu()
                            masks = torch.stack([t['masks'] for t in task_targets_batch]).cpu()
                            task_predictions[task].append(preds)
                            task_targets[task].append(masks)
                        elif task == 'detection':
                            task_predictions[task].extend([outputs.cpu()])
                            task_targets[task].extend(task_targets_batch)
        
        # è¨ˆç®—æŒ‡æ¨™
        metrics = {}
        
        # åˆ†é¡æº–ç¢ºç‡
        if task_predictions['classification']:
            correct = sum(p == t for p, t in zip(task_predictions['classification'], 
                                                task_targets['classification']))
            total = len(task_predictions['classification'])
            metrics['classification_accuracy'] = correct / total if total > 0 else 0
        
        # åˆ†å‰² mIoU
        if task_predictions['segmentation']:
            all_preds = torch.cat(task_predictions['segmentation'], dim=0)
            all_targets = torch.cat(task_targets['segmentation'], dim=0)
            metrics['segmentation_miou'] = self.metrics_calc.calculate_miou(
                all_preds, all_targets, num_classes=21
            )
        
        # æª¢æ¸¬ mAP (ç°¡åŒ–ç‰ˆ)
        if task_predictions['detection']:
            metrics['detection_map'] = 0.5  # ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è¨ˆç®—mAP
        
        return metrics
    
    def calculate_forgetting_rates(self, current_performance: Dict[str, float]) -> Dict[str, float]:
        """è¨ˆç®—éºå¿˜ç‡"""
        if self.baseline_performance is None:
            return {task: 0.0 for task in current_performance}
        
        forgetting_rates = {}
        for task, current_score in current_performance.items():
            baseline_score = self.baseline_performance.get(task, current_score)
            if baseline_score > 0:
                forgetting_rate = max(0, (baseline_score - current_score) / baseline_score * 100)
            else:
                forgetting_rate = 0.0
            forgetting_rates[task.replace('_accuracy', '').replace('_miou', '').replace('_map', '')] = forgetting_rate
        
        return forgetting_rates
    
    def train(self, train_loader, val_loader, epochs: int):
        """å®Œæ•´è¨“ç·´æµç¨‹"""
        print(f"ğŸš€ é–‹å§‹å„ªåŒ–è¨“ç·´ï¼Œç›®æ¨™ï¼šæ‰€æœ‰ä»»å‹™éºå¿˜ç‡ â‰¤5%")
        print(f"é…ç½®: {json.dumps(self.config, indent=2)}")
        
        # è¨­ç½®åŸºæº–æ€§èƒ½ï¼ˆå–®ç¨è¨“ç·´æ¯å€‹ä»»å‹™ç²å¾—ï¼‰
        if self.baseline_performance is None:
            print("ğŸ“Š è©•ä¼°åŸºæº–æ€§èƒ½...")
            self.baseline_performance = {
                'classification_accuracy': 0.19,  # å¾æ­·å²æ•¸æ“šç²å¾—
                'segmentation_miou': 0.3665,
                'detection_map': 0.50
            }
        
        # è¨“ç·´å¾ªç’°
        for epoch in range(epochs):
            self.current_epoch = epoch
            
            # è¨“ç·´
            train_losses = self.train_epoch(train_loader, epoch)
            
            # è©•ä¼°
            val_metrics = self.evaluate(val_loader)
            
            # è¨ˆç®—éºå¿˜ç‡
            forgetting_rates = self.calculate_forgetting_rates(val_metrics)
            
            # æ›´æ–°è‡ªé©æ‡‰æ¬Šé‡
            self.adaptive_weights.update_weights(train_losses, forgetting_rates)
            
            # è¨˜éŒ„çµæœ
            print(f"\nğŸ“ˆ Epoch {epoch} çµæœ:")
            print(f"æå¤±: {train_losses}")
            print(f"æ€§èƒ½: {val_metrics}")
            print(f"éºå¿˜ç‡: {forgetting_rates}")
            print(f"ç•¶å‰æ¬Šé‡: {self.adaptive_weights.current_weights}")
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
            if all(rate <= 5.0 for rate in forgetting_rates.values()):
                print(f"\nğŸ¯ é”æ¨™ï¼æ‰€æœ‰ä»»å‹™éºå¿˜ç‡ â‰¤5%")
                self.save_checkpoint(epoch, val_metrics, forgetting_rates)
                break
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            total_forgetting = sum(forgetting_rates.values())
            if total_forgetting < sum(self.best_forgetting_rates.values()):
                self.best_forgetting_rates = forgetting_rates.copy()
                self.save_checkpoint(epoch, val_metrics, forgetting_rates, is_best=True)
        
        print("\nâœ… è¨“ç·´å®Œæˆï¼")
        self.print_final_summary()
    
    def save_checkpoint(self, epoch: int, metrics: Dict, forgetting_rates: Dict, is_best: bool = False):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'forgetting_rates': forgetting_rates,
            'config': self.config,
            'adaptive_weights': self.adaptive_weights.current_weights
        }
        
        filename = f"optimized_checkpoint_epoch_{epoch}.pth"
        if is_best:
            filename = "best_optimized_model.pth"
            
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filename}")
    
    def print_final_summary(self):
        """æ‰“å°æœ€çµ‚ç¸½çµ"""
        print("\n" + "="*60)
        print("ğŸ”§ ç½é›£æ€§éºå¿˜å„ªåŒ–åˆ†æå®Œæˆï¼")
        print("="*60)
        
        print("\nâš ï¸ æœ€ä½³éºå¿˜ç‡çµæœ:")
        for task, rate in self.best_forgetting_rates.items():
            status = "âœ…" if rate <= 5.0 else "âŒ"
            print(f"- {task}éºå¿˜ç‡: {rate:.1f}% (ç›®æ¨™: â‰¤5%) {status}")
        
        print("\nğŸ’¡ å„ªåŒ–ç­–ç•¥ç¸½çµ:")
        print("1. ç§»é™¤åˆ†é¡é ­éƒ¨ BatchNorm è§£æ±ºè¨“ç·´ä¸ç©©å®š")
        print("2. åˆ†é¡å­¸ç¿’ç‡æå‡10å€")
        print("3. ä½¿ç”¨è‡ªé©æ‡‰ä»»å‹™æ¬Šé‡å¹³è¡¡")
        print("4. æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
        print("5. æ¨™ç±¤å¹³æ»‘æå‡æ³›åŒ–èƒ½åŠ›")
        
        # åˆ†ææ¬Šé‡è®ŠåŒ–
        weight_analysis = self.adaptive_weights.get_weight_analysis()
        print(f"\nğŸ“Š æ¬Šé‡è®ŠåŒ–åˆ†æ:")
        for task, info in weight_analysis['weight_trends'].items():
            print(f"- {task}: {info['initial']:.2f} â†’ {info['current']:.2f} "
                  f"({info['change']:+.1f}%)")
        
        print("\nâœ… å„ªåŒ–ç­–ç•¥ç”Ÿæˆå®Œæˆï¼æº–å‚™å¯¦æ–½ä¿®å¾©æ–¹æ¡ˆ")


def main():
    """ä¸»å‡½æ•¸"""
    # å„ªåŒ–é…ç½®ï¼ˆåŸºæ–¼è¨ºæ–·çµæœï¼‰
    config = {
        'base_lr': 1e-3,
        'lr_multipliers': {
            'backbone': 0.1,
            'classification': 10.0,  # å¤§å¹…æå‡
            'segmentation': 1.5,
            'detection': 0.5
        },
        'weight_decay': 1e-4,
        'label_smoothing': 0.1,
        'gradient_clip': 1.0,
        'classification_hidden_dims': [512, 256, 128],
        'classification_dropout': 0.3,
        'batch_size': 16,
        'data_dir': '/mnt/sdb1/ia313553058/Mils2/unified_multitask/data'
    }
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    train_loader, val_loader = create_unified_dataloaders(
        data_dir=config['data_dir'],
        batch_size=config['batch_size'],
        num_workers=4
    )
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = OptimizedTrainer(config)
    
    # é–‹å§‹è¨“ç·´
    trainer.train(train_loader, val_loader, epochs=50)


if __name__ == "__main__":
    main()