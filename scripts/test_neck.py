#!/usr/bin/env python3
"""
é ¸éƒ¨ç¶²è·¯æ¸¬è©¦è…³æœ¬
æ¸¬è©¦ FPN é ¸éƒ¨ç¶²è·¯çš„åŠŸèƒ½ã€æ€§èƒ½å’Œåƒæ•¸é‡
"""
import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.backbone import create_backbone
from src.models.neck import create_neck, FeaturePyramidNetwork, LightweightNeck


def test_neck_basic_functionality():
    """æ¸¬è©¦é ¸éƒ¨ç¶²è·¯åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦é ¸éƒ¨ç¶²è·¯åŸºæœ¬åŠŸèƒ½...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {device}")
    
    # å‰µå»º FPN é ¸éƒ¨ç¶²è·¯
    fpn = create_neck('fpn', in_channels_list=[16, 24, 48, 96], out_channels=256)
    fpn = fpn.to(device)
    
    # æ¨¡æ“¬éª¨å¹¹ç¶²è·¯è¼¸å‡º
    batch_size = 2
    input_features = {
        'layer1': torch.randn(batch_size, 16, 128, 128).to(device),  # 1/4
        'layer2': torch.randn(batch_size, 24, 64, 64).to(device),    # 1/8
        'layer3': torch.randn(batch_size, 48, 32, 32).to(device),    # 1/16
        'layer4': torch.randn(batch_size, 96, 16, 16).to(device)     # 1/32
    }
    
    # å‰å‘å‚³æ’­
    with torch.no_grad():
        output_features = fpn(input_features)
    
    # é©—è­‰è¼¸å‡º
    expected_shapes = {
        'P2': (batch_size, 256, 128, 128),
        'P3': (batch_size, 256, 64, 64),
        'P4': (batch_size, 256, 32, 32),
        'P5': (batch_size, 256, 16, 16)
    }
    
    print("âœ… FPN åŸºæœ¬åŠŸèƒ½æ¸¬è©¦:")
    print(f"  ğŸ“Š åƒæ•¸çµ±è¨ˆ: {fpn.get_parameter_count()}")
    print(f"  ğŸ“‹ ç‰¹å¾µä¿¡æ¯: {fpn.get_feature_info()}")
    
    success = True
    for name, expected_shape in expected_shapes.items():
        if name in output_features:
            actual_shape = tuple(output_features[name].shape)
            if actual_shape == expected_shape:
                print(f"  âœ… {name}: {actual_shape}")
            else:
                print(f"  âŒ {name}: æœŸæœ› {expected_shape}, å¯¦éš› {actual_shape}")
                success = False
        else:
            print(f"  âŒ ç¼ºå°‘è¼¸å‡ºç‰¹å¾µ: {name}")
            success = False
    
    return success, fpn


def test_lightweight_neck():
    """æ¸¬è©¦è¼•é‡åŒ–é ¸éƒ¨ç¶²è·¯"""
    print("\nğŸ§ª æ¸¬è©¦è¼•é‡åŒ–é ¸éƒ¨ç¶²è·¯...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºè¼•é‡åŒ–é ¸éƒ¨ç¶²è·¯
    lightweight_neck = create_neck('lightweight', in_channels_list=[16, 24, 48, 96], out_channels=128)
    lightweight_neck = lightweight_neck.to(device)
    
    # æ¨¡æ“¬è¼¸å…¥
    batch_size = 4
    input_features = {
        'layer1': torch.randn(batch_size, 16, 128, 128).to(device),
        'layer2': torch.randn(batch_size, 24, 64, 64).to(device),
        'layer3': torch.randn(batch_size, 48, 32, 32).to(device),
        'layer4': torch.randn(batch_size, 96, 16, 16).to(device)
    }
    
    # å‰å‘å‚³æ’­
    with torch.no_grad():
        output_features = lightweight_neck(input_features)
    
    print("âœ… è¼•é‡åŒ–é ¸éƒ¨ç¶²è·¯æ¸¬è©¦:")
    print(f"  ğŸ“Š åƒæ•¸çµ±è¨ˆ: {lightweight_neck.get_parameter_count()}")
    
    for name, feature in output_features.items():
        print(f"  ğŸ“ {name}: {feature.shape}")
    
    return lightweight_neck


def test_backbone_neck_integration():
    """æ¸¬è©¦éª¨å¹¹ç¶²è·¯èˆ‡é ¸éƒ¨ç¶²è·¯æ•´åˆ"""
    print("\nğŸ§ª æ¸¬è©¦éª¨å¹¹ç¶²è·¯èˆ‡é ¸éƒ¨ç¶²è·¯æ•´åˆ...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºéª¨å¹¹ç¶²è·¯
    backbone = create_backbone('mobilenetv3_small', pretrained=False)
    backbone = backbone.to(device)
    
    # å‰µå»ºé ¸éƒ¨ç¶²è·¯
    fpn = create_neck('fpn', in_channels_list=[16, 24, 48, 96], out_channels=256)
    fpn = fpn.to(device)
    
    # æ¸¬è©¦è¼¸å…¥
    batch_size = 2
    input_tensor = torch.randn(batch_size, 3, 512, 512).to(device)
    
    # å®Œæ•´å‰å‘å‚³æ’­
    with torch.no_grad():
        # éª¨å¹¹ç¶²è·¯ç‰¹å¾µæå–
        backbone_features = backbone(input_tensor)
        
        # é ¸éƒ¨ç¶²è·¯ç‰¹å¾µèåˆ
        neck_features = fpn(backbone_features)
    
    print("âœ… éª¨å¹¹ç¶²è·¯èˆ‡é ¸éƒ¨ç¶²è·¯æ•´åˆæ¸¬è©¦:")
    print("  ğŸ”— éª¨å¹¹ç¶²è·¯è¼¸å‡º:")
    for name, feature in backbone_features.items():
        print(f"    {name}: {feature.shape}")
    
    print("  ğŸ”— é ¸éƒ¨ç¶²è·¯è¼¸å‡º:")
    for name, feature in neck_features.items():
        print(f"    {name}: {feature.shape}")
    
    return backbone, fpn


def test_gradient_flow():
    """æ¸¬è©¦æ¢¯åº¦æµå‹•"""
    print("\nğŸ§ª æ¸¬è©¦æ¢¯åº¦æµå‹•...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºç¶²è·¯
    fpn = create_neck('fpn', in_channels_list=[16, 24, 48, 96], out_channels=256)
    fpn = fpn.to(device)
    fpn.train()
    
    # æ¨¡æ“¬è¼¸å…¥ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
    input_features = {
        'layer1': torch.randn(1, 16, 128, 128, device=device, requires_grad=True),
        'layer2': torch.randn(1, 24, 64, 64, device=device, requires_grad=True),
        'layer3': torch.randn(1, 48, 32, 32, device=device, requires_grad=True),
        'layer4': torch.randn(1, 96, 16, 16, device=device, requires_grad=True)
    }
    
    # å‰å‘å‚³æ’­
    output_features = fpn(input_features)
    
    # è¨ˆç®—æå¤±ï¼ˆç°¡å–®çš„å‡å€¼ï¼‰
    loss = sum(feature.mean() for feature in output_features.values())
    
    # åå‘å‚³æ’­
    loss.backward()
    
    print("âœ… æ¢¯åº¦æµå‹•æ¸¬è©¦:")
    print(f"  ğŸ“‰ ç¸½æå¤±: {loss.item():.6f}")
    
    # æª¢æŸ¥æ¢¯åº¦
    gradient_stats = {}
    for name, param in fpn.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            gradient_stats[name] = grad_norm
            if grad_norm > 0:
                print(f"  âœ… {name}: æ¢¯åº¦ç¯„æ•¸ {grad_norm:.6f}")
            else:
                print(f"  âš ï¸ {name}: æ¢¯åº¦ç‚ºé›¶")
        else:
            print(f"  âŒ {name}: ç„¡æ¢¯åº¦")
    
    return len([g for g in gradient_stats.values() if g > 0]) > 0


def test_parameter_efficiency():
    """æ¸¬è©¦åƒæ•¸æ•ˆç‡"""
    print("\nğŸ§ª æ¸¬è©¦åƒæ•¸æ•ˆç‡...")
    
    # æ¯”è¼ƒä¸åŒé…ç½®çš„åƒæ•¸é‡
    configs = [
        {'name': 'FPN-256', 'type': 'fpn', 'out_channels': 256},
        {'name': 'FPN-128', 'type': 'fpn', 'out_channels': 128},
        {'name': 'FPN-64', 'type': 'fpn', 'out_channels': 64},
        {'name': 'Lightweight-128', 'type': 'lightweight', 'out_channels': 128},
        {'name': 'Lightweight-64', 'type': 'lightweight', 'out_channels': 64},
    ]
    
    print("âœ… åƒæ•¸æ•ˆç‡æ¯”è¼ƒ:")
    
    results = []
    for config in configs:
        neck = create_neck(
            config['type'], 
            in_channels_list=[16, 24, 48, 96], 
            out_channels=config['out_channels']
        )
        
        param_count = neck.get_parameter_count()
        total_params = param_count['total_parameters']
        
        print(f"  ğŸ“Š {config['name']}: {total_params:,} åƒæ•¸ ({total_params/1e6:.3f}M)")
        
        results.append({
            'name': config['name'],
            'type': config['type'],
            'out_channels': config['out_channels'],
            'parameters': total_params
        })
    
    # æ‰¾å‡ºæœ€ä½³å¹³è¡¡é»
    print("\nğŸ¯ æ¨è–¦é…ç½®:")
    for result in results:
        if result['parameters'] < 1e6:  # < 1M åƒæ•¸
            efficiency = result['out_channels'] / (result['parameters'] / 1e6)
            print(f"  ğŸ† {result['name']}: {efficiency:.1f} é€šé“/Måƒæ•¸")
    
    return results


def test_inference_speed():
    """æ¸¬è©¦æ¨ç†é€Ÿåº¦"""
    print("\nğŸ§ª æ¸¬è©¦æ¨ç†é€Ÿåº¦...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºç¶²è·¯
    fpn = create_neck('fpn', in_channels_list=[16, 24, 48, 96], out_channels=256)
    fpn = fpn.to(device)
    fpn.eval()
    
    # æº–å‚™è¼¸å…¥
    input_features = {
        'layer1': torch.randn(1, 16, 128, 128).to(device),
        'layer2': torch.randn(1, 24, 64, 64).to(device),
        'layer3': torch.randn(1, 48, 32, 32).to(device),
        'layer4': torch.randn(1, 96, 16, 16).to(device)
    }
    
    # ç†±èº«
    with torch.no_grad():
        for _ in range(10):
            _ = fpn(input_features)
    
    # è¨ˆæ™‚
    times = []
    with torch.no_grad():
        for _ in range(100):
            start_time = time.time()
            _ = fpn(input_features)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # è½‰æ›ç‚ºæ¯«ç§’
    
    avg_time = np.mean(times)
    std_time = np.std(times)
    
    print("âœ… æ¨ç†é€Ÿåº¦æ¸¬è©¦:")
    print(f"  â±ï¸ å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.3f} Â± {std_time:.3f} ms")
    print(f"  ğŸš€ FPS: {1000/avg_time:.1f}")
    
    return avg_time < 50  # ç›®æ¨™ < 50ms


def run_comprehensive_tests():
    """é‹è¡Œå…¨é¢æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹é ¸éƒ¨ç¶²è·¯å…¨é¢æ¸¬è©¦...")
    print("=" * 60)
    
    test_results = {}
    
    # 1. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
    try:
        success, fpn_model = test_neck_basic_functionality()
        test_results['basic_functionality'] = success
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {e}")
        test_results['basic_functionality'] = False
        return test_results
    
    # 2. è¼•é‡åŒ–æ¸¬è©¦
    try:
        lightweight_model = test_lightweight_neck()
        test_results['lightweight'] = True
    except Exception as e:
        print(f"âŒ è¼•é‡åŒ–ç¶²è·¯æ¸¬è©¦å¤±æ•—: {e}")
        test_results['lightweight'] = False
    
    # 3. æ•´åˆæ¸¬è©¦
    try:
        backbone_model, neck_model = test_backbone_neck_integration()
        test_results['integration'] = True
    except Exception as e:
        print(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        test_results['integration'] = False
    
    # 4. æ¢¯åº¦æµå‹•æ¸¬è©¦
    try:
        gradient_success = test_gradient_flow()
        test_results['gradient_flow'] = gradient_success
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµå‹•æ¸¬è©¦å¤±æ•—: {e}")
        test_results['gradient_flow'] = False
    
    # 5. åƒæ•¸æ•ˆç‡æ¸¬è©¦
    try:
        param_results = test_parameter_efficiency()
        test_results['parameter_efficiency'] = True
        test_results['param_details'] = param_results
    except Exception as e:
        print(f"âŒ åƒæ•¸æ•ˆç‡æ¸¬è©¦å¤±æ•—: {e}")
        test_results['parameter_efficiency'] = False
    
    # 6. æ¨ç†é€Ÿåº¦æ¸¬è©¦
    try:
        speed_success = test_inference_speed()
        test_results['inference_speed'] = speed_success
    except Exception as e:
        print(f"âŒ æ¨ç†é€Ÿåº¦æ¸¬è©¦å¤±æ•—: {e}")
        test_results['inference_speed'] = False
    
    return test_results


def print_final_summary(test_results):
    """æ‰“å°æœ€çµ‚ç¸½çµ"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ é ¸éƒ¨ç¶²è·¯æ¸¬è©¦ç¸½çµ")
    print("=" * 60)
    
    total_tests = len([k for k in test_results.keys() if k != 'param_details'])
    passed_tests = sum(1 for k, v in test_results.items() if k != 'param_details' and v)
    
    print(f"âœ… æ¸¬è©¦é€šé: {passed_tests}/{total_tests}")
    
    if test_results.get('basic_functionality', False):
        print("âœ… åŸºæœ¬åŠŸèƒ½: PASS")
    else:
        print("âŒ åŸºæœ¬åŠŸèƒ½: FAIL")
    
    if test_results.get('lightweight', False):
        print("âœ… è¼•é‡åŒ–ç¶²è·¯: PASS")
    else:
        print("âŒ è¼•é‡åŒ–ç¶²è·¯: FAIL")
    
    if test_results.get('integration', False):
        print("âœ… éª¨å¹¹ç¶²è·¯æ•´åˆ: PASS")
    else:
        print("âŒ éª¨å¹¹ç¶²è·¯æ•´åˆ: FAIL")
    
    if test_results.get('gradient_flow', False):
        print("âœ… æ¢¯åº¦æµå‹•: PASS")
    else:
        print("âŒ æ¢¯åº¦æµå‹•: FAIL")
    
    if test_results.get('parameter_efficiency', False):
        print("âœ… åƒæ•¸æ•ˆç‡: PASS")
    else:
        print("âŒ åƒæ•¸æ•ˆç‡: FAIL")
    
    if test_results.get('inference_speed', False):
        print("âœ… æ¨ç†é€Ÿåº¦: PASS")
    else:
        print("âŒ æ¨ç†é€Ÿåº¦: FAIL")
    
    # æ¨è–¦é…ç½®
    if 'param_details' in test_results:
        print("\nğŸ¯ æ¨è–¦é…ç½®:")
        best_configs = [r for r in test_results['param_details'] if r['parameters'] < 1e6]
        if best_configs:
            best_config = max(best_configs, key=lambda x: x['out_channels'])
            print(f"  ğŸ† æœ€ä½³: {best_config['name']} ({best_config['parameters']:,} åƒæ•¸)")
        else:
            print("  âš ï¸ æ‰€æœ‰é…ç½®éƒ½è¶…é 1M åƒæ•¸é™åˆ¶")
    
    # æœ€çµ‚ç‹€æ…‹
    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼é ¸éƒ¨ç¶²è·¯å°±ç·’ã€‚")
        return True
    else:
        print(f"\nâš ï¸ {total_tests - passed_tests} å€‹æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦ä¿®å¾©ã€‚")
        return False


if __name__ == "__main__":
    # è¨­ç½® CUDA è¨­å‚™
    if torch.cuda.device_count() > 1:
        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
    
    print("ğŸ—ï¸ é ¸éƒ¨ç¶²è·¯æ¸¬è©¦è…³æœ¬")
    print(f"ğŸ“… æ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    print("\n" + "=" * 60)
    
    # é‹è¡Œæ¸¬è©¦
    results = run_comprehensive_tests()
    
    # æ‰“å°ç¸½çµ
    success = print_final_summary(results)
    
    # é€€å‡ºç¢¼
    sys.exit(0 if success else 1)