#!/usr/bin/env python3
"""
ä¾åºè¨“ç·´è…³æœ¬ - ä¿®å¾©ç‰ˆæœ¬
ä½¿ç”¨å¢å¼·çš„EWCé˜²éºå¿˜ç­–ç•¥ï¼Œç¢ºä¿éºå¿˜ç‡æ§åˆ¶åœ¨5%ä»¥å…§
"""
import os
import sys
import argparse
import json
import time
import torch
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.unified_model import create_unified_model
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.utils.sequential_trainer import create_sequential_trainer
from src.utils.ewc import create_ewc_handler


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='ä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´ - ä¿®å¾©ç‰ˆæœ¬')
    
    # åŸºæœ¬è¨­ç½®
    parser.add_argument('--model_config', type=str, default='default',
                       help='æ¨¡å‹é…ç½® (default, small, large)')
    parser.add_argument('--data_dir', type=str, default='./data',
                       help='æ•¸æ“šç›®éŒ„è·¯å¾‘')
    parser.add_argument('--save_dir', type=str, default='./sequential_training_results_fixed',
                       help='ä¿å­˜ç›®éŒ„è·¯å¾‘')
    
    # è¨“ç·´è¨­ç½®
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•¸æ“šåŠ è¼‰å·¥ä½œç·šç¨‹æ•¸')
    parser.add_argument('--device', type=str, default=None,
                       help='è¨ˆç®—è¨­å‚™ (cuda/cpu)')
    
    # éšæ®µè¨“ç·´è¨­ç½®
    parser.add_argument('--stage1_epochs', type=int, default=50,
                       help='éšæ®µ1 (åˆ†å‰²) è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--stage2_epochs', type=int, default=40,
                       help='éšæ®µ2 (æª¢æ¸¬) è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--stage3_epochs', type=int, default=30,
                       help='éšæ®µ3 (åˆ†é¡) è¨“ç·´è¼ªæ•¸')
    
    # EWCè¨­ç½® - ä½¿ç”¨æ›´é«˜çš„é è¨­å€¼
    parser.add_argument('--ewc_importance', type=float, default=50000.0,
                       help='EWCé‡è¦æ€§æ¬Šé‡ (å¤§å¹…æé«˜åˆ°50000)')
    parser.add_argument('--ewc_type', type=str, default='l2',
                       choices=['l2', 'online'],
                       help='EWCé¡å‹')
    parser.add_argument('--adaptive_ewc', action='store_true', default=True,
                       help='ä½¿ç”¨è‡ªé©æ‡‰EWCæ¬Šé‡ (é è¨­å•Ÿç”¨)')
    parser.add_argument('--forgetting_threshold', type=float, default=0.03,
                       help='å¯æ¥å—çš„éºå¿˜ç‡é–¾å€¼ (æ›´åš´æ ¼çš„3%)')
    
    # å„ªåŒ–å™¨è¨­ç½®
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                       help='åˆå§‹å­¸ç¿’ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                       help='æ¬Šé‡è¡°æ¸›')
    
    # å…¶ä»–è¨­ç½®
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--resume', type=str, default=None,
                       help='å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='è©³ç´°è¼¸å‡º')
    parser.add_argument('--debug', action='store_true',
                       help='èª¿è©¦æ¨¡å¼ (æ¸›å°‘è¨“ç·´è¼ªæ•¸)')
    parser.add_argument('--dry_run', action='store_true',
                       help='ç©ºé‹è¡Œ (ä¸ä¿å­˜çµæœ)')
    
    # æ–°å¢é¸é …
    parser.add_argument('--ewc_samples', type=int, default=100,
                       help='è¨ˆç®—FisherçŸ©é™£çš„æ¨£æœ¬æ•¸')
    parser.add_argument('--max_fisher_value', type=float, default=1e6,
                       help='Fisherå€¼ä¸Šé™')
    parser.add_argument('--gradient_clip', type=float, default=1.0,
                       help='æ¢¯åº¦è£å‰ªå€¼')
    
    args = parser.parse_args()
    
    # èª¿è©¦æ¨¡å¼è¨­ç½®
    if args.debug:
        print("ğŸ› èª¿è©¦æ¨¡å¼å•Ÿç”¨")
        args.stage1_epochs = min(args.stage1_epochs, 3)
        args.stage2_epochs = min(args.stage2_epochs, 3)
        args.stage3_epochs = min(args.stage3_epochs, 3)
        args.batch_size = 4
        args.num_workers = 0
    
    return args


def setup_environment(args):
    """è¨­ç½®è¨“ç·´ç’°å¢ƒ"""
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # è¨­ç½®è¨­å‚™
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    if not args.dry_run:
        os.makedirs(args.save_dir, exist_ok=True)
        
        # ä¿å­˜è¨“ç·´é…ç½®
        config_path = os.path.join(args.save_dir, 'training_config.json')
        with open(config_path, 'w') as f:
            json.dump(vars(args), f, indent=2)
    
    return args


def create_models_and_dataloaders(args):
    """å‰µå»ºæ¨¡å‹å’Œæ•¸æ“šåŠ è¼‰å™¨"""
    print("\nğŸ”§ å‰µå»ºæ¨¡å‹å’Œæ•¸æ“šåŠ è¼‰å™¨...")
    
    # å‰µå»ºçµ±ä¸€æ¨¡å‹
    model = create_unified_model(args.model_config)
    model = model.to(args.device)
    
    # çµ±è¨ˆæ¨¡å‹åƒæ•¸
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  ğŸ“Š ç¸½åƒæ•¸é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"  ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        sampling_strategy='balanced',
        task_weights=[1.0, 1.0, 1.0]
    )
    
    # çµ±è¨ˆæ•¸æ“šé›†å¤§å°
    # UnifiedDataLoaderä½¿ç”¨unified_datasetå±¬æ€§
    train_size = len(dataloaders['train'].unified_dataset)
    val_size = len(dataloaders['val'].unified_dataset)
    print(f"  ğŸ“Š è¨“ç·´é›†å¤§å°: {train_size}")
    print(f"  ğŸ“Š é©—è­‰é›†å¤§å°: {val_size}")
    
    return model, dataloaders


def create_enhanced_ewc_handler(model, args):
    """å‰µå»ºå¢å¼·çš„EWCè™•ç†å™¨"""
    print(f"\nğŸ›¡ï¸ å‰µå»ºå¢å¼·EWCè™•ç†å™¨...")
    print(f"  - åˆå§‹æ¬Šé‡: {args.ewc_importance}")
    print(f"  - EWCé¡å‹: {args.ewc_type}")
    print(f"  - è‡ªé©æ‡‰èª¿æ•´: {'å•Ÿç”¨' if args.adaptive_ewc else 'ç¦ç”¨'}")
    print(f"  - Fisherå€¼ä¸Šé™: {args.max_fisher_value}")
    print(f"  - éºå¿˜ç‡é–¾å€¼: {args.forgetting_threshold:.1%}")
    
    ewc = create_ewc_handler(
        model=model,
        importance=args.ewc_importance,
        ewc_type=args.ewc_type,
        max_fisher_value=args.max_fisher_value
    )
    
    return ewc


def create_enhanced_trainer(model, dataloaders, ewc, args):
    """å‰µå»ºå¢å¼·çš„ä¾åºè¨“ç·´å™¨"""
    # æº–å‚™è¨“ç·´å™¨æ•¸æ“šåŠ è¼‰å™¨ - ä½¿ç”¨SequentialTraineræœŸæœ›çš„éµåæ ¼å¼
    trainer_dataloaders = {
        'segmentation_train': dataloaders['train'].get_task_loader('segmentation'),
        'segmentation_val': dataloaders['val'].get_task_loader('segmentation'),
        'detection_train': dataloaders['train'].get_task_loader('detection'),
        'detection_val': dataloaders['val'].get_task_loader('detection'),
        'classification_train': dataloaders['train'].get_task_loader('classification'),
        'classification_val': dataloaders['val'].get_task_loader('classification')
    }
    
    # å‰µå»ºè¨“ç·´å™¨é…ç½® - åªåŒ…å«SequentialTraineræ”¯æŒçš„åƒæ•¸
    trainer_config = {
        'learning_rate': args.learning_rate,
        'device': args.device,
        'save_dir': args.save_dir if not args.dry_run else './checkpoints',
        'adaptive_ewc': args.adaptive_ewc,
        'forgetting_threshold': args.forgetting_threshold
    }
    
    # å‰µå»ºä¾åºè¨“ç·´å™¨
    trainer = create_sequential_trainer(
        model=model,
        dataloaders=trainer_dataloaders,
        ewc_importance=args.ewc_importance,
        **trainer_config
    )
    
    # ä½¿ç”¨æˆ‘å€‘çš„å¢å¼·EWCæ›¿æ›è¨“ç·´å™¨çš„EWC
    trainer.ewc = ewc
    
    return trainer


def train_with_enhanced_protection(trainer, args):
    """ä½¿ç”¨å¢å¼·ä¿è­·é€²è¡Œè¨“ç·´"""
    print("\nğŸš€ é–‹å§‹ä¾åºè¨“ç·´ï¼ˆå¢å¼·é˜²éºå¿˜ä¿è­·ï¼‰")
    print("="*60)
    
    start_time = time.time()
    results = {}
    
    # éšæ®µ1ï¼šåˆ†å‰²ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ1ï¼šåˆ†å‰²ä»»å‹™è¨“ç·´ ({args.stage1_epochs} epochs)")
    stage1_metrics = trainer.train_stage(
        stage_name='stage1_segmentation',
        task_type='segmentation',
        epochs=args.stage1_epochs
    )
    results['stage1'] = stage1_metrics
    
    # æª¢æŸ¥Stage 1æ€§èƒ½
    print(f"\nâœ… Stage 1 å®Œæˆ:")
    print(f"  - æœ€ä½³mIoU: {stage1_metrics.get('main_metric', stage1_metrics.get('miou', 0.0)):.4f}")
    stage1_time = time.time() - start_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage1_time:.1f}ç§’")
    
    # éšæ®µ2ï¼šæª¢æ¸¬ä»»å‹™ï¼ˆå¸¶EWCä¿è­·ï¼‰
    print(f"\nğŸ“Œ éšæ®µ2ï¼šæª¢æ¸¬ä»»å‹™è¨“ç·´ ({args.stage2_epochs} epochs) - EWCä¿è­·å•Ÿå‹•")
    print(f"  - ç•¶å‰EWCæ¬Šé‡: {trainer.ewc.adaptive_importance:.1f}")
    
    stage2_metrics = trainer.train_stage(
        stage_name='stage2_detection',
        task_type='detection',
        epochs=args.stage2_epochs
    )
    results['stage2'] = stage2_metrics
    
    # æª¢æŸ¥éºå¿˜æƒ…æ³
    print(f"\nâœ… Stage 2 å®Œæˆ:")
    print(f"  - æœ€ä½³mAP: {stage2_metrics.get('main_metric', stage2_metrics.get('map', 0.0)):.4f}")
    stage2_time = time.time() - start_time - stage1_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage2_time:.1f}ç§’")
    # è¨ˆç®—åˆ†å‰²ä»»å‹™çš„éºå¿˜ç‡
    with torch.no_grad():
        trainer.model.eval()
        current_seg_perf = trainer._validate_segmentation(trainer.dataloaders['segmentation_val'])
    seg_baseline = trainer.baseline_performance.get('segmentation', {}).get('main_metric', 0.0)
    seg_current = current_seg_perf.get('main_metric', 0.0)
    seg_forgetting = (seg_baseline - seg_current) / seg_baseline if seg_baseline > 0 else 0
    print(f"  - åˆ†å‰²ä»»å‹™éºå¿˜ç‡: {seg_forgetting:.2%} (å¾ {seg_baseline:.4f} åˆ° {seg_current:.4f})")
    
    if seg_forgetting > args.forgetting_threshold:
        print(f"  âš ï¸ éºå¿˜ç‡è¶…æ¨™ï¼èª¿æ•´å¾Œçš„EWCæ¬Šé‡: {trainer.ewc.adaptive_importance:.1f}")
    
    # éšæ®µ3ï¼šåˆ†é¡ä»»å‹™ï¼ˆå¸¶EWCä¿è­·ï¼‰
    print(f"\nğŸ“Œ éšæ®µ3ï¼šåˆ†é¡ä»»å‹™è¨“ç·´ ({args.stage3_epochs} epochs) - å®Œæ•´EWCä¿è­·")
    print(f"  - ç•¶å‰EWCæ¬Šé‡: {trainer.ewc.adaptive_importance:.1f}")
    
    stage3_metrics = trainer.train_stage(
        stage_name='stage3_classification',
        task_type='classification',
        epochs=args.stage3_epochs
    )
    results['stage3'] = stage3_metrics
    
    # æœ€çµ‚è©•ä¼°
    total_time = time.time() - start_time
    print(f"\nâœ… Stage 3 å®Œæˆ:")
    print(f"  - æœ€ä½³æº–ç¢ºç‡: {stage3_metrics.get('main_metric', stage3_metrics.get('accuracy', 0.0)):.4f}")
    stage3_time = time.time() - start_time - stage1_time - stage2_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage3_time:.1f}ç§’")
    
    # è¨ˆç®—ç¸½éºå¿˜ç‡
    total_forgetting = 0.0
    num_tasks = 0
    for task in trainer.completed_tasks[:-1]:  # ä¸åŒ…æ‹¬æœ€å¾Œä¸€å€‹ä»»å‹™
        if task in trainer.baseline_performance:
            with torch.no_grad():
                trainer.model.eval()
                if task == 'segmentation':
                    current_perf = trainer._validate_segmentation(trainer.dataloaders[f'{task}_val'])
                elif task == 'detection':
                    current_perf = trainer._validate_detection(trainer.dataloaders[f'{task}_val'])
                elif task == 'classification':
                    current_perf = trainer._validate_classification(trainer.dataloaders[f'{task}_val'])
            baseline = trainer.baseline_performance[task].get('main_metric', 0.0)
            current = current_perf.get('main_metric', 0.0)
            forgetting = (baseline - current) / baseline if baseline > 0 else 0
            total_forgetting += forgetting
            num_tasks += 1
    total_forgetting = total_forgetting / num_tasks if num_tasks > 0 else 0
    
    # æ‰“å°è¨“ç·´ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ“Š è¨“ç·´ç¸½çµ")
    print("="*60)
    print(f"ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print(f"ç¸½éºå¿˜ç‡: {total_forgetting:.2%}")
    print(f"æœ€çµ‚EWCæ¬Šé‡: {trainer.ewc.adaptive_importance:.1f}")
    
    # å„ä»»å‹™æœ€çµ‚æ€§èƒ½
    print("\nå„ä»»å‹™æœ€çµ‚æ€§èƒ½:")
    final_metrics = {}
    for task in trainer.completed_tasks:
        if f'{task}_val' in trainer.dataloaders:
            with torch.no_grad():
                trainer.model.eval()
                if task == 'segmentation':
                    perf = trainer._validate_segmentation(trainer.dataloaders[f'{task}_val'])
                elif task == 'detection':
                    perf = trainer._validate_detection(trainer.dataloaders[f'{task}_val'])
                elif task == 'classification':
                    perf = trainer._validate_classification(trainer.dataloaders[f'{task}_val'])
            final_metrics[task] = perf.get('main_metric', 0.0)
            print(f"  - {task}: {final_metrics[task]:.4f}")
    
    # æª¢æŸ¥æ˜¯å¦æ»¿è¶³è¦æ±‚
    if total_forgetting <= args.forgetting_threshold:
        print(f"\nâœ… æˆåŠŸï¼éºå¿˜ç‡ {total_forgetting:.2%} â‰¤ {args.forgetting_threshold:.1%}")
    else:
        print(f"\nâŒ å¤±æ•—ï¼éºå¿˜ç‡ {total_forgetting:.2%} > {args.forgetting_threshold:.1%}")
        print(f"\nå»ºè­°:")
        print(f"  1. ä½¿ç”¨æ›´é«˜çš„EWCæ¬Šé‡: {args.ewc_importance * 2}")
        print(f"  2. å•Ÿç”¨Online EWCæ¸›å°‘è¨ˆç®—é–‹éŠ·")
        print(f"  3. å¢åŠ FisherçŸ©é™£è¨ˆç®—æ¨£æœ¬æ•¸")
    
    # ä¿å­˜æœ€çµ‚çµæœ
    if not args.dry_run:
        results['summary'] = {
            'total_time': total_time,
            'total_forgetting': total_forgetting,
            'final_ewc_weight': trainer.ewc.adaptive_importance,
            'final_metrics': final_metrics,
            'success': total_forgetting <= args.forgetting_threshold,
            'stage_times': {
                'stage1': stage1_time,
                'stage2': stage2_time,
                'stage3': stage3_time
            }
        }
        
        results_path = os.path.join(args.save_dir, 'training_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {results_path}")
    
    return results


def visualize_results(results, args):
    """å¯è¦–åŒ–è¨“ç·´çµæœ"""
    if args.dry_run:
        return
    
    print("\nğŸ“Š ç”Ÿæˆè¨“ç·´æ›²ç·š...")
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å¾è¨“ç·´æ­·å²æå–æ•¸æ“š
    history_path = os.path.join(args.save_dir, 'training_history.json')
    if os.path.exists(history_path):
        with open(history_path, 'r') as f:
            history = json.load(f)
        
        # ç¹ªè£½æå¤±æ›²ç·š
        ax = axes[0, 0]
        for stage in ['stage1', 'stage2', 'stage3']:
            if stage in history:
                epochs = range(1, len(history[stage]['loss']) + 1)
                ax.plot(epochs, history[stage]['loss'], label=f'{stage} loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title('Training Loss')
        ax.legend()
        ax.grid(True)
        
        # ç¹ªè£½æŒ‡æ¨™æ›²ç·š
        ax = axes[0, 1]
        metrics_map = {
            'stage1': ('mIoU', 'mIoU'),
            'stage2': ('mAP', 'mAP'),
            'stage3': ('accuracy', 'Accuracy')
        }
        
        for stage, (metric_key, label) in metrics_map.items():
            if stage in history and metric_key in history[stage]:
                epochs = range(1, len(history[stage][metric_key]) + 1)
                ax.plot(epochs, history[stage][metric_key], label=f'{stage} {label}')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Metric')
        ax.set_title('Validation Metrics')
        ax.legend()
        ax.grid(True)
        
        # ç¹ªè£½EWCæ¬Šé‡è®ŠåŒ–
        ax = axes[1, 0]
        if 'ewc_importance' in history:
            epochs = range(1, len(history['ewc_importance']) + 1)
            ax.plot(epochs, history['ewc_importance'], 'r-', linewidth=2)
        ax.set_xlabel('Training Progress')
        ax.set_ylabel('EWC Weight')
        ax.set_title('Adaptive EWC Weight')
        ax.grid(True)
        
        # ç¹ªè£½éºå¿˜ç‡
        ax = axes[1, 1]
        if 'forgetting_rates' in history:
            stages = list(history['forgetting_rates'].keys())
            rates = list(history['forgetting_rates'].values())
            colors = ['green' if r <= args.forgetting_threshold else 'red' for r in rates]
            bars = ax.bar(stages, rates, color=colors)
            ax.axhline(y=args.forgetting_threshold, color='black', linestyle='--', 
                      label=f'Threshold ({args.forgetting_threshold:.1%})')
            ax.set_ylabel('Forgetting Rate')
            ax.set_title('Task Forgetting Rates')
            ax.legend()
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, rate in zip(bars, rates):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{rate:.1%}', ha='center', va='bottom')
    
    plt.tight_layout()
    save_path = os.path.join(args.save_dir, 'training_curves.png')
    plt.savefig(save_path, dpi=150)
    print(f"  ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜åˆ°: {save_path}")
    plt.close()


def main():
    """ä¸»å‡½æ•¸"""
    # è§£æåƒæ•¸
    args = parse_args()
    
    # è¨­ç½®ç’°å¢ƒ
    args = setup_environment(args)
    
    print("ğŸš€ å•Ÿå‹•ä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´ï¼ˆä¿®å¾©ç‰ˆæœ¬ï¼‰")
    print("="*60)
    print(f"è¨“ç·´é…ç½®:")
    print(f"  - æ¨¡å‹: {args.model_config}")
    print(f"  - è¨­å‚™: {args.device}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - EWCæ¬Šé‡: {args.ewc_importance}")
    print(f"  - è‡ªé©æ‡‰EWC: {'æ˜¯' if args.adaptive_ewc else 'å¦'}")
    print(f"  - éºå¿˜ç‡é–¾å€¼: {args.forgetting_threshold:.1%}")
    print(f"  - è¨“ç·´è¼ªæ•¸: {args.stage1_epochs}/{args.stage2_epochs}/{args.stage3_epochs}")
    print("="*60)
    
    # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
    model, dataloaders = create_models_and_dataloaders(args)
    
    # å‰µå»ºå¢å¼·çš„EWCè™•ç†å™¨
    ewc = create_enhanced_ewc_handler(model, args)
    
    # å‰µå»ºå¢å¼·çš„è¨“ç·´å™¨
    trainer = create_enhanced_trainer(model, dataloaders, ewc, args)
    
    # é–‹å§‹è¨“ç·´
    results = train_with_enhanced_protection(trainer, args)
    
    # å¯è¦–åŒ–çµæœ
    visualize_results(results, args)
    
    print("\nâœ¨ è¨“ç·´å®Œæˆï¼")
    
    # è¿”å›æˆåŠŸç‹€æ…‹
    return 0 if results['summary']['success'] else 1


if __name__ == "__main__":
    exit(main())