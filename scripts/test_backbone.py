#!/usr/bin/env python3
"""
éª¨å¹¹ç¶²è·¯æ¸¬è©¦è…³æœ¬
æ¸¬è©¦ç‰¹å¾µæå–ã€æ¨ç†é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨
"""
import os
import sys
import time
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.profiler import profile, ProfilerActivity
import psutil
import gc
import numpy as np
from typing import Dict, List

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.backbone import create_backbone, BackboneNetwork

class BackboneTester:
    """éª¨å¹¹ç¶²è·¯æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        
    def print_header(self, title):
        """æ‰“å°æ¸¬è©¦æ¨™é¡Œ"""
        print(f"\n{'='*70}")
        print(f"{title:^70}")
        print(f"{'='*70}")
    
    def test_feature_extraction(self, model_name='mobilenetv3_small'):
        """æ¸¬è©¦ç‰¹å¾µæå–åŠŸèƒ½"""
        self.print_header(f"ç‰¹å¾µæå–æ¸¬è©¦ - {model_name}")
        
        # å‰µå»ºæ¨¡å‹
        backbone = create_backbone(model_name, pretrained=False)
        backbone = backbone.to(self.device)
        backbone.eval()
        
        test_results = {}
        
        # æ¸¬è©¦ä¸åŒè¼¸å…¥å°ºå¯¸
        input_sizes = [224, 320, 512]
        
        for size in input_sizes:
            print(f"\nğŸ“ æ¸¬è©¦è¼¸å…¥å°ºå¯¸: {size}x{size}")
            
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            x = torch.randn(1, 3, size, size).to(self.device)
            
            with torch.no_grad():
                try:
                    features = backbone(x)
                    
                    size_results = {
                        'input_size': (size, size),
                        'features': {}
                    }
                    
                    # æª¢æŸ¥ç‰¹å¾µåœ–å°ºå¯¸
                    for layer_name, feature in features.items():
                        expected_stride = backbone.get_feature_info()['feature_strides'][layer_name]
                        expected_size = size // expected_stride
                        actual_size = feature.shape[-1]
                        
                        size_results['features'][layer_name] = {
                            'shape': list(feature.shape),
                            'expected_size': expected_size,
                            'actual_size': actual_size,
                            'correct': expected_size == actual_size
                        }
                        
                        status = "âœ…" if expected_size == actual_size else "âŒ"
                        print(f"  {layer_name}: {feature.shape} {status}")
                        if expected_size != actual_size:
                            print(f"    æœŸæœ›å°ºå¯¸: {expected_size}, å¯¦éš›å°ºå¯¸: {actual_size}")
                    
                    test_results[f'size_{size}'] = size_results
                    
                except Exception as e:
                    print(f"âŒ å°ºå¯¸ {size} æ¸¬è©¦å¤±æ•—: {e}")
                    test_results[f'size_{size}'] = {'error': str(e)}
        
        # æ¸¬è©¦åƒæ•¸æ•¸é‡
        param_info = backbone.get_parameter_count()
        feature_info = backbone.get_feature_info()
        
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  ç¸½åƒæ•¸é‡: {param_info['total_parameters']:,}")
        print(f"  å¯è¨“ç·´åƒæ•¸: {param_info['trainable_parameters']:,}")
        print(f"  ç‰¹å¾µé€šé“æ•¸: {feature_info['feature_channels']}")
        
        self.results[f'{model_name}_features'] = {
            'test_results': test_results,
            'parameter_info': param_info,
            'feature_info': feature_info
        }
        
        return test_results
    
    def test_inference_speed(self, model_name='mobilenetv3_small', num_runs=100):
        """æ¸¬è©¦æ¨ç†é€Ÿåº¦"""
        self.print_header(f"æ¨ç†é€Ÿåº¦æ¸¬è©¦ - {model_name}")
        
        # å‰µå»ºæ¨¡å‹
        backbone = create_backbone(model_name, pretrained=False)
        backbone = backbone.to(self.device)
        backbone.eval()
        
        # æ¸¬è©¦ä¸åŒè¼¸å…¥å°ºå¯¸çš„æ¨ç†é€Ÿåº¦
        input_sizes = [224, 320, 512]
        batch_sizes = [1, 4, 8]
        
        speed_results = {}
        
        for size in input_sizes:
            for batch_size in batch_sizes:
                print(f"\nâ±ï¸  æ¸¬è©¦: {batch_size}x{size}x{size}")
                
                # å‰µå»ºæ¸¬è©¦æ•¸æ“š
                x = torch.randn(batch_size, 3, size, size).to(self.device)
                
                # é ç†±
                with torch.no_grad():
                    for _ in range(10):
                        _ = backbone(x)
                
                # åŒæ­¥GPU
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                # æ¸¬è©¦æ¨ç†æ™‚é–“
                times = []
                with torch.no_grad():
                    for _ in range(num_runs):
                        start_time = time.time()
                        features = backbone(x)
                        
                        if self.device.type == 'cuda':
                            torch.cuda.synchronize()
                        
                        end_time = time.time()
                        times.append((end_time - start_time) * 1000)  # è½‰æ›ç‚ºæ¯«ç§’
                
                # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
                times = np.array(times)
                mean_time = np.mean(times)
                std_time = np.std(times)
                min_time = np.min(times)
                max_time = np.max(times)
                
                # è¨ˆç®—ååé‡
                throughput = (batch_size * 1000) / mean_time  # samples/sec
                
                result_key = f'batch{batch_size}_size{size}'
                speed_results[result_key] = {
                    'batch_size': batch_size,
                    'input_size': size,
                    'mean_time_ms': mean_time,
                    'std_time_ms': std_time,
                    'min_time_ms': min_time,
                    'max_time_ms': max_time,
                    'throughput_samples_per_sec': throughput
                }
                
                print(f"  å¹³å‡æ™‚é–“: {mean_time:.2f} Â± {std_time:.2f} ms")
                print(f"  ç¯„åœ: {min_time:.2f} - {max_time:.2f} ms")
                print(f"  ååé‡: {throughput:.1f} samples/sec")
                
                # æª¢æŸ¥æ˜¯å¦ç¬¦åˆ150msé™åˆ¶ï¼ˆå–®å€‹æ¨£æœ¬ï¼‰
                single_sample_time = mean_time / batch_size
                if single_sample_time <= 150:
                    print(f"  âœ… ç¬¦åˆ150msé™åˆ¶ ({single_sample_time:.2f}ms/sample)")
                else:
                    print(f"  âŒ è¶…é150msé™åˆ¶ ({single_sample_time:.2f}ms/sample)")
        
        self.results[f'{model_name}_speed'] = speed_results
        return speed_results
    
    def test_memory_usage(self, model_name='mobilenetv3_small'):
        """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨"""
        self.print_header(f"è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦ - {model_name}")
        
        # æ¸…ç†è¨˜æ†¶é«”
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
        
        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024**3  # GB
        
        if self.device.type == 'cuda':
            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        else:
            initial_gpu_memory = 0
        
        print(f"åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨:")
        print(f"  CPU: {initial_memory:.3f} GB")
        print(f"  GPU: {initial_gpu_memory:.3f} GB")
        
        # å‰µå»ºæ¨¡å‹
        backbone = create_backbone(model_name, pretrained=False)
        backbone = backbone.to(self.device)
        
        # è¨˜éŒ„æ¨¡å‹è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”
        model_memory = process.memory_info().rss / 1024**3
        if self.device.type == 'cuda':
            model_gpu_memory = torch.cuda.memory_allocated() / 1024**3
        else:
            model_gpu_memory = 0
        
        print(f"\næ¨¡å‹è¼‰å…¥å¾Œ:")
        print(f"  CPU: {model_memory:.3f} GB (+{model_memory-initial_memory:.3f})")
        print(f"  GPU: {model_gpu_memory:.3f} GB (+{model_gpu_memory-initial_gpu_memory:.3f})")
        
        # æ¸¬è©¦ä¸åŒbatch sizeçš„è¨˜æ†¶é«”ä½¿ç”¨
        batch_sizes = [1, 4, 8, 16]
        memory_results = {}
        
        for batch_size in batch_sizes:
            print(f"\nğŸ” æ¸¬è©¦ batch_size={batch_size}")
            
            # æ¸…ç†è¨˜æ†¶é«”
            gc.collect()
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            # å‰µå»ºæ¸¬è©¦æ•¸æ“š
            x = torch.randn(batch_size, 3, 512, 512).to(self.device)
            
            # å‰å‘å‚³æ’­
            with torch.no_grad():
                features = backbone(x)
            
            # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨
            inference_memory = process.memory_info().rss / 1024**3
            if self.device.type == 'cuda':
                inference_gpu_memory = torch.cuda.memory_allocated() / 1024**3
                peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**3
            else:
                inference_gpu_memory = 0
                peak_gpu_memory = 0
            
            memory_results[f'batch_{batch_size}'] = {
                'batch_size': batch_size,
                'cpu_memory_gb': inference_memory,
                'gpu_memory_gb': inference_gpu_memory,
                'peak_gpu_memory_gb': peak_gpu_memory,
                'memory_increase_gb': inference_memory - model_memory
            }
            
            print(f"  CPU: {inference_memory:.3f} GB")
            print(f"  GPU: {inference_gpu_memory:.3f} GB (å³°å€¼: {peak_gpu_memory:.3f} GB)")
            
            # æ¸…ç†ç‰¹å¾µåœ–
            del features, x
        
        self.results[f'{model_name}_memory'] = memory_results
        return memory_results
    
    def test_pretrained_loading(self, model_name='mobilenetv3_small'):
        """æ¸¬è©¦é è¨“ç·´æ¬Šé‡è¼‰å…¥"""
        self.print_header(f"é è¨“ç·´æ¬Šé‡æ¸¬è©¦ - {model_name}")
        
        try:
            # æ¸¬è©¦ä¸åŠ è¼‰é è¨“ç·´æ¬Šé‡
            print("æ¸¬è©¦éš¨æ©Ÿåˆå§‹åŒ–...")
            backbone_random = create_backbone(model_name, pretrained=False)
            
            # æ¸¬è©¦åŠ è¼‰é è¨“ç·´æ¬Šé‡
            print("\næ¸¬è©¦é è¨“ç·´æ¬Šé‡è¼‰å…¥...")
            backbone_pretrained = create_backbone(model_name, pretrained=True)
            
            # æ¯”è¼ƒæ¬Šé‡
            random_params = dict(backbone_random.named_parameters())
            pretrained_params = dict(backbone_pretrained.named_parameters())
            
            different_params = 0
            total_params = 0
            
            for name in random_params.keys():
                if name in pretrained_params:
                    if not torch.equal(random_params[name], pretrained_params[name]):
                        different_params += 1
                    total_params += 1
            
            print(f"\næ¬Šé‡æ¯”è¼ƒçµæœ:")
            print(f"  ç¸½åƒæ•¸å±¤æ•¸: {total_params}")
            print(f"  ä¸åŒçš„åƒæ•¸å±¤: {different_params}")
            print(f"  æ¬Šé‡è¼‰å…¥ç‡: {different_params/total_params*100:.1f}%")
            
            return True
            
        except Exception as e:
            print(f"âŒ é è¨“ç·´æ¬Šé‡æ¸¬è©¦å¤±æ•—: {e}")
            return False
    
    def run_comprehensive_test(self, model_names=['mobilenetv3_small']):
        """é‹è¡Œå®Œæ•´æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹éª¨å¹¹ç¶²è·¯ç¶œåˆæ¸¬è©¦")
        print(f"ğŸ”§ è¨­å‚™: {self.device}")
        
        all_passed = True
        
        for model_name in model_names:
            print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å‹: {model_name}")
            
            try:
                # ç‰¹å¾µæå–æ¸¬è©¦
                feature_results = self.test_feature_extraction(model_name)
                
                # æ¨ç†é€Ÿåº¦æ¸¬è©¦  
                speed_results = self.test_inference_speed(model_name, num_runs=50)
                
                # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
                memory_results = self.test_memory_usage(model_name)
                
                # é è¨“ç·´æ¬Šé‡æ¸¬è©¦
                pretrained_ok = self.test_pretrained_loading(model_name)
                
                # æª¢æŸ¥æ˜¯å¦é€šéåŸºæœ¬è¦æ±‚
                param_count = self.results[f'{model_name}_features']['parameter_info']['total_parameters']
                
                print(f"\nğŸ“‹ {model_name} æ¸¬è©¦ç¸½çµ:")
                print(f"  åƒæ•¸æ•¸é‡: {param_count:,} {'âœ…' if param_count < 8_000_000 else 'âŒ'}")
                print(f"  é è¨“ç·´è¼‰å…¥: {'âœ…' if pretrained_ok else 'âŒ'}")
                
                # æª¢æŸ¥é€Ÿåº¦è¦æ±‚
                speed_ok = True
                for key, result in speed_results.items():
                    if 'batch1' in key and 'size512' in key:
                        single_time = result['mean_time_ms']
                        if single_time > 150:
                            speed_ok = False
                            break
                
                print(f"  é€Ÿåº¦è¦æ±‚ (<150ms): {'âœ…' if speed_ok else 'âŒ'}")
                
                if param_count >= 8_000_000 or not speed_ok:
                    all_passed = False
                    
            except Exception as e:
                print(f"âŒ {model_name} æ¸¬è©¦å¤±æ•—: {e}")
                all_passed = False
        
        return all_passed
    
    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        self.print_header("æ¸¬è©¦å ±å‘Š")
        
        print("ğŸ“Š è©³ç´°çµæœå·²ä¿å­˜åˆ° self.results")
        print("ğŸ¯ é—œéµæŒ‡æ¨™:")
        
        for model_name in ['mobilenetv3_small']:
            if f'{model_name}_features' in self.results:
                param_info = self.results[f'{model_name}_features']['parameter_info']
                print(f"\n{model_name}:")
                print(f"  ç¸½åƒæ•¸: {param_info['total_parameters']:,}")
                
                if f'{model_name}_speed' in self.results:
                    speed_info = self.results[f'{model_name}_speed']
                    if 'batch1_size512' in speed_info:
                        time_512 = speed_info['batch1_size512']['mean_time_ms']
                        print(f"  512x512æ¨ç†æ™‚é–“: {time_512:.2f}ms")
                
                if f'{model_name}_memory' in self.results:
                    memory_info = self.results[f'{model_name}_memory']
                    if 'batch_1' in memory_info:
                        gpu_mem = memory_info['batch_1']['gpu_memory_gb']
                        print(f"  GPUè¨˜æ†¶é«”ä½¿ç”¨: {gpu_mem:.3f}GB")


def main():
    """ä¸»å‡½æ•¸"""
    tester = BackboneTester()
    
    # é‹è¡Œç¶œåˆæ¸¬è©¦
    success = tester.run_comprehensive_test(['mobilenetv3_small'])
    
    # ç”Ÿæˆå ±å‘Š
    tester.generate_report()
    
    if success:
        print("\nâœ… éª¨å¹¹ç¶²è·¯å¯¦ç¾å®Œæˆï¼")
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼Œæº–å‚™é€²å…¥ä¸‹ä¸€éšæ®µ")
    else:
        print("\nâŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦å„ªåŒ–")
    
    return success


if __name__ == "__main__":
    main()