#!/usr/bin/env python3
"""
æœ€çµ‚ç‰ˆä¾åºè¨“ç·´è…³æœ¬ - å¼·åŒ–é˜²éºå¿˜ç­–ç•¥
æ¡ç”¨æ›´æ¿€é€²çš„åƒæ•¸å‡çµå’Œå­¸ç¿’ç‡ç­–ç•¥
"""
import os
import sys
import argparse
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from datetime import datetime
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.unified_model import create_unified_model
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.utils.sequential_trainer import create_sequential_trainer
from src.utils.ewc import create_ewc_handler


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='æœ€çµ‚ç‰ˆä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´')
    
    # åŸºæœ¬è¨­ç½®
    parser.add_argument('--data_dir', type=str, default='./data')
    parser.add_argument('--save_dir', type=str, default='./final_results')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--num_workers', type=int, default=4)
    
    # è¨“ç·´è¼ªæ•¸
    parser.add_argument('--stage1_epochs', type=int, default=30)
    parser.add_argument('--stage2_epochs', type=int, default=20)
    parser.add_argument('--stage3_epochs', type=int, default=15)
    
    # é˜²éºå¿˜ç­–ç•¥åƒæ•¸
    parser.add_argument('--ewc_importance', type=float, default=10000.0)
    parser.add_argument('--stage1_lr', type=float, default=1e-3)
    parser.add_argument('--stage2_lr', type=float, default=1e-5)  # æ¥µä½å­¸ç¿’ç‡
    parser.add_argument('--stage3_lr', type=float, default=5e-6)  # æ›´ä½å­¸ç¿’ç‡
    parser.add_argument('--weight_decay', type=float, default=5e-3)  # å¼·æ­£å‰‡åŒ–
    
    # å…¶ä»–åƒæ•¸
    parser.add_argument('--device', type=str, default=None)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--debug', action='store_true')
    
    args = parser.parse_args()
    
    if args.debug:
        args.stage1_epochs = 5
        args.stage2_epochs = 3
        args.stage3_epochs = 2
        args.batch_size = 8
    
    return args


def aggressive_freeze_strategy(model, stage):
    """æ¿€é€²çš„åƒæ•¸å‡çµç­–ç•¥"""
    frozen_params = 0
    total_params = 0
    
    if stage == 1:
        # Stage 1: ä¸å‡çµä»»ä½•åƒæ•¸
        return model
    
    elif stage == 2:
        # Stage 2: å‡çµéª¨å¹¹ç¶²è·¯çš„å‰60%å±¤
        backbone_params = list(model.backbone.named_parameters())
        freeze_count = int(len(backbone_params) * 0.6)
        
        for idx, (name, param) in enumerate(backbone_params):
            total_params += 1
            if idx < freeze_count:
                param.requires_grad = False
                frozen_params += 1
        
        # åŒæ™‚å‡çµåˆ†å‰²é ­éƒ¨
        for name, param in model.head.named_parameters():
            if 'segmentation' in name:
                param.requires_grad = False
                frozen_params += 1
            total_params += 1
    
    elif stage == 3:
        # Stage 3: å‡çµæ‰€æœ‰éª¨å¹¹ç¶²è·¯å’Œå·²è¨“ç·´çš„ä»»å‹™é ­éƒ¨
        # å‡çµæ•´å€‹éª¨å¹¹ç¶²è·¯
        for name, param in model.backbone.named_parameters():
            param.requires_grad = False
            frozen_params += 1
            total_params += 1
        
        # å‡çµå…±äº«å±¤
        for name, param in model.head.named_parameters():
            if 'shared' in name:
                param.requires_grad = False
                frozen_params += 1
            total_params += 1
        
        # å‡çµåˆ†å‰²å’Œæª¢æ¸¬é ­éƒ¨
        for name, param in model.head.named_parameters():
            if 'segmentation' in name or 'detection' in name:
                param.requires_grad = False
                frozen_params += 1
    
    # çµ±è¨ˆå¯è¨“ç·´åƒæ•¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_model_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nğŸ”’ éšæ®µ {stage} åƒæ•¸å‡çµç­–ç•¥:")
    print(f"  - å‡çµåƒæ•¸æ•¸é‡: {frozen_params}")
    print(f"  - å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_model_params:,}")
    print(f"  - å¯è¨“ç·´æ¯”ä¾‹: {trainable_params/total_model_params*100:.1f}%")
    
    return model


def create_stage_optimizer(model, stage, base_lr, weight_decay):
    """ç‚ºæ¯å€‹éšæ®µå‰µå»ºç‰¹å®šçš„å„ªåŒ–å™¨"""
    # åªå„ªåŒ–æœªå‡çµçš„åƒæ•¸
    params_to_optimize = [p for p in model.parameters() if p.requires_grad]
    
    if stage == 1:
        # Stage 1: æ¨™æº–å„ªåŒ–å™¨
        optimizer = optim.AdamW(params_to_optimize, lr=base_lr, weight_decay=weight_decay)
    elif stage == 2:
        # Stage 2: æ›´ä½çš„å­¸ç¿’ç‡ï¼Œæ›´é«˜çš„å‹•é‡
        optimizer = optim.SGD(params_to_optimize, lr=base_lr, 
                            momentum=0.95, weight_decay=weight_decay, nesterov=True)
    else:
        # Stage 3: æ¥µä½å­¸ç¿’ç‡çš„SGD
        optimizer = optim.SGD(params_to_optimize, lr=base_lr, 
                            momentum=0.99, weight_decay=weight_decay)
    
    return optimizer


def main():
    args = parse_args()
    
    # è¨­ç½®ç’°å¢ƒ
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    print("ğŸš€ æœ€çµ‚ç‰ˆä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´")
    print("="*60)
    print("å¼·åŒ–é˜²éºå¿˜ç­–ç•¥:")
    print(f"  1. æ¿€é€²åƒæ•¸å‡çµ")
    print(f"  2. æ¥µä½å­¸ç¿’ç‡: {args.stage1_lr} â†’ {args.stage2_lr} â†’ {args.stage3_lr}")
    print(f"  3. å¼·æ­£å‰‡åŒ–: weight_decay={args.weight_decay}")
    print(f"  4. é«˜EWCæ¬Šé‡: {args.ewc_importance}")
    print("="*60)
    
    # å‰µå»ºæ¨¡å‹
    print("\nğŸ“¦ å‰µå»ºæ¨¡å‹...")
    model = create_unified_model('default')
    model = model.to(args.device)
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    print("\nğŸ“Š å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨...")
    dataloaders = create_unified_dataloaders(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        sampling_strategy='balanced'
    )
    
    # æº–å‚™è¨“ç·´å™¨æ•¸æ“š
    trainer_dataloaders = {
        'segmentation_train': dataloaders['train'].get_task_loader('segmentation'),
        'segmentation_val': dataloaders['val'].get_task_loader('segmentation'),
        'detection_train': dataloaders['train'].get_task_loader('detection'),
        'detection_val': dataloaders['val'].get_task_loader('detection'),
        'classification_train': dataloaders['train'].get_task_loader('classification'),
        'classification_val': dataloaders['val'].get_task_loader('classification')
    }
    
    # å‰µå»ºEWCè™•ç†å™¨
    print(f"\nğŸ›¡ï¸ å‰µå»ºEWCè™•ç†å™¨ (importance={args.ewc_importance})...")
    ewc = create_ewc_handler(
        model=model,
        importance=args.ewc_importance,
        ewc_type='l2'
    )
    
    # è¨˜éŒ„çµæœ
    results = {
        'config': vars(args),
        'stages': {},
        'forgetting_rates': {}
    }
    
    # Stage 1: åˆ†å‰²ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ1ï¼šåˆ†å‰²ä»»å‹™ ({args.stage1_epochs} epochs)")
    print(f"  å­¸ç¿’ç‡: {args.stage1_lr}")
    
    # ä¸å‡çµä»»ä½•åƒæ•¸
    model = aggressive_freeze_strategy(model, stage=1)
    
    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = create_stage_optimizer(model, 1, args.stage1_lr, args.weight_decay)
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = create_sequential_trainer(
        model=model,
        dataloaders=trainer_dataloaders,
        ewc_importance=args.ewc_importance,
        learning_rate=args.stage1_lr,
        device=args.device,
        save_dir=args.save_dir,
        adaptive_ewc=False,  # é—œé–‰è‡ªé©æ‡‰ï¼Œä½¿ç”¨å›ºå®šæ¬Šé‡
        forgetting_threshold=0.05
    )
    
    # ä½¿ç”¨æˆ‘å€‘çš„EWCå’Œå„ªåŒ–å™¨
    trainer.ewc = ewc
    trainer.optimizer = optimizer
    
    # è¨“ç·´Stage 1
    stage1_start = time.time()
    stage1_metrics = trainer.train_stage(
        stage_name='stage1_segmentation',
        task_type='segmentation',
        epochs=args.stage1_epochs
    )
    stage1_time = time.time() - stage1_start
    
    results['stages']['segmentation'] = {
        'metrics': stage1_metrics,
        'time': stage1_time,
        'best_score': stage1_metrics.get('best_metric', 0)
    }
    
    print(f"\nâœ… Stage 1 å®Œæˆ!")
    print(f"  æœ€ä½³mIoU: {stage1_metrics.get('best_metric', 0):.4f}")
    print(f"  è¨“ç·´æ™‚é–“: {stage1_time/60:.1f}åˆ†é˜")
    
    # ä¿å­˜Stage 1çš„åŸºæº–æ€§èƒ½
    baseline_seg = stage1_metrics.get('best_metric', 0)
    
    # Stage 2: æª¢æ¸¬ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ2ï¼šæª¢æ¸¬ä»»å‹™ ({args.stage2_epochs} epochs)")
    print(f"  å­¸ç¿’ç‡: {args.stage2_lr} (é™ä½{args.stage1_lr/args.stage2_lr:.0f}å€)")
    
    # æ¿€é€²å‡çµç­–ç•¥
    model = aggressive_freeze_strategy(model, stage=2)
    
    # å‰µå»ºæ–°å„ªåŒ–å™¨ï¼ˆåªå„ªåŒ–æœªå‡çµåƒæ•¸ï¼‰
    optimizer = create_stage_optimizer(model, 2, args.stage2_lr, args.weight_decay)
    trainer.optimizer = optimizer
    
    # è¨“ç·´Stage 2
    stage2_start = time.time()
    stage2_metrics = trainer.train_stage(
        stage_name='stage2_detection',
        task_type='detection',
        epochs=args.stage2_epochs
    )
    stage2_time = time.time() - stage2_start
    
    results['stages']['detection'] = {
        'metrics': stage2_metrics,
        'time': stage2_time,
        'best_score': stage2_metrics.get('best_metric', 0)
    }
    
    # æª¢æŸ¥åˆ†å‰²ä»»å‹™éºå¿˜
    with torch.no_grad():
        model.eval()
        seg_perf = trainer._validate_segmentation(trainer_dataloaders['segmentation_val'])
        current_seg = seg_perf.get('main_metric', seg_perf.get('miou', 0))
    
    seg_forgetting = (baseline_seg - current_seg) / baseline_seg if baseline_seg > 0 else 0
    results['forgetting_rates']['segmentation_after_detection'] = seg_forgetting
    
    print(f"\nâœ… Stage 2 å®Œæˆ!")
    print(f"  æœ€ä½³mAP: {stage2_metrics.get('best_metric', 0):.4f}")
    print(f"  åˆ†å‰²éºå¿˜ç‡: {seg_forgetting:.2%}")
    print(f"  è¨“ç·´æ™‚é–“: {stage2_time/60:.1f}åˆ†é˜")
    
    # Stage 3: åˆ†é¡ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ3ï¼šåˆ†é¡ä»»å‹™ ({args.stage3_epochs} epochs)")
    print(f"  å­¸ç¿’ç‡: {args.stage3_lr} (é™ä½{args.stage1_lr/args.stage3_lr:.0f}å€)")
    
    # æœ€æ¿€é€²çš„å‡çµç­–ç•¥
    model = aggressive_freeze_strategy(model, stage=3)
    
    # å‰µå»ºæ–°å„ªåŒ–å™¨
    optimizer = create_stage_optimizer(model, 3, args.stage3_lr, args.weight_decay)
    trainer.optimizer = optimizer
    
    # è¨“ç·´Stage 3
    stage3_start = time.time()
    stage3_metrics = trainer.train_stage(
        stage_name='stage3_classification',
        task_type='classification',
        epochs=args.stage3_epochs
    )
    stage3_time = time.time() - stage3_start
    
    results['stages']['classification'] = {
        'metrics': stage3_metrics,
        'time': stage3_time,
        'best_score': stage3_metrics.get('best_metric', 0)
    }
    
    # æœ€çµ‚è©•ä¼°æ‰€æœ‰ä»»å‹™
    print("\nğŸ“Š æœ€çµ‚æ€§èƒ½è©•ä¼°...")
    final_performance = {}
    
    with torch.no_grad():
        model.eval()
        
        # åˆ†å‰²
        seg_perf = trainer._validate_segmentation(trainer_dataloaders['segmentation_val'])
        final_seg = seg_perf.get('main_metric', seg_perf.get('miou', 0))
        final_performance['segmentation'] = final_seg
        
        # æª¢æ¸¬
        det_perf = trainer._validate_detection(trainer_dataloaders['detection_val'])
        final_det = det_perf.get('main_metric', det_perf.get('map', 0))
        final_performance['detection'] = final_det
        
        # åˆ†é¡
        cls_perf = trainer._validate_classification(trainer_dataloaders['classification_val'])
        final_cls = cls_perf.get('main_metric', cls_perf.get('accuracy', 0))
        final_performance['classification'] = final_cls
    
    # è¨ˆç®—æœ€çµ‚éºå¿˜ç‡
    seg_forgetting_final = (baseline_seg - final_seg) / baseline_seg if baseline_seg > 0 else 0
    det_forgetting_final = (stage2_metrics.get('best_metric', 0) - final_det) / stage2_metrics.get('best_metric', 1) if stage2_metrics.get('best_metric', 0) > 0 else 0
    
    avg_forgetting = (seg_forgetting_final + det_forgetting_final) / 2
    
    results['final_performance'] = final_performance
    results['final_forgetting_rates'] = {
        'segmentation': seg_forgetting_final,
        'detection': det_forgetting_final,
        'average': avg_forgetting
    }
    results['total_time'] = stage1_time + stage2_time + stage3_time
    
    # æ‰“å°ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ† è¨“ç·´å®Œæˆç¸½çµ")
    print("="*60)
    print(f"ç¸½è¨“ç·´æ™‚é–“: {results['total_time']/60:.1f}åˆ†é˜")
    print("\næœ€çµ‚æ€§èƒ½:")
    print(f"  åˆ†å‰² mIoU: {final_seg:.4f} (éºå¿˜ç‡: {seg_forgetting_final:.2%})")
    print(f"  æª¢æ¸¬ mAP: {final_det:.4f} (éºå¿˜ç‡: {det_forgetting_final:.2%})")
    print(f"  åˆ†é¡æº–ç¢ºç‡: {final_cls:.4f}")
    print(f"\nå¹³å‡éºå¿˜ç‡: {avg_forgetting:.2%}")
    
    if avg_forgetting <= 0.05:
        print("\nâœ… æˆåŠŸï¼å¹³å‡éºå¿˜ç‡ â‰¤ 5%")
    else:
        print(f"\nâš ï¸ å¹³å‡éºå¿˜ç‡ {avg_forgetting:.2%} > 5%")
    
    # ä¿å­˜çµæœ
    results_path = os.path.join(args.save_dir, 'final_results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {results_path}")
    
    return 0 if avg_forgetting <= 0.05 else 1


if __name__ == "__main__":
    exit(main())