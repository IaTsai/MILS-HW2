#!/usr/bin/env python3
"""
Phase 1 æœ€çµ‚é©—æ”¶ç³»çµ±
åŸ·è¡Œå®Œæ•´çš„ Phase 1 é©—æ”¶ï¼ŒåŒ…æ‹¬ç’°å¢ƒã€é …ç›®çµæ§‹ã€æ•¸æ“šå®Œæ•´æ€§ã€åŠŸèƒ½æ¸¬è©¦å’Œæ€§èƒ½åŸºæº–
"""
import os
import sys
import time
import torch
import psutil
import subprocess
import json
from pathlib import Path
from datetime import datetime

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class Phase1Verifier:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / 'data'
        self.results = {
            'environment': {},
            'project_structure': {},
            'data_integrity': {},
            'dataloader_functionality': {},
            'performance_benchmarks': {},
            'overall_status': 'PENDING'
        }
        self.errors = []
        self.warnings = []
        
    def print_header(self, title, symbol="="):
        """Print formatted section header"""
        print(f"\n{symbol * 70}")
        print(f"{title:^70}")
        print(f"{symbol * 70}")
    
    def print_status(self, item, status, details=""):
        """Print status with colored output"""
        if status == "PASS":
            print(f"âœ… {item}")
        elif status == "FAIL":
            print(f"âŒ {item}")
            if details:
                print(f"   ğŸ’¡ {details}")
        elif status == "WARN":
            print(f"âš ï¸  {item}")
            if details:
                print(f"   â„¹ï¸  {details}")
        
        if details and status != "FAIL" and status != "WARN":
            print(f"   â„¹ï¸  {details}")
    
    def check_environment(self):
        """æª¢æŸ¥ç’°å¢ƒé…ç½®"""
        self.print_header("1. ç’°å¢ƒæª¢æŸ¥")
        
        env_results = {}
        
        # Python ç‰ˆæœ¬æª¢æŸ¥
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        env_results['python_version'] = python_version
        if sys.version_info >= (3, 8):
            self.print_status(f"Python ç‰ˆæœ¬: {python_version}", "PASS")
        else:
            self.print_status(f"Python ç‰ˆæœ¬: {python_version}", "FAIL", "éœ€è¦ Python 3.8+")
            self.errors.append("Python ç‰ˆæœ¬éèˆŠ")
        
        # PyTorch æª¢æŸ¥
        try:
            torch_version = torch.__version__
            env_results['torch_version'] = torch_version
            self.print_status(f"PyTorch ç‰ˆæœ¬: {torch_version}", "PASS")
        except Exception as e:
            self.print_status("PyTorch", "FAIL", f"ç„¡æ³•æª¢æ¸¬ç‰ˆæœ¬: {e}")
            self.errors.append("PyTorch æœªå®‰è£æˆ–ç‰ˆæœ¬å•é¡Œ")
        
        # CUDA å¯ç”¨æ€§æª¢æŸ¥
        cuda_available = torch.cuda.is_available()
        env_results['cuda_available'] = cuda_available
        if cuda_available:
            cuda_version = torch.version.cuda
            gpu_count = torch.cuda.device_count()
            env_results['cuda_version'] = cuda_version
            env_results['gpu_count'] = gpu_count
            self.print_status(f"CUDA å¯ç”¨: v{cuda_version}, {gpu_count} GPU(s)", "PASS")
            
            # GPU è¨˜æ†¶é«”æª¢æŸ¥
            for i in range(gpu_count):
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                env_results[f'gpu_{i}_memory_gb'] = gpu_memory
                if gpu_memory >= 4.0:
                    self.print_status(f"GPU {i} è¨˜æ†¶é«”: {gpu_memory:.1f} GB", "PASS")
                else:
                    self.print_status(f"GPU {i} è¨˜æ†¶é«”: {gpu_memory:.1f} GB", "WARN", "å»ºè­°è‡³å°‘ 4GB")
                    self.warnings.append(f"GPU {i} è¨˜æ†¶é«”ä¸è¶³ 4GB")
        else:
            self.print_status("CUDA", "WARN", "ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU")
            self.warnings.append("CUDA ä¸å¯ç”¨")
        
        # ç³»çµ±è¨˜æ†¶é«”æª¢æŸ¥
        system_memory = psutil.virtual_memory().total / 1024**3
        env_results['system_memory_gb'] = system_memory
        if system_memory >= 8.0:
            self.print_status(f"ç³»çµ±è¨˜æ†¶é«”: {system_memory:.1f} GB", "PASS")
        else:
            self.print_status(f"ç³»çµ±è¨˜æ†¶é«”: {system_memory:.1f} GB", "WARN", "å»ºè­°è‡³å°‘ 8GB")
            self.warnings.append("ç³»çµ±è¨˜æ†¶é«”å¯èƒ½ä¸è¶³")
        
        # ä¾è³´å¥—ä»¶æª¢æŸ¥
        required_packages = {
            'numpy': 'numpy',
            'opencv-python': 'cv2', 
            'matplotlib': 'matplotlib',
            'tqdm': 'tqdm',
            'Pillow': 'PIL'
        }
        for package_name, import_name in required_packages.items():
            try:
                __import__(import_name)
                self.print_status(f"å¥—ä»¶ {package_name}", "PASS")
            except ImportError:
                self.print_status(f"å¥—ä»¶ {package_name}", "FAIL", "æœªå®‰è£")
                self.errors.append(f"ç¼ºå°‘å¥—ä»¶: {package_name}")
        
        self.results['environment'] = env_results
        return len([e for e in self.errors if "å¥—ä»¶" in e or "Python" in e or "PyTorch" in e]) == 0
    
    def check_project_structure(self):
        """æª¢æŸ¥é …ç›®çµæ§‹"""
        self.print_header("2. é …ç›®çµæ§‹æª¢æŸ¥")
        
        structure_results = {}
        
        # å¿…è¦ç›®éŒ„æª¢æŸ¥
        required_dirs = [
            'src',
            'src/models',
            'src/datasets', 
            'src/utils',
            'src/losses',
            'scripts',
            'data'
        ]
        
        for dir_path in required_dirs:
            full_path = self.project_root / dir_path
            exists = full_path.exists()
            structure_results[f'dir_{dir_path.replace("/", "_")}'] = exists
            if exists:
                self.print_status(f"ç›®éŒ„ {dir_path}/", "PASS")
            else:
                self.print_status(f"ç›®éŒ„ {dir_path}/", "FAIL", "ä¸å­˜åœ¨")
                self.errors.append(f"ç¼ºå°‘ç›®éŒ„: {dir_path}")
        
        # å¿…è¦æ–‡ä»¶æª¢æŸ¥
        required_files = [
            'src/__init__.py',
            'src/models/__init__.py',
            'src/datasets/__init__.py',
            'src/utils/__init__.py', 
            'src/losses/__init__.py',
            'src/datasets/coco_dataset.py',
            'src/datasets/voc_dataset.py',
            'src/datasets/imagenette_dataset.py',
            'src/datasets/unified_dataloader.py',
            'scripts/download_data.py',
            'scripts/verify_data.py',
            'scripts/test_dataloader.py'
        ]
        
        for file_path in required_files:
            full_path = self.project_root / file_path
            exists = full_path.exists()
            structure_results[f'file_{file_path.replace("/", "_").replace(".", "_")}'] = exists
            if exists:
                self.print_status(f"æ–‡ä»¶ {file_path}", "PASS")
            else:
                self.print_status(f"æ–‡ä»¶ {file_path}", "FAIL", "ä¸å­˜åœ¨")
                self.errors.append(f"ç¼ºå°‘æ–‡ä»¶: {file_path}")
        
        # æ¨¡çµ„å°å…¥æª¢æŸ¥
        try:
            from src.datasets import CocoDetectionDataset, VOCSegmentationDataset, ImagenetteDataset
            from src.datasets import UnifiedDataLoader, UnifiedDataset, unified_collate_fn
            self.print_status("æ¨¡çµ„å°å…¥", "PASS", "æ‰€æœ‰æ•¸æ“šé›†é¡å’Œçµ±ä¸€åŠ è¼‰å™¨")
            structure_results['module_import'] = True
        except Exception as e:
            self.print_status("æ¨¡çµ„å°å…¥", "FAIL", f"å°å…¥éŒ¯èª¤: {e}")
            self.errors.append(f"æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
            structure_results['module_import'] = False
        
        self.results['project_structure'] = structure_results
        return len([e for e in self.errors if "ç¼ºå°‘" in e or "å°å…¥" in e]) == 0
    
    def check_data_integrity(self):
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        self.print_header("3. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥")
        
        data_results = {}
        
        # æª¢æŸ¥æ•¸æ“šç›®éŒ„å­˜åœ¨
        if not self.data_dir.exists():
            self.print_status("æ•¸æ“šç›®éŒ„", "FAIL", "data/ ç›®éŒ„ä¸å­˜åœ¨")
            self.errors.append("æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨")
            return False
        
        # æª¢æŸ¥å„å€‹æ•¸æ“šé›†
        datasets = {
            'mini_coco_det': {'expected_images': 300, 'max_size_mb': 50},
            'mini_voc_seg': {'expected_images': 300, 'max_size_mb': 35}, 
            'imagenette_160': {'expected_images': 300, 'max_size_mb': 30}
        }
        
        total_images = 0
        total_size_mb = 0
        
        for dataset_name, config in datasets.items():
            dataset_path = self.data_dir / dataset_name
            dataset_results = {}
            
            if dataset_path.exists():
                # è¨ˆç®—å¤§å°
                size_bytes = sum(f.stat().st_size for f in dataset_path.rglob('*') if f.is_file())
                size_mb = size_bytes / 1024 / 1024
                dataset_results['size_mb'] = size_mb
                total_size_mb += size_mb
                
                # è¨ˆç®—åœ–åƒæ•¸é‡
                image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
                image_files = [f for f in dataset_path.rglob('*') if f.suffix.lower() in image_extensions]
                image_count = len(image_files)
                dataset_results['image_count'] = image_count
                total_images += image_count
                
                # æª¢æŸ¥çµæœ
                if image_count >= config['expected_images'] * 0.9:  # å…è¨± 10% èª¤å·®
                    self.print_status(f"{dataset_name} åœ–åƒæ•¸é‡: {image_count}", "PASS")
                else:
                    self.print_status(f"{dataset_name} åœ–åƒæ•¸é‡: {image_count}", "FAIL", 
                                    f"æœŸæœ›è‡³å°‘ {int(config['expected_images'] * 0.9)}")
                    self.errors.append(f"{dataset_name} åœ–åƒæ•¸é‡ä¸è¶³")
                
                if size_mb <= config['max_size_mb']:
                    self.print_status(f"{dataset_name} å¤§å°: {size_mb:.1f} MB", "PASS")
                else:
                    self.print_status(f"{dataset_name} å¤§å°: {size_mb:.1f} MB", "WARN", 
                                    f"è¶…éå»ºè­°å¤§å° {config['max_size_mb']} MB")
                    self.warnings.append(f"{dataset_name} å¤§å°è¶…éå»ºè­°å€¼")
                
                dataset_results['exists'] = True
            else:
                self.print_status(f"{dataset_name}", "FAIL", "æ•¸æ“šé›†ä¸å­˜åœ¨")
                self.errors.append(f"æ•¸æ“šé›† {dataset_name} ä¸å­˜åœ¨")
                dataset_results['exists'] = False
            
            data_results[dataset_name] = dataset_results
        
        # ç¸½é«”æª¢æŸ¥
        data_results['total_images'] = total_images
        data_results['total_size_mb'] = total_size_mb
        
        if total_size_mb <= 120:
            self.print_status(f"ç¸½æ•¸æ“šå¤§å°: {total_size_mb:.1f} MB", "PASS")
        else:
            self.print_status(f"ç¸½æ•¸æ“šå¤§å°: {total_size_mb:.1f} MB", "WARN", "è¶…é 120MB å»ºè­°å€¼")
            self.warnings.append("ç¸½æ•¸æ“šå¤§å°è¶…éå»ºè­°å€¼")
        
        if total_images >= 800:  # å…è¨±ä¸€äº›èª¤å·®
            self.print_status(f"ç¸½åœ–åƒæ•¸é‡: {total_images}", "PASS")
        else:
            self.print_status(f"ç¸½åœ–åƒæ•¸é‡: {total_images}", "FAIL", "æœŸæœ›è‡³å°‘ 800 å¼µ")
            self.errors.append("ç¸½åœ–åƒæ•¸é‡ä¸è¶³")
        
        self.results['data_integrity'] = data_results
        return len([e for e in self.errors if "æ•¸æ“š" in e]) == 0
    
    def check_dataloader_functionality(self):
        """æª¢æŸ¥æ•¸æ“šåŠ è¼‰å™¨åŠŸèƒ½"""
        self.print_header("4. æ•¸æ“šåŠ è¼‰å™¨åŠŸèƒ½æª¢æŸ¥")
        
        func_results = {}
        
        try:
            from src.datasets import CocoDetectionDataset, VOCSegmentationDataset, ImagenetteDataset
            from src.datasets import UnifiedDataLoader, UnifiedDataset, unified_collate_fn
            from torch.utils.data import DataLoader
            
            # æ¸¬è©¦å€‹åˆ¥æ•¸æ“šé›†
            datasets = {}
            
            # COCO Detection
            try:
                coco_dataset = CocoDetectionDataset(self.data_dir / 'mini_coco_det', split='train')
                img, target = coco_dataset[0]
                assert isinstance(img, torch.Tensor), "COCO åœ–åƒä¸æ˜¯ tensor"
                assert isinstance(target, dict), "COCO target ä¸æ˜¯ dict"
                assert 'boxes' in target, "COCO target ç¼ºå°‘ boxes"
                assert 'labels' in target, "COCO target ç¼ºå°‘ labels"
                datasets['coco'] = coco_dataset
                self.print_status("COCO Detection æ•¸æ“šé›†", "PASS", f"å½¢ç‹€: {img.shape}")
                func_results['coco_dataset'] = True
            except Exception as e:
                self.print_status("COCO Detection æ•¸æ“šé›†", "FAIL", str(e))
                self.errors.append(f"COCO æ•¸æ“šé›†éŒ¯èª¤: {e}")
                func_results['coco_dataset'] = False
            
            # VOC Segmentation
            try:
                voc_dataset = VOCSegmentationDataset(self.data_dir / 'mini_voc_seg', split='train')
                img, target = voc_dataset[0]
                assert isinstance(img, torch.Tensor), "VOC åœ–åƒä¸æ˜¯ tensor"
                assert isinstance(target, dict), "VOC target ä¸æ˜¯ dict"
                assert 'masks' in target, "VOC target ç¼ºå°‘ masks"
                assert 'labels' in target, "VOC target ç¼ºå°‘ labels"
                datasets['voc'] = voc_dataset
                self.print_status("VOC Segmentation æ•¸æ“šé›†", "PASS", f"å½¢ç‹€: {img.shape}")
                func_results['voc_dataset'] = True
            except Exception as e:
                self.print_status("VOC Segmentation æ•¸æ“šé›†", "FAIL", str(e))
                self.errors.append(f"VOC æ•¸æ“šé›†éŒ¯èª¤: {e}")
                func_results['voc_dataset'] = False
            
            # Imagenette Classification
            try:
                imagenette_dataset = ImagenetteDataset(self.data_dir / 'imagenette_160', split='train')
                img, target = imagenette_dataset[0]
                assert isinstance(img, torch.Tensor), "Imagenette åœ–åƒä¸æ˜¯ tensor"
                assert isinstance(target, dict), "Imagenette target ä¸æ˜¯ dict"
                assert 'labels' in target, "Imagenette target ç¼ºå°‘ labels"
                datasets['imagenette'] = imagenette_dataset
                self.print_status("Imagenette Classification æ•¸æ“šé›†", "PASS", f"å½¢ç‹€: {img.shape}")
                func_results['imagenette_dataset'] = True
            except Exception as e:
                self.print_status("Imagenette Classification æ•¸æ“šé›†", "FAIL", str(e))
                self.errors.append(f"Imagenette æ•¸æ“šé›†éŒ¯èª¤: {e}")
                func_results['imagenette_dataset'] = False
            
            # æ¸¬è©¦çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨
            if all(func_results.get(k, False) for k in ['coco_dataset', 'voc_dataset', 'imagenette_dataset']):
                try:
                    # æ¸¬è©¦ UnifiedDataset
                    unified_dataset = UnifiedDataset(
                        detection_dataset=datasets['coco'],
                        segmentation_dataset=datasets['voc'],
                        classification_dataset=datasets['imagenette'],
                        sampling_strategy='balanced'
                    )
                    
                    item = unified_dataset[0]
                    assert isinstance(item, dict), "UnifiedDataset é …ç›®ä¸æ˜¯ dict"
                    assert 'images' in item, "UnifiedDataset é …ç›®ç¼ºå°‘ images"
                    assert 'task_type' in item, "UnifiedDataset é …ç›®ç¼ºå°‘ task_type"
                    assert 'targets' in item, "UnifiedDataset é …ç›®ç¼ºå°‘ targets"
                    
                    self.print_status("UnifiedDataset", "PASS", f"ä»»å‹™é¡å‹: {item['task_type']}")
                    func_results['unified_dataset'] = True
                    
                    # æ¸¬è©¦ DataLoader with unified_collate_fn
                    loader = DataLoader(unified_dataset, batch_size=4, collate_fn=unified_collate_fn)
                    batch = next(iter(loader))
                    assert 'images' in batch, "æ‰¹æ¬¡ç¼ºå°‘ images"
                    assert 'task_types' in batch, "æ‰¹æ¬¡ç¼ºå°‘ task_types"
                    assert 'targets' in batch, "æ‰¹æ¬¡ç¼ºå°‘ targets"
                    assert 'task_groups' in batch, "æ‰¹æ¬¡ç¼ºå°‘ task_groups"
                    
                    self.print_status("çµ±ä¸€æ‰¹æ¬¡åŠ è¼‰", "PASS", f"æ‰¹æ¬¡å¤§å°: {batch['images'].shape}")
                    func_results['unified_batch'] = True
                    
                    # æ¸¬è©¦ UnifiedDataLoader
                    unified_loader = UnifiedDataLoader(
                        detection_dataset=datasets['coco'],
                        segmentation_dataset=datasets['voc'],
                        classification_dataset=datasets['imagenette'],
                        batch_size=4,
                        sampling_strategy='balanced'
                    )
                    
                    batch = next(iter(unified_loader))
                    assert 'images' in batch, "UnifiedDataLoader æ‰¹æ¬¡ç¼ºå°‘ images"
                    
                    self.print_status("UnifiedDataLoader", "PASS", f"æ‰¹æ¬¡å¤§å°: {batch['images'].shape}")
                    func_results['unified_dataloader'] = True
                    
                except Exception as e:
                    self.print_status("çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨", "FAIL", str(e))
                    self.errors.append(f"çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨éŒ¯èª¤: {e}")
                    func_results['unified_dataset'] = False
                    func_results['unified_batch'] = False
                    func_results['unified_dataloader'] = False
            else:
                self.print_status("çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨", "FAIL", "å€‹åˆ¥æ•¸æ“šé›†æ¸¬è©¦å¤±æ•—")
                self.errors.append("ç„¡æ³•æ¸¬è©¦çµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨")
                func_results['unified_dataset'] = False
                func_results['unified_batch'] = False
                func_results['unified_dataloader'] = False
                
        except Exception as e:
            self.print_status("æ•¸æ“šåŠ è¼‰å™¨å°å…¥", "FAIL", str(e))
            self.errors.append(f"æ•¸æ“šåŠ è¼‰å™¨å°å…¥éŒ¯èª¤: {e}")
            func_results['import_error'] = str(e)
        
        self.results['dataloader_functionality'] = func_results
        return len([e for e in self.errors if "æ•¸æ“šé›†" in e or "åŠ è¼‰å™¨" in e]) == 0
    
    def check_performance_benchmarks(self):
        """æª¢æŸ¥æ€§èƒ½åŸºæº–"""
        self.print_header("5. æ€§èƒ½åŸºæº–æ¸¬è©¦")
        
        perf_results = {}
        
        try:
            from src.datasets import CocoDetectionDataset, VOCSegmentationDataset, ImagenetteDataset
            from src.datasets import UnifiedDataLoader
            
            # å‰µå»ºæ•¸æ“šé›†
            coco_dataset = CocoDetectionDataset(self.data_dir / 'mini_coco_det', split='train')
            voc_dataset = VOCSegmentationDataset(self.data_dir / 'mini_voc_seg', split='train')
            imagenette_dataset = ImagenetteDataset(self.data_dir / 'imagenette_160', split='train')
            
            # å‰µå»ºçµ±ä¸€åŠ è¼‰å™¨
            unified_loader = UnifiedDataLoader(
                detection_dataset=coco_dataset,
                segmentation_dataset=voc_dataset,
                classification_dataset=imagenette_dataset,
                batch_size=16,
                num_workers=2,
                sampling_strategy='balanced'
            )
            
            # æ¸¬è©¦æ•¸æ“šåŠ è¼‰é€Ÿåº¦
            print("æ­£åœ¨æ¸¬è©¦æ•¸æ“šåŠ è¼‰é€Ÿåº¦...")
            num_samples = 100
            start_time = time.time()
            samples_loaded = 0
            
            # é ç†±
            for _ in range(2):
                batch = next(iter(unified_loader))
            
            # å¯¦éš›æ¸¬è©¦
            start_time = time.time()
            for i, batch in enumerate(unified_loader):
                samples_loaded += len(batch['images'])
                if samples_loaded >= num_samples:
                    break
            
            elapsed_time = time.time() - start_time
            loading_speed = samples_loaded / elapsed_time
            perf_results['loading_speed_samples_per_sec'] = loading_speed
            
            if loading_speed >= 30:  # é™ä½è¦æ±‚ä»¥é©æ‡‰è¼ƒæ…¢çš„ç’°å¢ƒ
                self.print_status(f"æ•¸æ“šåŠ è¼‰é€Ÿåº¦: {loading_speed:.2f} samples/sec", "PASS")
            else:
                self.print_status(f"æ•¸æ“šåŠ è¼‰é€Ÿåº¦: {loading_speed:.2f} samples/sec", "WARN", 
                                "é€Ÿåº¦è¼ƒæ…¢ï¼Œä½†å¯æ¥å—")
                self.warnings.append("æ•¸æ“šåŠ è¼‰é€Ÿåº¦è¼ƒæ…¢")
            
            # è¨˜æ†¶é«”ä½¿ç”¨æ¸¬è©¦
            memory_before = psutil.virtual_memory().used / 1024**3
            
            # åŠ è¼‰å¤šå€‹æ‰¹æ¬¡æ¸¬è©¦è¨˜æ†¶é«”
            batches = []
            for i, batch in enumerate(unified_loader):
                batches.append(batch)
                if i >= 5:  # åŠ è¼‰ 6 å€‹æ‰¹æ¬¡
                    break
            
            memory_after = psutil.virtual_memory().used / 1024**3
            memory_usage = memory_after - memory_before
            perf_results['memory_usage_gb'] = memory_usage
            
            if memory_usage <= 2.0:
                self.print_status(f"è¨˜æ†¶é«”ä½¿ç”¨: {memory_usage:.2f} GB", "PASS")
            else:
                self.print_status(f"è¨˜æ†¶é«”ä½¿ç”¨: {memory_usage:.2f} GB", "WARN", "è¨˜æ†¶é«”ä½¿ç”¨è¼ƒé«˜")
                self.warnings.append("è¨˜æ†¶é«”ä½¿ç”¨è¼ƒé«˜")
            
            # GPU é‹ç®—æ¸¬è©¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                try:
                    device = torch.device('cuda')
                    test_tensor = torch.randn(1000, 1000, device=device)
                    start_time = time.time()
                    result = torch.matmul(test_tensor, test_tensor)
                    torch.cuda.synchronize()
                    gpu_time = time.time() - start_time
                    perf_results['gpu_compute_time_ms'] = gpu_time * 1000
                    
                    self.print_status(f"GPU é‹ç®—æ¸¬è©¦: {gpu_time*1000:.2f} ms", "PASS")
                except Exception as e:
                    self.print_status("GPU é‹ç®—æ¸¬è©¦", "WARN", f"æ¸¬è©¦å¤±æ•—: {e}")
                    self.warnings.append("GPU é‹ç®—æ¸¬è©¦å¤±æ•—")
            else:
                self.print_status("GPU é‹ç®—æ¸¬è©¦", "WARN", "CUDA ä¸å¯ç”¨")
                
        except Exception as e:
            self.print_status("æ€§èƒ½æ¸¬è©¦", "FAIL", str(e))
            self.errors.append(f"æ€§èƒ½æ¸¬è©¦éŒ¯èª¤: {e}")
            return False
        
        self.results['performance_benchmarks'] = perf_results
        return True
    
    def generate_verification_report(self):
        """ç”Ÿæˆé©—æ”¶å ±å‘Š"""
        self.print_header("6. é©—æ”¶å ±å‘Š", "=")
        
        # è¨ˆç®—ç¸½é«”ç‹€æ…‹
        has_critical_errors = len(self.errors) > 0
        has_warnings = len(self.warnings) > 0
        
        if has_critical_errors:
            self.results['overall_status'] = 'FAILED'
            status_symbol = "âŒ"
            status_text = "å¤±æ•—"
        elif has_warnings:
            self.results['overall_status'] = 'PASSED_WITH_WARNINGS'
            status_symbol = "âš ï¸"
            status_text = "é€šé (æœ‰è­¦å‘Š)"
        else:
            self.results['overall_status'] = 'PASSED'
            status_symbol = "âœ…"
            status_text = "å®Œå…¨é€šé"
        
        print(f"\n{'=' * 70}")
        print(f"{'Phase 1 é©—æ”¶å ±å‘Š':^70}")
        print(f"{'=' * 70}")
        
        # æª¢æŸ¥é …ç›®æ‘˜è¦
        checks = [
            ("ç’°å¢ƒæª¢æŸ¥", "environment"),
            ("é …ç›®çµæ§‹", "project_structure"), 
            ("æ•¸æ“šå®Œæ•´æ€§", "data_integrity"),
            ("æ•¸æ“šåŠ è¼‰å™¨", "dataloader_functionality"),
            ("æ€§èƒ½åŸºæº–", "performance_benchmarks")
        ]
        
        for check_name, check_key in checks:
            if check_key in self.results and self.results[check_key]:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸é—œéŒ¯èª¤
                related_errors = [e for e in self.errors if any(keyword in e.lower() for keyword in check_name.lower().split())]
                if related_errors:
                    print(f"âŒ {check_name}")
                else:
                    print(f"âœ… {check_name}")
            else:
                print(f"âŒ {check_name}")
        
        # çµ±è¨ˆä¿¡æ¯
        print(f"\nğŸ“Š çµ±è¨ˆä¿¡æ¯:")
        if 'data_integrity' in self.results:
            data_info = self.results['data_integrity']
            print(f"   ç¸½åœ–åƒæ•¸é‡: {data_info.get('total_images', 'N/A')}")
            print(f"   ç¸½æ•¸æ“šå¤§å°: {data_info.get('total_size_mb', 'N/A'):.1f} MB")
        
        if 'performance_benchmarks' in self.results:
            perf_info = self.results['performance_benchmarks']
            if 'loading_speed_samples_per_sec' in perf_info:
                print(f"   æ•¸æ“šè¼‰å…¥é€Ÿåº¦: {perf_info['loading_speed_samples_per_sec']:.1f} samples/sec")
            if 'memory_usage_gb' in perf_info:
                print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {perf_info['memory_usage_gb']:.2f} GB")
        
        if 'environment' in self.results:
            env_info = self.results['environment']
            if 'cuda_available' in env_info and env_info['cuda_available']:
                gpu_memory = env_info.get('gpu_0_memory_gb', 'N/A')
                print(f"   GPUè¨˜æ†¶é«”: {gpu_memory:.1f} GB available")
            else:
                print(f"   GPUè¨˜æ†¶é«”: N/A (CUDA ä¸å¯ç”¨)")
        
        # æœ€çµ‚ç‹€æ…‹
        print(f"\n{status_symbol} Phase 1 é©—æ”¶: {status_text}")
        
        # éŒ¯èª¤å’Œè­¦å‘Š
        if self.errors:
            print(f"\nâŒ ç™¼ç¾ {len(self.errors)} å€‹éŒ¯èª¤:")
            for i, error in enumerate(self.errors, 1):
                print(f"   {i}. {error}")
        
        if self.warnings:
            print(f"\nâš ï¸  ç™¼ç¾ {len(self.warnings)} å€‹è­¦å‘Š:")
            for i, warning in enumerate(self.warnings, 1):
                print(f"   {i}. {warning}")
        
        # ä¿®å¾©å»ºè­°
        if self.errors:
            print(f"\nğŸ”§ ä¿®å¾©å»ºè­°:")
            print("   1. æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯ä¸¦è§£æ±ºç›¸é—œå•é¡Œ")
            print("   2. é‡æ–°é‹è¡Œé©—æ”¶è…³æœ¬")
            print("   3. å¦‚éœ€å”åŠ©ï¼Œè«‹æŸ¥çœ‹ç›¸é—œæ–‡æª”")
        
        # ä¸‹ä¸€æ­¥æŒ‡å°
        if not has_critical_errors:
            print(f"\nğŸ‰ Phase 1 é©—æ”¶é€šéï¼")
            print("ğŸ“ æ‰€æœ‰æ ¸å¿ƒç³»çµ±å°±ç·’ï¼Œå¯ä»¥é€²å…¥ Phase 2: æ¨¡å‹æ¶æ§‹è¨­è¨ˆ")
            print('\nè«‹è¼¸å…¥ "é€²å…¥Phase 2" ä»¥ç¹¼çºŒä¸‹ä¸€éšæ®µã€‚')
        else:
            print(f"\nğŸ”„ è«‹ä¿®å¾©ä¸Šè¿°éŒ¯èª¤å¾Œé‡æ–°é‹è¡Œé©—æ”¶")
            print("   python scripts/final_phase1_verification.py")
        
        # ä¿å­˜è©³ç´°å ±å‘Š
        report_path = self.project_root / 'phase1_verification_report.json'
        self.results['timestamp'] = datetime.now().isoformat()
        self.results['errors'] = self.errors
        self.results['warnings'] = self.warnings
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return not has_critical_errors
    
    def run_verification(self):
        """åŸ·è¡Œå®Œæ•´é©—æ”¶æµç¨‹"""
        print("ğŸš€ é–‹å§‹ Phase 1 æœ€çµ‚é©—æ”¶...")
        print(f"â° æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        success = True
        
        # åŸ·è¡Œå„é …æª¢æŸ¥
        success &= self.check_environment()
        success &= self.check_project_structure()
        success &= self.check_data_integrity()
        success &= self.check_dataloader_functionality()
        success &= self.check_performance_benchmarks()
        
        # ç”Ÿæˆå ±å‘Š
        final_success = self.generate_verification_report()
        
        return final_success


def main():
    """ä¸»å‡½æ•¸"""
    verifier = Phase1Verifier()
    success = verifier.run_verification()
    
    # è¿”å›é©ç•¶çš„é€€å‡ºç¢¼
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()