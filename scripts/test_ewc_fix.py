#!/usr/bin/env python3
"""
å¿«é€Ÿæ¸¬è©¦ä¿®å¾©å¾Œçš„EWCæ•ˆæœ
é©—è­‰FisherçŸ©é™£è¨ˆç®—å’Œéºå¿˜ç‡æ§åˆ¶
"""
import os
import sys
import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import DataLoader
import time

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.unified_model import create_unified_model
from src.utils.ewc import create_ewc_handler, ewc_loss
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.losses.segmentation_loss import create_segmentation_loss
from src.losses.detection_loss import create_detection_loss
from src.losses.classification_loss import create_classification_loss


def test_fisher_calculation():
    """æ¸¬è©¦FisherçŸ©é™£è¨ˆç®—çš„æ•¸å€¼ç©©å®šæ€§"""
    print("\n" + "="*50)
    print("ğŸ§ª æ¸¬è©¦FisherçŸ©é™£è¨ˆç®—")
    print("="*50)
    
    # å‰µå»ºæ¨¡å‹
    model = create_unified_model('default')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # å‰µå»ºEWCè™•ç†å™¨
    ewc = create_ewc_handler(model, importance=5000.0)  # ä½¿ç”¨æ–°çš„é è¨­å€¼
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(
        data_dir='./data',
        batch_size=4,
        num_workers=0
    )
    
    # ç²å–åˆ†å‰²ä»»å‹™çš„åŠ è¼‰å™¨
    seg_loader = dataloaders['train'].get_task_loader('segmentation')
    
    # è¨ˆç®—FisherçŸ©é™£
    print("\nğŸ“Š è¨ˆç®—FisherçŸ©é™£...")
    start_time = time.time()
    fisher_matrix = ewc.compute_fisher_matrix(seg_loader, task_id=0, num_samples=20, verbose=True)
    compute_time = time.time() - start_time
    
    # é©—è­‰Fisherå€¼ç¯„åœ
    fisher_values = []
    for name, tensor in fisher_matrix.items():
        fisher_values.extend(tensor.cpu().numpy().flatten())
    
    fisher_values = np.array(fisher_values)
    print(f"\nğŸ“ˆ FisherçŸ©é™£çµ±è¨ˆ:")
    print(f"  - è¨ˆç®—æ™‚é–“: {compute_time:.2f}ç§’")
    print(f"  - å¹³å‡å€¼: {fisher_values.mean():.6f}")
    print(f"  - æ¨™æº–å·®: {fisher_values.std():.6f}")
    print(f"  - æœ€å°å€¼: {fisher_values.min():.6f}")
    print(f"  - æœ€å¤§å€¼: {fisher_values.max():.6f}")
    print(f"  - éé›¶å€¼æ¯”ä¾‹: {(fisher_values > 1e-8).mean():.2%}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰çˆ†ç‚¸å€¼
    if fisher_values.max() > 1e6:
        print("âš ï¸ è­¦å‘Š: Fisherå€¼è¶…é1e6ä¸Šé™!")
    else:
        print("âœ… Fisherå€¼åœ¨æ­£å¸¸ç¯„åœå…§")
    
    return ewc, fisher_values


def test_ewc_penalty_growth():
    """æ¸¬è©¦EWCæ‡²ç½°é …çš„å¢é•·æƒ…æ³"""
    print("\n" + "="*50)
    print("ğŸ§ª æ¸¬è©¦EWCæ‡²ç½°é …å¢é•·")
    print("="*50)
    
    # å‰µå»ºæ¨¡å‹å’ŒEWC
    model = create_unified_model('default')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    ewc = create_ewc_handler(model, importance=5000.0)
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(
        data_dir='./data',
        batch_size=4,
        num_workers=0
    )
    
    # å®Œæˆç¬¬ä¸€å€‹ä»»å‹™
    seg_loader = dataloaders['train'].get_task_loader('segmentation')
    ewc.finish_task(seg_loader, task_id=0, verbose=False)
    
    # æ¨¡æ“¬åƒæ•¸è®ŠåŒ–ä¸¦æ¸¬è©¦æ‡²ç½°é …
    print("\nğŸ“Š æ¸¬è©¦ä¸åŒåƒæ•¸è®ŠåŒ–ä¸‹çš„EWCæ‡²ç½°é …:")
    
    # ä¿å­˜åŸå§‹åƒæ•¸
    original_state = model.state_dict()
    
    # æ¸¬è©¦ä¸åŒç¨‹åº¦çš„åƒæ•¸è®ŠåŒ–
    for scale in [0.0, 0.01, 0.05, 0.1, 0.5, 1.0]:
        # æ·»åŠ å™ªéŸ³åˆ°åƒæ•¸
        for name, param in model.named_parameters():
            if param.requires_grad:
                noise = torch.randn_like(param) * scale
                param.data = original_state[name] + noise
        
        # è¨ˆç®—æ‡²ç½°é …
        penalty = ewc.penalty(model)
        print(f"  åƒæ•¸è®ŠåŒ– {scale:>4.2f}: æ‡²ç½°é … = {penalty.item():>12.6f}")
    
    # æ¢å¾©åŸå§‹åƒæ•¸
    model.load_state_dict(original_state)
    
    return ewc


def test_adaptive_importance():
    """æ¸¬è©¦è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´"""
    print("\n" + "="*50)
    print("ğŸ§ª æ¸¬è©¦è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´")
    print("="*50)
    
    # å‰µå»ºæ¨¡å‹å’ŒEWC
    model = create_unified_model('default')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    ewc = create_ewc_handler(model, importance=5000.0)
    
    print(f"\nåˆå§‹æ¬Šé‡: {ewc.adaptive_importance:.1f}")
    
    # æ¸¬è©¦ä¸åŒéºå¿˜ç‡ä¸‹çš„æ¬Šé‡èª¿æ•´
    test_rates = [0.02, 0.05, 0.10, 0.50, 0.90]
    
    for rate in test_rates:
        ewc.update_adaptive_importance(rate, target_rate=0.05)
        print(f"éºå¿˜ç‡ {rate:.2%} â†’ èª¿æ•´å¾Œæ¬Šé‡: {ewc.adaptive_importance:.1f}")
    
    return ewc


def test_multitask_training():
    """æ¸¬è©¦å¤šä»»å‹™è¨“ç·´çš„éºå¿˜æ§åˆ¶"""
    print("\n" + "="*50)
    print("ğŸ§ª æ¸¬è©¦å¤šä»»å‹™è¨“ç·´éºå¿˜æ§åˆ¶")
    print("="*50)
    
    # å‰µå»ºæ¨¡å‹
    model = create_unified_model('default')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # å‰µå»ºEWCè™•ç†å™¨
    ewc = create_ewc_handler(model, importance=20000.0)  # æ›´é«˜çš„åˆå§‹æ¬Šé‡
    
    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(
        data_dir='./data',
        batch_size=8,
        num_workers=0
    )
    
    # å‰µå»ºæå¤±å‡½æ•¸
    seg_criterion = create_segmentation_loss()
    det_criterion = create_detection_loss()
    cls_criterion = create_classification_loss()
    
    # ç¬¬ä¸€éšæ®µï¼šè¨“ç·´åˆ†å‰²ä»»å‹™
    print("\nğŸ¯ Stage 1: è¨“ç·´åˆ†å‰²ä»»å‹™")
    seg_loader = dataloaders['train'].get_task_loader('segmentation')
    
    model.train()
    for epoch in range(2):  # å¿«é€Ÿæ¸¬è©¦
        epoch_loss = 0.0
        for batch_idx, batch in enumerate(seg_loader):
            if batch_idx >= 5:  # åªè¨“ç·´å¹¾å€‹æ‰¹æ¬¡
                break
            
            images, targets = batch
            images = images.to(device)
            
            optimizer.zero_grad()
            outputs = model(images, task_type='segmentation')
            
            # æº–å‚™ç›®æ¨™
            if isinstance(targets, list):
                # VOCæ•¸æ“šé›†è¿”å›å­—å…¸æ ¼å¼
                if isinstance(targets[0], dict) and 'masks' in targets[0]:
                    masks = torch.stack([t['masks'] for t in targets]).to(device)
                elif isinstance(targets[0], torch.Tensor):
                    masks = torch.stack(targets).to(device)
                else:
                    # è™•ç†å…¶ä»–æ ¼å¼
                    print(f"è­¦å‘Š: æœªçŸ¥çš„targetsæ ¼å¼: {type(targets[0])}")
                    masks = targets[0].to(device) if hasattr(targets[0], 'to') else targets[0]
            elif isinstance(targets, dict) and 'masks' in targets:
                masks = targets['masks'].to(device)
            elif isinstance(targets, torch.Tensor):
                masks = targets.to(device)
            else:
                print(f"è­¦å‘Š: æœªçŸ¥çš„targetsé¡å‹: {type(targets)}")
                masks = targets
            
            loss_output = seg_criterion(outputs['segmentation'], masks)
            # è™•ç†å¯èƒ½çš„tupleè¿”å›å€¼
            if isinstance(loss_output, tuple):
                loss = loss_output[0]  # å–ç¬¬ä¸€å€‹å…ƒç´ ä½œç‚ºä¸»æå¤±
            else:
                loss = loss_output
            
            # å¦‚æœæœ‰EWCæ‡²ç½°ï¼Œæ·»åŠ é€²å»
            if ewc.task_count > 0:
                total_loss, ewc_penalty = ewc_loss(loss, ewc, model)
                print(f"    EWCæ‡²ç½°: {ewc_penalty.item():.6f}")
            else:
                total_loss = loss
                ewc_penalty = None
            
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        print(f"  Epoch {epoch+1}: å¹³å‡æå¤± = {epoch_loss/(batch_idx+1):.4f}")
    
    # è©•ä¼°åˆ†å‰²æ€§èƒ½
    model.eval()
    seg_val_loader = dataloaders['val'].get_task_loader('segmentation')
    seg_perf_before = evaluate_segmentation(model, seg_val_loader, device)
    print(f"  åˆ†å‰²æ€§èƒ½: {seg_perf_before:.4f}")
    
    # å®Œæˆä»»å‹™1ï¼Œè¨­ç½®EWC
    print("\nğŸ“¥ è¨­ç½®EWCä¿è­·...")
    ewc.finish_task(seg_loader, task_id=0, verbose=True)
    
    # ç¬¬äºŒéšæ®µï¼šè¨“ç·´æª¢æ¸¬ä»»å‹™
    print("\nğŸ¯ Stage 2: è¨“ç·´æª¢æ¸¬ä»»å‹™ï¼ˆå¸¶EWCä¿è­·ï¼‰")
    det_loader = dataloaders['train'].get_task_loader('detection')
    
    model.train()
    for epoch in range(2):
        epoch_loss = 0.0
        epoch_ewc = 0.0
        for batch_idx, batch in enumerate(det_loader):
            if batch_idx >= 5:
                break
            
            if isinstance(batch, list) and len(batch) == 2:
                images = batch[0].to(device)
                targets = batch[1]
            else:
                continue
            
            optimizer.zero_grad()
            outputs = model(images, task_type='detection')
            
            loss_output = det_criterion(outputs['detection'], targets)
            # è™•ç†å¯èƒ½çš„tupleè¿”å›å€¼
            if isinstance(loss_output, tuple):
                loss = loss_output[0]  # å–ç¬¬ä¸€å€‹å…ƒç´ ä½œç‚ºä¸»æå¤±
            else:
                loss = loss_output
            
            # æ·»åŠ EWCæ‡²ç½°
            total_loss, ewc_penalty = ewc_loss(loss, ewc, model)
            epoch_ewc += ewc_penalty.item()
            
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        print(f"  Epoch {epoch+1}: æå¤± = {epoch_loss/(batch_idx+1):.4f}, EWC = {epoch_ewc/(batch_idx+1):.4f}")
    
    # é‡æ–°è©•ä¼°åˆ†å‰²æ€§èƒ½
    model.eval()
    seg_perf_after = evaluate_segmentation(model, seg_val_loader, device)
    forgetting_rate = (seg_perf_before - seg_perf_after) / seg_perf_before if seg_perf_before > 0 else 0
    
    print(f"\nğŸ“Š éºå¿˜åˆ†æ:")
    print(f"  åˆ†å‰²æ€§èƒ½è®ŠåŒ–: {seg_perf_before:.4f} â†’ {seg_perf_after:.4f}")
    print(f"  éºå¿˜ç‡: {forgetting_rate:.2%}")
    
    if forgetting_rate > 0.05:
        print("âš ï¸ éºå¿˜ç‡ä»ç„¶è¶…é5%ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æ•´!")
        ewc.update_adaptive_importance(forgetting_rate)
        print(f"  æ–°çš„EWCæ¬Šé‡: {ewc.adaptive_importance:.1f}")
    else:
        print("âœ… éºå¿˜ç‡æ§åˆ¶åœ¨5%ä»¥å…§!")
    
    return forgetting_rate


def evaluate_segmentation(model, dataloader, device):
    """ç°¡å–®çš„åˆ†å‰²è©•ä¼°å‡½æ•¸"""
    model.eval()
    total_correct = 0
    total_pixels = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 5:  # å¿«é€Ÿæ¸¬è©¦
                break
            
            images, targets = batch
            images = images.to(device)
            
            outputs = model(images, task_type='segmentation')
            preds = outputs['segmentation'].argmax(dim=1)
            
            if isinstance(targets, list):
                # VOCæ•¸æ“šé›†è¿”å›å­—å…¸æ ¼å¼
                if isinstance(targets[0], dict) and 'masks' in targets[0]:
                    masks = torch.stack([t['masks'] for t in targets]).to(device)
                elif isinstance(targets[0], torch.Tensor):
                    masks = torch.stack(targets).to(device)
                else:
                    # è™•ç†å…¶ä»–æ ¼å¼
                    masks = targets[0].to(device) if hasattr(targets[0], 'to') else targets[0]
            elif isinstance(targets, dict) and 'masks' in targets:
                masks = targets['masks'].to(device)
            elif isinstance(targets, torch.Tensor):
                masks = targets.to(device)
            else:
                masks = targets
            
            total_correct += (preds == masks).sum().item()
            total_pixels += masks.numel()
    
    return total_correct / total_pixels if total_pixels > 0 else 0


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ä¿®å¾©çš„EWCå¯¦ç¾")
    print("="*60)
    
    # 1. æ¸¬è©¦FisherçŸ©é™£è¨ˆç®—
    ewc1, fisher_values = test_fisher_calculation()
    
    # 2. æ¸¬è©¦EWCæ‡²ç½°é …å¢é•·
    ewc2 = test_ewc_penalty_growth()
    
    # 3. æ¸¬è©¦è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´
    ewc3 = test_adaptive_importance()
    
    # 4. æ¸¬è©¦å¤šä»»å‹™è¨“ç·´
    forgetting_rate = test_multitask_training()
    
    # ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµ")
    print("="*60)
    print(f"âœ… FisherçŸ©é™£è¨ˆç®—: æ•¸å€¼ç©©å®š")
    print(f"âœ… EWCæ‡²ç½°é …: æ­£å¸¸å¢é•·")
    print(f"âœ… è‡ªé©æ‡‰æ¬Šé‡: åŠŸèƒ½æ­£å¸¸")
    print(f"{'âœ…' if forgetting_rate < 0.05 else 'âš ï¸'} éºå¿˜æ§åˆ¶: {forgetting_rate:.2%}")
    
    print("\nğŸ¯ å»ºè­°:")
    if forgetting_rate > 0.05:
        print(f"  - ä½¿ç”¨æ›´é«˜çš„EWCæ¬Šé‡ (å»ºè­°: {20000 * (1 + forgetting_rate * 10):.0f})")
        print(f"  - å•Ÿç”¨è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´")
        print(f"  - è€ƒæ…®ä½¿ç”¨Online EWCæ¸›å°‘è¨ˆç®—é–‹éŠ·")
    else:
        print(f"  - ç•¶å‰è¨­ç½®æœ‰æ•ˆï¼Œå¯ä»¥é–‹å§‹å®Œæ•´è¨“ç·´")
    
    print("\nâœ¨ æ¸¬è©¦å®Œæˆ!")


if __name__ == "__main__":
    main()