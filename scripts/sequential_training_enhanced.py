#!/usr/bin/env python3
"""
ä¾åºè¨“ç·´è…³æœ¬ - å¢å¼·é˜²éºå¿˜ç‰ˆæœ¬
å¯¦æ–½æ–°çš„é˜²éºå¿˜ç­–ç•¥ï¼šå­¸ç¿’ç‡èª¿åº¦ + åƒæ•¸å‡çµ + å¢å¼·æ­£å‰‡åŒ–
"""
import os
import sys
import argparse
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.unified_model import create_unified_model
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.utils.sequential_trainer import create_sequential_trainer
from src.utils.ewc import create_ewc_handler


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='ä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´ - å¢å¼·é˜²éºå¿˜ç‰ˆæœ¬')
    
    # åŸºæœ¬è¨­ç½®
    parser.add_argument('--model_config', type=str, default='default',
                       help='æ¨¡å‹é…ç½® (default, small, large)')
    parser.add_argument('--data_dir', type=str, default='./data',
                       help='æ•¸æ“šç›®éŒ„è·¯å¾‘')
    parser.add_argument('--save_dir', type=str, default='./sequential_training_enhanced',
                       help='ä¿å­˜ç›®éŒ„è·¯å¾‘')
    
    # è¨“ç·´è¨­ç½®
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•¸æ“šåŠ è¼‰å·¥ä½œç·šç¨‹æ•¸')
    parser.add_argument('--device', type=str, default=None,
                       help='è¨ˆç®—è¨­å‚™ (cuda/cpu)')
    
    # éšæ®µè¨“ç·´è¨­ç½®
    parser.add_argument('--stage1_epochs', type=int, default=50,
                       help='éšæ®µ1 (åˆ†å‰²) è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--stage2_epochs', type=int, default=40,
                       help='éšæ®µ2 (æª¢æ¸¬) è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--stage3_epochs', type=int, default=30,
                       help='éšæ®µ3 (åˆ†é¡) è¨“ç·´è¼ªæ•¸')
    
    # å¢å¼·é˜²éºå¿˜ç­–ç•¥åƒæ•¸
    parser.add_argument('--ewc_importance', type=float, default=5000.0,
                       help='EWCé‡è¦æ€§æ¬Šé‡')
    parser.add_argument('--stage1_lr', type=float, default=1e-3,
                       help='éšæ®µ1å­¸ç¿’ç‡')
    parser.add_argument('--stage2_lr', type=float, default=1e-4,
                       help='éšæ®µ2å­¸ç¿’ç‡ (é™ä½10å€)')
    parser.add_argument('--stage3_lr', type=float, default=1e-5,
                       help='éšæ®µ3å­¸ç¿’ç‡ (é™ä½100å€)')
    parser.add_argument('--weight_decay', type=float, default=1e-3,
                       help='æ¬Šé‡è¡°æ¸› (å¢å¼·10å€)')
    parser.add_argument('--freeze_backbone_layers', action='store_true', default=True,
                       help='å‡çµéª¨å¹¹ç¶²è·¯åº•å±¤')
    parser.add_argument('--dropout_rate', type=float, default=0.3,
                       help='Dropoutç‡')
    
    # EWCè¨­ç½®
    parser.add_argument('--ewc_type', type=str, default='l2',
                       choices=['l2', 'online'],
                       help='EWCé¡å‹')
    parser.add_argument('--adaptive_ewc', action='store_true', default=True,
                       help='ä½¿ç”¨è‡ªé©æ‡‰EWCæ¬Šé‡')
    parser.add_argument('--forgetting_threshold', type=float, default=0.05,
                       help='å¯æ¥å—çš„éºå¿˜ç‡é–¾å€¼')
    
    # å…¶ä»–è¨­ç½®
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='è©³ç´°è¼¸å‡º')
    parser.add_argument('--debug', action='store_true',
                       help='èª¿è©¦æ¨¡å¼')
    parser.add_argument('--dry_run', action='store_true',
                       help='ç©ºé‹è¡Œ')
    
    # æ–°å¢é¸é …
    parser.add_argument('--ewc_samples', type=int, default=100,
                       help='è¨ˆç®—FisherçŸ©é™£çš„æ¨£æœ¬æ•¸')
    parser.add_argument('--gradient_clip', type=float, default=1.0,
                       help='æ¢¯åº¦è£å‰ªå€¼')
    parser.add_argument('--learning_rate', type=float, default=None,
                       help='çµ±ä¸€å­¸ç¿’ç‡ï¼ˆè¦†è“‹éšæ®µå­¸ç¿’ç‡ï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æœæä¾›äº†çµ±ä¸€å­¸ç¿’ç‡ï¼Œè¦†è“‹éšæ®µå­¸ç¿’ç‡
    if args.learning_rate is not None:
        args.stage1_lr = args.learning_rate
        args.stage2_lr = args.learning_rate / 10  # éšæ®µ2é™ä½10å€
        args.stage3_lr = args.learning_rate / 100  # éšæ®µ3é™ä½100å€
    
    # èª¿è©¦æ¨¡å¼è¨­ç½®
    if args.debug:
        print("ğŸ› èª¿è©¦æ¨¡å¼å•Ÿç”¨")
        args.stage1_epochs = min(args.stage1_epochs, 3)
        args.stage2_epochs = min(args.stage2_epochs, 3)
        args.stage3_epochs = min(args.stage3_epochs, 3)
        args.batch_size = 4
        args.num_workers = 0
    
    return args


def setup_environment(args):
    """è¨­ç½®è¨“ç·´ç’°å¢ƒ"""
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # è¨­ç½®è¨­å‚™
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    if not args.dry_run:
        os.makedirs(args.save_dir, exist_ok=True)
        
        # ä¿å­˜è¨“ç·´é…ç½®
        config_path = os.path.join(args.save_dir, 'training_config.json')
        with open(config_path, 'w') as f:
            json.dump(vars(args), f, indent=2)
    
    return args


def freeze_backbone_layers(model, layers_to_freeze=['layer1', 'layer2']):
    """å‡çµéª¨å¹¹ç¶²è·¯çš„åº•å±¤åƒæ•¸"""
    frozen_count = 0
    total_count = 0
    
    for name, param in model.backbone.named_parameters():
        total_count += 1
        if any(layer in name for layer in layers_to_freeze):
            param.requires_grad = False
            frozen_count += 1
    
    print(f"  - å‡çµåƒæ•¸: {frozen_count}/{total_count} å€‹éª¨å¹¹ç¶²è·¯åƒæ•¸")
    
    # è¨ˆç®—å‰©é¤˜å¯è¨“ç·´åƒæ•¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  - å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    return model


def add_dropout_to_model(model, dropout_rate=0.3):
    """ç‚ºæ¨¡å‹æ·»åŠ Dropoutå±¤å¢å¼·æ­£å‰‡åŒ–"""
    # åœ¨é ­éƒ¨ç¶²è·¯ä¸­æ·»åŠ Dropout
    if hasattr(model.head, 'shared_conv'):
        # åœ¨å…±äº«å·ç©å±¤å¾Œæ·»åŠ Dropout
        old_shared = model.head.shared_conv
        model.head.shared_conv = nn.Sequential(
            old_shared,
            nn.Dropout2d(dropout_rate)
        )
    
    print(f"  - æ·»åŠ Dropoutå±¤: rate={dropout_rate}")
    return model


class EnhancedSequentialTrainer:
    """å¢å¼·çš„ä¾åºè¨“ç·´å™¨ï¼Œå¯¦æ–½æ–°çš„é˜²éºå¿˜ç­–ç•¥"""
    
    def __init__(self, base_trainer, stage_lrs, weight_decay, gradient_clip):
        self.base_trainer = base_trainer
        self.stage_lrs = stage_lrs  # éšæ®µå­¸ç¿’ç‡å­—å…¸
        self.weight_decay = weight_decay
        self.gradient_clip = gradient_clip
        self.current_stage = 0
        
    def train_stage(self, stage_name, task_type, epochs):
        """ä½¿ç”¨éšæ®µç‰¹å®šçš„å­¸ç¿’ç‡è¨“ç·´"""
        # ç²å–ç•¶å‰éšæ®µçš„å­¸ç¿’ç‡
        stage_lr = self.stage_lrs.get(self.current_stage, 1e-3)
        print(f"\n  ğŸ“Œ ä½¿ç”¨å­¸ç¿’ç‡: {stage_lr} (éšæ®µ{self.current_stage + 1})")
        
        # æ›´æ–°å„ªåŒ–å™¨
        self.base_trainer.optimizer = optim.AdamW(
            self.base_trainer.model.parameters(),
            lr=stage_lr,
            weight_decay=self.weight_decay
        )
        
        # è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨
        self.base_trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.base_trainer.optimizer,
            T_max=epochs,
            eta_min=stage_lr * 0.1
        )
        
        # èª¿ç”¨åŸºç¤è¨“ç·´å™¨çš„è¨“ç·´æ–¹æ³•
        results = self.base_trainer.train_stage(stage_name, task_type, epochs)
        
        # æ›´æ–°éšæ®µè¨ˆæ•¸
        self.current_stage += 1
        
        return results
    
    def __getattr__(self, name):
        """å§”è¨—çµ¦åŸºç¤è¨“ç·´å™¨"""
        return getattr(self.base_trainer, name)


def create_enhanced_trainer(model, dataloaders, ewc, args):
    """å‰µå»ºå¢å¼·çš„ä¾åºè¨“ç·´å™¨"""
    # æº–å‚™è¨“ç·´å™¨æ•¸æ“šåŠ è¼‰å™¨
    trainer_dataloaders = {
        'segmentation_train': dataloaders['train'].get_task_loader('segmentation'),
        'segmentation_val': dataloaders['val'].get_task_loader('segmentation'),
        'detection_train': dataloaders['train'].get_task_loader('detection'),
        'detection_val': dataloaders['val'].get_task_loader('detection'),
        'classification_train': dataloaders['train'].get_task_loader('classification'),
        'classification_val': dataloaders['val'].get_task_loader('classification')
    }
    
    # å‰µå»ºåŸºç¤è¨“ç·´å™¨
    base_trainer = create_sequential_trainer(
        model=model,
        dataloaders=trainer_dataloaders,
        ewc_importance=args.ewc_importance,
        learning_rate=args.stage1_lr,  # åˆå§‹å­¸ç¿’ç‡
        device=args.device,
        save_dir=args.save_dir if not args.dry_run else './checkpoints',
        adaptive_ewc=args.adaptive_ewc,
        forgetting_threshold=args.forgetting_threshold
    )
    
    # ä½¿ç”¨å¢å¼·çš„EWC
    base_trainer.ewc = ewc
    
    # å‰µå»ºéšæ®µå­¸ç¿’ç‡å­—å…¸
    stage_lrs = {
        0: args.stage1_lr,
        1: args.stage2_lr,
        2: args.stage3_lr
    }
    
    # å‰µå»ºå¢å¼·è¨“ç·´å™¨
    enhanced_trainer = EnhancedSequentialTrainer(
        base_trainer=base_trainer,
        stage_lrs=stage_lrs,
        weight_decay=args.weight_decay,
        gradient_clip=args.gradient_clip
    )
    
    return enhanced_trainer


def train_with_enhanced_protection(trainer, args):
    """ä½¿ç”¨å¢å¼·ä¿è­·é€²è¡Œè¨“ç·´"""
    print("\nğŸš€ é–‹å§‹ä¾åºè¨“ç·´ï¼ˆå¢å¼·é˜²éºå¿˜ä¿è­·ï¼‰")
    print("="*60)
    print("é˜²éºå¿˜ç­–ç•¥:")
    print(f"  1. å­¸ç¿’ç‡èª¿åº¦: {args.stage1_lr} â†’ {args.stage2_lr} â†’ {args.stage3_lr}")
    print(f"  2. åƒæ•¸å‡çµ: {'å•Ÿç”¨' if args.freeze_backbone_layers else 'ç¦ç”¨'}")
    print(f"  3. æ¬Šé‡è¡°æ¸›: {args.weight_decay}")
    print(f"  4. Dropout: {args.dropout_rate}")
    print(f"  5. EWCæ¬Šé‡: {args.ewc_importance}")
    print("="*60)
    
    start_time = time.time()
    results = {}
    
    # éšæ®µ1ï¼šåˆ†å‰²ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ1ï¼šåˆ†å‰²ä»»å‹™è¨“ç·´ ({args.stage1_epochs} epochs)")
    stage1_metrics = trainer.train_stage(
        stage_name='stage1_segmentation',
        task_type='segmentation',
        epochs=args.stage1_epochs
    )
    results['stage1'] = stage1_metrics
    
    # æª¢æŸ¥Stage 1æ€§èƒ½
    print(f"\nâœ… Stage 1 å®Œæˆ:")
    print(f"  - æœ€ä½³mIoU: {stage1_metrics.get('best_metric', 0.0):.4f}")
    stage1_time = time.time() - start_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage1_time/60:.1f}åˆ†é˜")
    
    # éšæ®µ2ï¼šæª¢æ¸¬ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ2ï¼šæª¢æ¸¬ä»»å‹™è¨“ç·´ ({args.stage2_epochs} epochs)")
    print(f"  - EWCä¿è­·å•Ÿå‹•")
    print(f"  - å­¸ç¿’ç‡é™ä½10å€: {args.stage2_lr}")
    
    stage2_metrics = trainer.train_stage(
        stage_name='stage2_detection',
        task_type='detection',
        epochs=args.stage2_epochs
    )
    results['stage2'] = stage2_metrics
    
    # æª¢æŸ¥éºå¿˜æƒ…æ³
    print(f"\nâœ… Stage 2 å®Œæˆ:")
    print(f"  - æœ€ä½³mAP: {stage2_metrics.get('best_metric', 0.0):.4f}")
    stage2_time = time.time() - start_time - stage1_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage2_time/60:.1f}åˆ†é˜")
    
    # è¨ˆç®—åˆ†å‰²ä»»å‹™çš„éºå¿˜ç‡
    with torch.no_grad():
        trainer.model.eval()
        current_seg_perf = trainer._validate_segmentation(trainer.dataloaders['segmentation_val'])
    seg_baseline = trainer.baseline_performance.get('segmentation', {}).get('main_metric', 0.0)
    seg_current = current_seg_perf.get('main_metric', current_seg_perf.get('miou', 0.0))
    seg_forgetting = (seg_baseline - seg_current) / seg_baseline if seg_baseline > 0 else 0
    print(f"  - åˆ†å‰²ä»»å‹™éºå¿˜ç‡: {seg_forgetting:.2%} (å¾ {seg_baseline:.4f} åˆ° {seg_current:.4f})")
    
    # éšæ®µ3ï¼šåˆ†é¡ä»»å‹™
    print(f"\nğŸ“Œ éšæ®µ3ï¼šåˆ†é¡ä»»å‹™è¨“ç·´ ({args.stage3_epochs} epochs)")
    print(f"  - å®Œæ•´EWCä¿è­·")
    print(f"  - å­¸ç¿’ç‡é™ä½100å€: {args.stage3_lr}")
    
    stage3_metrics = trainer.train_stage(
        stage_name='stage3_classification',
        task_type='classification',
        epochs=args.stage3_epochs
    )
    results['stage3'] = stage3_metrics
    
    # æœ€çµ‚è©•ä¼°
    total_time = time.time() - start_time
    print(f"\nâœ… Stage 3 å®Œæˆ:")
    print(f"  - æœ€ä½³æº–ç¢ºç‡: {stage3_metrics.get('best_metric', 0.0):.4f}")
    stage3_time = time.time() - start_time - stage1_time - stage2_time
    print(f"  - è¨“ç·´æ™‚é–“: {stage3_time/60:.1f}åˆ†é˜")
    
    # è¨ˆç®—ç¸½éºå¿˜ç‡
    total_forgetting = 0.0
    forgetting_details = {}
    num_tasks = 0
    
    for task in trainer.completed_tasks[:-1]:  # ä¸åŒ…æ‹¬æœ€å¾Œä¸€å€‹ä»»å‹™
        if task in trainer.baseline_performance:
            with torch.no_grad():
                trainer.model.eval()
                if task == 'segmentation':
                    current_perf = trainer._validate_segmentation(trainer.dataloaders[f'{task}_val'])
                elif task == 'detection':
                    current_perf = trainer._validate_detection(trainer.dataloaders[f'{task}_val'])
                elif task == 'classification':
                    current_perf = trainer._validate_classification(trainer.dataloaders[f'{task}_val'])
            
            baseline = trainer.baseline_performance[task].get('main_metric', 0.0)
            current = current_perf.get('main_metric', current_perf.get('miou' if task == 'segmentation' else 'map' if task == 'detection' else 'accuracy', 0.0))
            forgetting = (baseline - current) / baseline if baseline > 0 else 0
            forgetting_details[task] = {
                'baseline': baseline,
                'current': current,
                'forgetting': forgetting
            }
            total_forgetting += forgetting
            num_tasks += 1
    
    total_forgetting = total_forgetting / num_tasks if num_tasks > 0 else 0
    
    # æ‰“å°è¨“ç·´ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ“Š è¨“ç·´ç¸½çµ")
    print("="*60)
    print(f"ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f} åˆ†é˜")
    print(f"ç¸½éºå¿˜ç‡: {total_forgetting:.2%}")
    print(f"æœ€çµ‚EWCæ¬Šé‡: {trainer.ewc.adaptive_importance:.1f}")
    
    # å„ä»»å‹™æœ€çµ‚æ€§èƒ½
    print("\nå„ä»»å‹™æœ€çµ‚æ€§èƒ½:")
    final_metrics = {}
    for task in trainer.completed_tasks:
        if f'{task}_val' in trainer.dataloaders:
            with torch.no_grad():
                trainer.model.eval()
                if task == 'segmentation':
                    perf = trainer._validate_segmentation(trainer.dataloaders[f'{task}_val'])
                elif task == 'detection':
                    perf = trainer._validate_detection(trainer.dataloaders[f'{task}_val'])
                elif task == 'classification':
                    perf = trainer._validate_classification(trainer.dataloaders[f'{task}_val'])
            
            metric_value = perf.get('main_metric', 
                                   perf.get('miou' if task == 'segmentation' else 
                                          'map' if task == 'detection' else 
                                          'accuracy', 0.0))
            final_metrics[task] = metric_value
            print(f"  - {task}: {metric_value:.4f}")
    
    # è©³ç´°éºå¿˜åˆ†æ
    print("\néºå¿˜ç‡è©³æƒ…:")
    for task, details in forgetting_details.items():
        print(f"  - {task}: {details['baseline']:.4f} â†’ {details['current']:.4f} (éºå¿˜ç‡: {details['forgetting']:.2%})")
    
    # æª¢æŸ¥æ˜¯å¦æ»¿è¶³è¦æ±‚
    if total_forgetting <= args.forgetting_threshold:
        print(f"\nâœ… æˆåŠŸï¼éºå¿˜ç‡ {total_forgetting:.2%} â‰¤ {args.forgetting_threshold:.1%}")
    else:
        print(f"\nâŒ å¤±æ•—ï¼éºå¿˜ç‡ {total_forgetting:.2%} > {args.forgetting_threshold:.1%}")
    
    # ä¿å­˜æœ€çµ‚çµæœ
    if not args.dry_run:
        results['summary'] = {
            'total_time': total_time,
            'total_forgetting': total_forgetting,
            'forgetting_details': forgetting_details,
            'final_ewc_weight': trainer.ewc.adaptive_importance,
            'final_metrics': final_metrics,
            'success': total_forgetting <= args.forgetting_threshold,
            'stage_times': {
                'stage1': stage1_time,
                'stage2': stage2_time,
                'stage3': stage3_time
            },
            'strategy': {
                'stage_lrs': [args.stage1_lr, args.stage2_lr, args.stage3_lr],
                'weight_decay': args.weight_decay,
                'dropout': args.dropout_rate,
                'frozen_layers': args.freeze_backbone_layers,
                'ewc_importance': args.ewc_importance
            }
        }
        
        results_path = os.path.join(args.save_dir, 'training_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {results_path}")
    
    return results


def main():
    """ä¸»å‡½æ•¸"""
    # è§£æåƒæ•¸
    args = parse_args()
    
    # è¨­ç½®ç’°å¢ƒ
    args = setup_environment(args)
    
    print("ğŸš€ å•Ÿå‹•ä¾åºå¤šä»»å‹™å­¸ç¿’è¨“ç·´ï¼ˆå¢å¼·é˜²éºå¿˜ç‰ˆæœ¬ï¼‰")
    print("="*60)
    print(f"è¨“ç·´é…ç½®:")
    print(f"  - æ¨¡å‹: {args.model_config}")
    print(f"  - è¨­å‚™: {args.device}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - é˜²éºå¿˜ç­–ç•¥:")
    print(f"    * å­¸ç¿’ç‡èª¿åº¦: {args.stage1_lr} â†’ {args.stage2_lr} â†’ {args.stage3_lr}")
    print(f"    * åƒæ•¸å‡çµ: {'æ˜¯' if args.freeze_backbone_layers else 'å¦'}")
    print(f"    * æ¬Šé‡è¡°æ¸›: {args.weight_decay}")
    print(f"    * Dropout: {args.dropout_rate}")
    print(f"    * EWCæ¬Šé‡: {args.ewc_importance}")
    print(f"  - éºå¿˜ç‡é–¾å€¼: {args.forgetting_threshold:.1%}")
    print(f"  - è¨“ç·´è¼ªæ•¸: {args.stage1_epochs}/{args.stage2_epochs}/{args.stage3_epochs}")
    print("="*60)
    
    # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
    print("\nğŸ”§ å‰µå»ºæ¨¡å‹å’Œæ•¸æ“šåŠ è¼‰å™¨...")
    model = create_unified_model(args.model_config)
    model = model.to(args.device)
    
    # å‡çµéª¨å¹¹ç¶²è·¯åº•å±¤ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    if args.freeze_backbone_layers:
        print("\nğŸ”’ å‡çµéª¨å¹¹ç¶²è·¯åº•å±¤åƒæ•¸...")
        model = freeze_backbone_layers(model)
    
    # æ·»åŠ Dropoutå¢å¼·æ­£å‰‡åŒ–
    print("\nğŸ’§ æ·»åŠ Dropoutå±¤...")
    model = add_dropout_to_model(model, args.dropout_rate)
    
    # çµ±è¨ˆæ¨¡å‹åƒæ•¸
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
    print(f"  - ç¸½åƒæ•¸é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"  - å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        sampling_strategy='balanced',
        task_weights=[1.0, 1.0, 1.0]
    )
    
    # å‰µå»ºå¢å¼·çš„EWCè™•ç†å™¨
    print(f"\nğŸ›¡ï¸ å‰µå»ºEWCè™•ç†å™¨...")
    ewc = create_ewc_handler(
        model=model,
        importance=args.ewc_importance,
        ewc_type=args.ewc_type
    )
    
    # å‰µå»ºå¢å¼·çš„è¨“ç·´å™¨
    trainer = create_enhanced_trainer(model, dataloaders, ewc, args)
    
    # é–‹å§‹è¨“ç·´
    results = train_with_enhanced_protection(trainer, args)
    
    print("\nâœ¨ è¨“ç·´å®Œæˆï¼")
    
    # è¿”å›æˆåŠŸç‹€æ…‹
    return 0 if results['summary']['success'] else 1


if __name__ == "__main__":
    exit(main())