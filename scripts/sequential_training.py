#!/usr/bin/env python3
"""
ä¾åºè¨“ç·´è…³æœ¬
å¯¦ç¾å®Œæ•´çš„ä¸‰éšæ®µä¾åºè¨“ç·´æµç¨‹ï¼šåˆ†å‰² â†’ æª¢æ¸¬ â†’ åˆ†é¡
é›†æˆEWCé˜²éºå¿˜æ©Ÿåˆ¶å’Œæ€§èƒ½ç›£æ§ç³»çµ±
"""
import os
import sys
import time
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import json

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.unified_model import create_unified_model
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.utils.sequential_trainer import create_sequential_trainer


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='Sequential Multi-Task Training')
    
    # æ¨¡å‹åƒæ•¸
    parser.add_argument('--model_config', type=str, default='default',
                      help='æ¨¡å‹é…ç½® (default, lightweight, large)')
    parser.add_argument('--pretrained', action='store_true',
                      help='ä½¿ç”¨é è¨“ç·´æ¬Šé‡')
    
    # è¨“ç·´åƒæ•¸
    parser.add_argument('--batch_size', type=int, default=16,
                      help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                      help='å­¸ç¿’ç‡')
    parser.add_argument('--stage1_epochs', type=int, default=50,
                      help='éšæ®µ1è¨“ç·´è¼ªæ•¸ (åˆ†å‰²)')
    parser.add_argument('--stage2_epochs', type=int, default=40,
                      help='éšæ®µ2è¨“ç·´è¼ªæ•¸ (æª¢æ¸¬)')
    parser.add_argument('--stage3_epochs', type=int, default=30,
                      help='éšæ®µ3è¨“ç·´è¼ªæ•¸ (åˆ†é¡)')
    
    # EWCåƒæ•¸
    parser.add_argument('--ewc_importance', type=float, default=1000.0,
                      help='EWCé‡è¦æ€§æ¬Šé‡')
    parser.add_argument('--adaptive_ewc', action='store_true', default=True,
                      help='ä½¿ç”¨è‡ªé©æ‡‰EWCæ¬Šé‡èª¿æ•´')
    parser.add_argument('--forgetting_threshold', type=float, default=0.05,
                      help='å¯æ¥å—çš„éºå¿˜é–¾å€¼ (5%)')
    
    # æ•¸æ“šåƒæ•¸
    parser.add_argument('--data_dir', type=str, default='./data',
                      help='æ•¸æ“šç›®éŒ„')
    parser.add_argument('--num_workers', type=int, default=4,
                      help='æ•¸æ“šåŠ è¼‰å·¥ä½œé€²ç¨‹æ•¸')
    
    # ä¿å­˜åƒæ•¸
    parser.add_argument('--save_dir', type=str, default='./sequential_training_results',
                      help='çµæœä¿å­˜ç›®éŒ„')
    parser.add_argument('--save_interval', type=int, default=10,
                      help='æª¢æŸ¥é»ä¿å­˜é–“éš”')
    
    # è¨­å‚™åƒæ•¸
    parser.add_argument('--device', type=str, default='auto',
                      help='è¨“ç·´è¨­å‚™ (auto, cpu, cuda, cuda:0)')
    parser.add_argument('--mixed_precision', action='store_true',
                      help='ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´')
    
    # èª¿è©¦åƒæ•¸
    parser.add_argument('--debug', action='store_true',
                      help='èª¿è©¦æ¨¡å¼ (ä½¿ç”¨æ›´å°‘çš„æ•¸æ“š)')
    parser.add_argument('--dry_run', action='store_true',
                      help='è©¦é‹è¡Œæ¨¡å¼ (åªé‹è¡Œå¹¾å€‹batch)')
    
    return parser.parse_args()


def setup_device(device_arg: str) -> str:
    """è¨­ç½®è¨“ç·´è¨­å‚™"""
    if device_arg == 'auto':
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"ğŸš€ è‡ªå‹•é¸æ“‡è¨­å‚™: {device}")
            print(f"ğŸ“± GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ GPUè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            device = 'cpu'
            print(f"ğŸ’» è‡ªå‹•é¸æ“‡è¨­å‚™: {device}")
    else:
        device = device_arg
        print(f"ğŸ¯ æŒ‡å®šè¨­å‚™: {device}")
    
    return device


def create_dataloaders(args):
    """å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨"""
    print("ğŸ“‚ å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨...")
    
    # æ•¸æ“šé…ç½®
    data_config = {
        'data_dir': args.data_dir,
        'batch_size': args.batch_size,
        'num_workers': args.num_workers,
        'sampling_strategy': 'balanced',
        'task_weights': [1.0, 1.0, 1.0]
    }
    
    if args.debug:
        print("ğŸ› èª¿è©¦æ¨¡å¼ï¼šä½¿ç”¨è¼ƒå°‘æ•¸æ“š")
    
    # å‰µå»ºçµ±ä¸€æ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_unified_dataloaders(**data_config)
    
    # æ‰“å°æ•¸æ“šçµ±è¨ˆ
    for name, loader in dataloaders.items():
        if loader is not None:
            print(f"  {name}: {len(loader)} batches")
    
    return dataloaders


def monitor_performance():
    """æ€§èƒ½ç›£æ§å‡½æ•¸"""
    def calculate_miou(predictions, targets):
        """è¨ˆç®—mIoU"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„mIoUè¨ˆç®—
        # ç°¡åŒ–ç‰ˆæœ¬
        return 0.75
    
    def calculate_map(predictions, targets):
        """è¨ˆç®—mAP"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„mAPè¨ˆç®—
        # ç°¡åŒ–ç‰ˆæœ¬
        return 0.65
    
    def calculate_top1(predictions, targets):
        """è¨ˆç®—Top-1æº–ç¢ºç‡"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„æº–ç¢ºç‡è¨ˆç®—
        # ç°¡åŒ–ç‰ˆæœ¬
        return 0.85
    
    current_metrics = {
        'segmentation_miou': calculate_miou(None, None),
        'detection_map': calculate_map(None, None),
        'classification_top1': calculate_top1(None, None)
    }
    
    # æ¨¡æ“¬åŸºæº–æ€§èƒ½
    baseline_miou = 0.78
    baseline_map = 0.68
    baseline_top1 = 0.87
    
    forgetting_check = {
        'seg_drop': baseline_miou - current_metrics['segmentation_miou'],
        'det_drop': baseline_map - current_metrics['detection_map'],
        'cls_drop': baseline_top1 - current_metrics['classification_top1'],
        'acceptable': True  # å¦‚æœæ‰€æœ‰ä¸‹é™ <= 5%
    }
    
    # æª¢æŸ¥æ˜¯å¦å¯æ¥å—
    all_drops = [
        forgetting_check['seg_drop'],
        forgetting_check['det_drop'], 
        forgetting_check['cls_drop']
    ]
    max_drop = max(all_drops)
    forgetting_check['acceptable'] = max_drop <= 0.05
    
    return current_metrics, forgetting_check


def run_sequential_training(args):
    """é‹è¡Œä¾åºè¨“ç·´"""
    print("ğŸš€ é–‹å§‹ä¾åºè¨“ç·´...")
    print("=" * 70)
    
    # è¨­ç½®è¨­å‚™
    device = setup_device(args.device)
    
    # å‰µå»ºä¿å­˜ç›®éŒ„
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ çµæœä¿å­˜åˆ°: {save_dir}")
    
    # ä¿å­˜è¨“ç·´é…ç½®
    config_path = save_dir / 'training_config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(vars(args), f, indent=2, ensure_ascii=False)
    print(f"âš™ï¸ è¨“ç·´é…ç½®ä¿å­˜åˆ°: {config_path}")
    
    # å‰µå»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ å‰µå»ºçµ±ä¸€å¤šä»»å‹™æ¨¡å‹ (é…ç½®: {args.model_config})...")
    model = create_unified_model(args.model_config)
    
    # æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹åƒæ•¸: {total_params:,} ç¸½è¨ˆ, {trainable_params:,} å¯è¨“ç·´")
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    dataloaders = create_dataloaders(args)
    
    # å‰µå»ºä¾åºè¨“ç·´å™¨
    # é‡æ§‹æ•¸æ“šåŠ è¼‰å™¨ä»¥ç¬¦åˆé æœŸæ ¼å¼
    trainer_dataloaders = {
        'segmentation_train': dataloaders['train'].get_task_loader('segmentation'),
        'segmentation_val': dataloaders['val'].get_task_loader('segmentation'),
        'detection_train': dataloaders['train'].get_task_loader('detection'),
        'detection_val': dataloaders['val'].get_task_loader('detection'),
        'classification_train': dataloaders['train'].get_task_loader('classification'),
        'classification_val': dataloaders['val'].get_task_loader('classification'),
        'unified_train': dataloaders['train'].unified_loader,
        'unified_val': dataloaders['val'].unified_loader
    }
    
    print(f"\nğŸ”§ å‰µå»ºä¾åºè¨“ç·´å™¨...")
    trainer = create_sequential_trainer(
        model=model,
        dataloaders=trainer_dataloaders,
        ewc_importance=args.ewc_importance,
        save_dir=str(save_dir),
        device=device,
        learning_rate=args.learning_rate,
        adaptive_ewc=args.adaptive_ewc,
        forgetting_threshold=args.forgetting_threshold
    )
    
    print(f"  EWCé‡è¦æ€§æ¬Šé‡: {args.ewc_importance}")
    print(f"  è‡ªé©æ‡‰EWC: {'é–‹å•Ÿ' if args.adaptive_ewc else 'é—œé–‰'}")
    print(f"  éºå¿˜é–¾å€¼: {args.forgetting_threshold*100:.1f}%")
    
    # ä¸‰éšæ®µè¨“ç·´æµç¨‹
    print("\n" + "=" * 70)
    print("ğŸ¯ é–‹å§‹ä¸‰éšæ®µä¾åºè¨“ç·´æµç¨‹")
    print("=" * 70)
    
    training_start_time = time.time()
    stage_results = {}
    
    try:
        # Stage 1: åˆ†å‰²ä»»å‹™è¨“ç·´
        print(f"\nğŸ¨ Stage 1: åˆ†å‰²ä»»å‹™è¨“ç·´")
        print(f"ğŸ“Š è¨“ç·´è¼ªæ•¸: {args.stage1_epochs}")
        stage1_start = time.time()
        
        stage1_metrics = trainer.train_stage(
            stage_name='stage1_segmentation',
            task_type='segmentation',
            epochs=args.stage1_epochs,
            save_checkpoints=True
        )
        
        stage1_time = time.time() - stage1_start
        stage_results['stage1'] = {
            'metrics': stage1_metrics,
            'time': stage1_time
        }
        
        print(f"âœ… Stage 1 å®Œæˆï¼ç”¨æ™‚: {stage1_time/60:.1f}åˆ†é˜")
        print(f"ğŸ“Š åˆ†å‰²mIoUåŸºæº–: {stage1_metrics['main_metric']:.4f}")
        
        if args.dry_run:
            print("ğŸ§ª è©¦é‹è¡Œæ¨¡å¼ï¼šåœæ­¢åœ¨Stage 1")
            return stage_results
        
        # Stage 2: æª¢æ¸¬ä»»å‹™è¨“ç·´ + EWC
        print(f"\nğŸ¯ Stage 2: æª¢æ¸¬ä»»å‹™è¨“ç·´ + EWC")
        print(f"ğŸ“Š è¨“ç·´è¼ªæ•¸: {args.stage2_epochs}")
        stage2_start = time.time()
        
        stage2_metrics = trainer.train_stage(
            stage_name='stage2_detection',
            task_type='detection',
            epochs=args.stage2_epochs,
            save_checkpoints=True
        )
        
        stage2_time = time.time() - stage2_start
        stage_results['stage2'] = {
            'metrics': stage2_metrics,
            'time': stage2_time
        }
        
        print(f"âœ… Stage 2 å®Œæˆï¼ç”¨æ™‚: {stage2_time/60:.1f}åˆ†é˜")
        print(f"ğŸ“Š æª¢æ¸¬mAP: {stage2_metrics['main_metric']:.4f}")
        
        # æª¢æŸ¥éºå¿˜ - Stage 2å¾Œ
        print(f"\nğŸ” Stage 2å¾Œéºå¿˜æª¢æŸ¥...")
        forgetting_info_s2 = trainer.check_forgetting()
        stage_results['stage2']['forgetting'] = forgetting_info_s2
        
        # Stage 3: åˆ†é¡ä»»å‹™è¨“ç·´ + EWC
        print(f"\nğŸ“Š Stage 3: åˆ†é¡ä»»å‹™è¨“ç·´ + EWC")
        print(f"ğŸ“Š è¨“ç·´è¼ªæ•¸: {args.stage3_epochs}")
        stage3_start = time.time()
        
        stage3_metrics = trainer.train_stage(
            stage_name='stage3_classification',
            task_type='classification',
            epochs=args.stage3_epochs,
            save_checkpoints=True
        )
        
        stage3_time = time.time() - stage3_start
        stage_results['stage3'] = {
            'metrics': stage3_metrics,
            'time': stage3_time
        }
        
        print(f"âœ… Stage 3 å®Œæˆï¼ç”¨æ™‚: {stage3_time/60:.1f}åˆ†é˜")
        print(f"ğŸ“Š åˆ†é¡æº–ç¢ºç‡: {stage3_metrics['main_metric']:.4f}")
        
        # æœ€çµ‚éºå¿˜æª¢æŸ¥
        print(f"\nğŸ” æœ€çµ‚éºå¿˜æª¢æŸ¥...")
        final_forgetting_info = trainer.check_forgetting()
        stage_results['stage3']['forgetting'] = final_forgetting_info
        
        # æœ€çµ‚æ€§èƒ½è©•ä¼°
        print(f"\nğŸ“Š æœ€çµ‚æ€§èƒ½è©•ä¼°...")
        final_metrics = trainer.evaluate_all_tasks()
        stage_results['final_metrics'] = final_metrics
        
        total_time = time.time() - training_start_time
        stage_results['total_time'] = total_time
        
        print(f"\nğŸ‰ ä¾åºè¨“ç·´å®Œæˆï¼ç¸½ç”¨æ™‚: {total_time/60:.1f}åˆ†é˜")
        
        # æ‰“å°æœ€çµ‚çµæœç¸½çµ
        print_final_summary(stage_results, trainer)
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        total_time = time.time() - training_start_time
        stage_results['interrupted'] = True
        stage_results['total_time'] = total_time
    
    except Exception as e:
        print(f"\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        total_time = time.time() - training_start_time
        stage_results['error'] = str(e)
        stage_results['total_time'] = total_time
    
    finally:
        # ä¿å­˜è¨“ç·´çµæœå’Œæ­·å²
        print(f"\nğŸ’¾ ä¿å­˜è¨“ç·´çµæœ...")
        
        # ä¿å­˜çµæœæ‘˜è¦
        results_path = save_dir / 'training_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            # è½‰æ›tensorç‚ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_results = {}
            for key, value in stage_results.items():
                if isinstance(value, dict):
                    serializable_results[key] = {}
                    for k, v in value.items():
                        if torch.is_tensor(v):
                            serializable_results[key][k] = v.item()
                        else:
                            serializable_results[key][k] = v
                else:
                    serializable_results[key] = value
            
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š çµæœæ‘˜è¦ä¿å­˜åˆ°: {results_path}")
        
        # ä¿å­˜è¨“ç·´æ­·å²
        trainer.save_training_history()
        
        # ç¹ªè£½è¨“ç·´æ›²ç·š
        try:
            trainer.plot_training_curves()
        except Exception as e:
            print(f"âš ï¸ ç¹ªè£½è¨“ç·´æ›²ç·šå¤±æ•—: {e}")
        
        print(f"ğŸ’¾ æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°: {save_dir}")
    
    return stage_results


def print_final_summary(results: dict, trainer):
    """æ‰“å°æœ€çµ‚çµæœç¸½çµ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ ä¾åºè¨“ç·´çµæœç¸½çµ")
    print("=" * 70)
    
    # è¨“ç·´éšæ®µçµæœ
    for stage in ['stage1', 'stage2', 'stage3']:
        if stage in results:
            stage_info = results[stage]
            metrics = stage_info['metrics']
            time_taken = stage_info['time']
            
            stage_names = {
                'stage1': 'ğŸ¨ Stage 1 (åˆ†å‰²)',
                'stage2': 'ğŸ¯ Stage 2 (æª¢æ¸¬)',
                'stage3': 'ğŸ“Š Stage 3 (åˆ†é¡)'
            }
            
            print(f"{stage_names[stage]}:")
            print(f"  æ€§èƒ½æŒ‡æ¨™: {metrics['main_metric']:.4f}")
            print(f"  è¨“ç·´æ™‚é–“: {time_taken/60:.1f}åˆ†é˜")
            
            if 'forgetting' in stage_info:
                forgetting = stage_info['forgetting']
                print(f"  éºå¿˜ç¨‹åº¦: {forgetting['max_drop']*100:.2f}%")
                print(f"  éºå¿˜æª¢æŸ¥: {'âœ… é€šé' if forgetting['acceptable'] else 'âŒ å¤±æ•—'}")
    
    # æœ€çµ‚æ€§èƒ½
    if 'final_metrics' in results:
        print(f"\nğŸ“Š æœ€çµ‚æ€§èƒ½:")
        final_metrics = results['final_metrics']
        for task, metrics in final_metrics.items():
            print(f"  {task}: {metrics['main_metric']:.4f}")
    
    # åŸºæº–æ¯”è¼ƒ
    print(f"\nğŸ“ˆ åŸºæº–æ€§èƒ½æ¯”è¼ƒ:")
    for task in trainer.baseline_performance:
        baseline = trainer.baseline_performance[task]['main_metric']
        current = trainer.current_performance.get(task, {}).get('main_metric', 0)
        drop = baseline - current
        drop_percent = (drop / baseline * 100) if baseline > 0 else 0
        
        status = "âœ…" if drop_percent <= 5 else "âŒ"
        print(f"  {task}: åŸºæº–={baseline:.4f}, ç•¶å‰={current:.4f}, ä¸‹é™={drop_percent:.2f}% {status}")
    
    # ç¸½é«”è©•ä¼°
    total_time = results.get('total_time', 0)
    print(f"\nâ±ï¸ ç¸½è¨“ç·´æ™‚é–“: {total_time/60:.1f}åˆ†é˜")
    
    # æœ€çµ‚éºå¿˜æª¢æŸ¥
    if 'stage3' in results and 'forgetting' in results['stage3']:
        final_forgetting = results['stage3']['forgetting']
        max_forgetting = final_forgetting['max_drop'] * 100
        
        if final_forgetting['acceptable']:
            print(f"ğŸ‰ EWCé˜²éºå¿˜æˆåŠŸï¼æœ€å¤§éºå¿˜ç¨‹åº¦: {max_forgetting:.2f}% â‰¤ 5%")
        else:
            print(f"âš ï¸ éºå¿˜ç¨‹åº¦è¶…éé–¾å€¼: {max_forgetting:.2f}% > 5%")
    
    print("=" * 70)


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_arguments()
    
    print("ğŸš€ ä¾åºå¤šä»»å‹™è¨“ç·´è…³æœ¬")
    print(f"ğŸ“… é–‹å§‹æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA: {torch.version.cuda}")
        print(f"ğŸ“± GPUæ•¸é‡: {torch.cuda.device_count()}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    print("\nğŸ“‹ è¨“ç·´è¨ˆç•«:")
    print("Stage 1: åˆ†å‰²ä»»å‹™ (è¨˜éŒ„åŸºæº–)")
    print("Stage 2: æª¢æ¸¬ä»»å‹™ + EWC") 
    print("Stage 3: åˆ†é¡ä»»å‹™ + EWC")
    print(f"ğŸ¯ ç›®æ¨™: éºå¿˜ç¨‹åº¦ â‰¤ {args.forgetting_threshold*100:.1f}%")
    
    # é‹è¡Œè¨“ç·´
    results = run_sequential_training(args)
    
    # æª¢æŸ¥è¨“ç·´çµæœ
    if results:
        if 'error' in results:
            print(f"\nâŒ è¨“ç·´å¤±æ•—: {results['error']}")
            sys.exit(1)
        elif 'interrupted' in results:
            print(f"\nâš ï¸ è¨“ç·´è¢«ä¸­æ–·ï¼Œéƒ¨åˆ†çµæœå·²ä¿å­˜")
            sys.exit(2)
        else:
            print(f"\nâœ… Phase 3 å¯¦ç¾å®Œæˆï¼æº–å‚™é€²å…¥ Phase 4: è©•ä¼°èˆ‡å„ªåŒ–")
            sys.exit(0)
    else:
        print(f"\nâŒ è¨“ç·´å¤±æ•—ï¼Œæœªç”¢ç”Ÿçµæœ")
        sys.exit(1)


if __name__ == "__main__":
    main()