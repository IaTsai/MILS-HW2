"""
å¤šä»»å‹™æå¤±å‡½æ•¸
çµ±ä¸€ç®¡ç†æª¢æ¸¬ã€åˆ†å‰²ã€åˆ†é¡ä»»å‹™çš„æå¤±å‡½æ•¸ï¼Œæ”¯æ´è‡ªé©æ‡‰æ¬Šé‡å¹³è¡¡ç­–ç•¥
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union, Any
import math
import numpy as np

from .detection_loss import create_detection_loss
from .segmentation_loss import create_segmentation_loss  
from .classification_loss import create_classification_loss


class MultiTaskLoss(nn.Module):
    """
    å¤šä»»å‹™æå¤±å‡½æ•¸
    
    çµ±ä¸€ç®¡ç†æ‰€æœ‰ä»»å‹™çš„æå¤±è¨ˆç®—ï¼Œæ”¯æ´ï¼š
    1. å›ºå®šæ¬Šé‡ç­–ç•¥
    2. è‡ªé©æ‡‰æ¬Šé‡å¹³è¡¡ (Uncertainty Weighting)
    3. æ¢¯åº¦æ­¸ä¸€åŒ–å¹³è¡¡ (GradNorm)
    4. ä»»å‹™å„ªå…ˆç´šèª¿åº¦
    5. æå¤±å€¼æ­¸ä¸€åŒ–
    
    Args:
        task_weights: ä»»å‹™æ¬Šé‡å­—å…¸ {'detection': w1, 'segmentation': w2, 'classification': w3}
        weighting_strategy: æ¬Šé‡ç­–ç•¥ ('fixed', 'uncertainty', 'gradnorm', 'dynamic')
        loss_configs: å„ä»»å‹™æå¤±é…ç½®
        uncertainty_init: ä¸ç¢ºå®šæ€§æ¬Šé‡åˆå§‹å€¼
        gradnorm_alpha: GradNorm å¹³è¡¡åƒæ•¸
        temperature: æº«åº¦åƒæ•¸ (ç”¨æ–¼æ¬Šé‡ç¸®æ”¾)
    """
    
    def __init__(self,
                 task_weights: Optional[Dict[str, float]] = None,
                 weighting_strategy: str = 'uncertainty',
                 loss_configs: Optional[Dict[str, Dict]] = None,
                 uncertainty_init: float = 0.0,
                 gradnorm_alpha: float = 1.5,
                 temperature: float = 2.0,
                 normalize_losses: bool = True,
                 task_balancing: bool = True):
        super().__init__()
        
        # åŸºæœ¬è¨­ç½®
        self.weighting_strategy = weighting_strategy
        self.gradnorm_alpha = gradnorm_alpha
        self.temperature = temperature
        self.normalize_losses = normalize_losses
        self.task_balancing = task_balancing
        
        # ä»»å‹™åˆ—è¡¨
        self.tasks = ['detection', 'segmentation', 'classification']
        
        # è¨­ç½®é»˜èªä»»å‹™æ¬Šé‡
        if task_weights is None:
            task_weights = {'detection': 1.0, 'segmentation': 1.0, 'classification': 1.0}
        self.task_weights = task_weights
        
        # è¨­ç½®é»˜èªæå¤±é…ç½®
        if loss_configs is None:
            loss_configs = {
                'detection': {'num_classes': 10, 'iou_loss_type': 'giou'},
                'segmentation': {'num_classes': 21, 'loss_type': 'combined'},
                'classification': {'num_classes': 10, 'loss_type': 'combined'}
            }
        
        # å‰µå»ºå„ä»»å‹™æå¤±å‡½æ•¸
        self.detection_loss = create_detection_loss(**loss_configs['detection'])
        self.segmentation_loss = create_segmentation_loss(**loss_configs['segmentation'])
        self.classification_loss = create_classification_loss(**loss_configs['classification'])
        
        # ä¸ç¢ºå®šæ€§æ¬Šé‡ (å¯å­¸ç¿’åƒæ•¸)
        if weighting_strategy == 'uncertainty':
            self.log_vars = nn.ParameterDict({
                task: nn.Parameter(torch.tensor(uncertainty_init))
                for task in self.tasks
            })
        
        # GradNorm ç›¸é—œ
        if weighting_strategy == 'gradnorm':
            self.task_weights_param = nn.ParameterDict({
                task: nn.Parameter(torch.tensor(weight))
                for task, weight in task_weights.items()
            })
            self.initial_losses = {}
            self.training_step = 0
        
        # æå¤±æ­·å²è¨˜éŒ„ (ç”¨æ–¼å‹•æ…‹èª¿æ•´)
        self.loss_history = {task: [] for task in self.tasks}
        self.loss_moving_avg = {task: 0.0 for task in self.tasks}
        self.alpha_ema = 0.9  # æŒ‡æ•¸ç§»å‹•å¹³å‡åƒæ•¸
        
        # è¨ˆæ•¸å™¨
        self.step_count = 0
    
    def uncertainty_weighting(self, task_losses: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        ä¸ç¢ºå®šæ€§åŠ æ¬Š (Multi-Task Learning Using Uncertainty)
        
        Loss = Î£(1/(2*ÏƒÂ²) * L_i + log(Ïƒ))
        
        Args:
            task_losses: å„ä»»å‹™æå¤±å­—å…¸
        
        Returns:
            total_loss: åŠ æ¬Šç¸½æå¤±
            weights: å¯¦éš›ä½¿ç”¨çš„æ¬Šé‡
        """
        total_loss = 0
        weights = {}
        
        for task, loss in task_losses.items():
            if task in self.log_vars:
                # ÏƒÂ² = exp(log_var)
                precision = torch.exp(-self.log_vars[task])
                
                # æå¤±é …: 1/(2*ÏƒÂ²) * L_i + 1/2 * log(ÏƒÂ²)
                weighted_loss = precision * loss + 0.5 * self.log_vars[task]
                total_loss += weighted_loss
                
                # è¨˜éŒ„å¯¦éš›æ¬Šé‡
                weights[task] = precision.item()
            else:
                total_loss += loss
                weights[task] = 1.0
        
        return total_loss, weights
    
    def gradnorm_weighting(self, task_losses: Dict[str, torch.Tensor], 
                          shared_params: List[nn.Parameter]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        GradNorm å‹•æ…‹æ¬Šé‡å¹³è¡¡
        
        åŸºæ–¼æ¢¯åº¦ç¯„æ•¸å¹³è¡¡å„ä»»å‹™æ¬Šé‡
        
        Args:
            task_losses: å„ä»»å‹™æå¤±å­—å…¸
            shared_params: å…±äº«åƒæ•¸åˆ—è¡¨
        
        Returns:
            total_loss: åŠ æ¬Šç¸½æå¤±  
            weights: å¯¦éš›ä½¿ç”¨çš„æ¬Šé‡
        """
        if self.training_step == 0:
            # åˆå§‹åŒ–ï¼šè¨˜éŒ„åˆå§‹æå¤±
            self.initial_losses = {task: loss.item() for task, loss in task_losses.items()}
        
        # è¨ˆç®—ç•¶å‰æå¤±æ¯”ç‡
        loss_ratios = {}
        for task, loss in task_losses.items():
            if task in self.initial_losses and self.initial_losses[task] > 0:
                loss_ratios[task] = loss.item() / self.initial_losses[task]
            else:
                loss_ratios[task] = 1.0
        
        # è¨ˆç®—ç›®æ¨™æå¤±æ¯”ç‡ (åŸºæ–¼è¨“ç·´é€²åº¦)
        avg_loss_ratio = sum(loss_ratios.values()) / len(loss_ratios)
        target_ratios = {task: avg_loss_ratio ** self.gradnorm_alpha for task in self.tasks}
        
        # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
        total_loss = 0
        task_grads = {}
        
        for task, loss in task_losses.items():
            if task in self.task_weights_param:
                weight = torch.abs(self.task_weights_param[task])  # ç¢ºä¿æ¬Šé‡ç‚ºæ­£
                weighted_loss = weight * loss
                total_loss += weighted_loss
                
                # è¨ˆç®—è©²ä»»å‹™å°å…±äº«åƒæ•¸çš„æ¢¯åº¦ç¯„æ•¸
                if shared_params:
                    grads = torch.autograd.grad(weighted_loss, shared_params, 
                                              retain_graph=True, create_graph=True)
                    grad_norm = torch.norm(torch.cat([g.view(-1) for g in grads]))
                    task_grads[task] = grad_norm
                else:
                    task_grads[task] = torch.tensor(1.0, device=loss.device)
        
        # æ›´æ–°ä»»å‹™æ¬Šé‡ (å¦‚æœè™•æ–¼è¨“ç·´æ¨¡å¼)
        if self.training and shared_params:
            avg_grad = sum(task_grads.values()) / len(task_grads)
            
            for task in self.tasks:
                if task in self.task_weights_param and task in task_grads:
                    # è¨ˆç®—æ¬Šé‡æ›´æ–°
                    relative_target = target_ratios[task] / avg_loss_ratio
                    relative_grad = task_grads[task] / avg_grad
                    
                    # GradNorm æ›´æ–°è¦å‰‡
                    grad_target = avg_grad * (relative_target ** self.gradnorm_alpha)
                    grad_loss = F.l1_loss(task_grads[task], grad_target.detach())
                    
                    # åå‘å‚³æ’­æ›´æ–°æ¬Šé‡
                    grad_loss.backward(retain_graph=True)
        
        weights = {task: torch.abs(self.task_weights_param[task]).item() 
                  for task in self.task_weights_param}
        
        self.training_step += 1
        return total_loss, weights
    
    def dynamic_weighting(self, task_losses: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        å‹•æ…‹æ¬Šé‡èª¿æ•´
        
        åŸºæ–¼æå¤±è®ŠåŒ–è¶¨å‹¢å‹•æ…‹èª¿æ•´æ¬Šé‡
        
        Args:
            task_losses: å„ä»»å‹™æå¤±å­—å…¸
        
        Returns:
            total_loss: åŠ æ¬Šç¸½æå¤±
            weights: å¯¦éš›ä½¿ç”¨çš„æ¬Šé‡
        """
        total_loss = 0
        weights = {}
        
        # æ›´æ–°æå¤±ç§»å‹•å¹³å‡
        for task, loss in task_losses.items():
            loss_val = loss.item()
            self.loss_history[task].append(loss_val)
            
            # ä¿æŒæ­·å²è¨˜éŒ„é•·åº¦
            if len(self.loss_history[task]) > 100:
                self.loss_history[task].pop(0)
            
            # æ›´æ–°ç§»å‹•å¹³å‡
            self.loss_moving_avg[task] = (self.alpha_ema * self.loss_moving_avg[task] + 
                                        (1 - self.alpha_ema) * loss_val)
        
        # è¨ˆç®—å‹•æ…‹æ¬Šé‡
        if self.step_count > 10:  # ç­‰å¾…è¶³å¤ çš„æ­·å²æ•¸æ“š
            loss_stds = {}
            for task in self.tasks:
                if len(self.loss_history[task]) > 5:
                    # è¨ˆç®—æå¤±è®ŠåŒ–çš„æ¨™æº–å·®
                    recent_losses = self.loss_history[task][-10:]
                    loss_stds[task] = np.std(recent_losses) if len(recent_losses) > 1 else 1.0
                else:
                    loss_stds[task] = 1.0
            
            # æ ¹æ“šæå¤±ç©©å®šæ€§èª¿æ•´æ¬Šé‡ (ä¸ç©©å®šçš„ä»»å‹™çµ¦æ›´é«˜æ¬Šé‡)
            total_std = sum(loss_stds.values())
            for task, loss in task_losses.items():
                if total_std > 0:
                    stability_weight = loss_stds[task] / total_std
                    base_weight = self.task_weights.get(task, 1.0)
                    dynamic_weight = base_weight * (1 + stability_weight)
                else:
                    dynamic_weight = self.task_weights.get(task, 1.0)
                
                weights[task] = dynamic_weight
                total_loss += dynamic_weight * loss
        else:
            # ä½¿ç”¨å›ºå®šæ¬Šé‡
            for task, loss in task_losses.items():
                weight = self.task_weights.get(task, 1.0)
                weights[task] = weight
                total_loss += weight * loss
        
        self.step_count += 1
        return total_loss, weights
    
    def fixed_weighting(self, task_losses: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        å›ºå®šæ¬Šé‡ç­–ç•¥
        
        Args:
            task_losses: å„ä»»å‹™æå¤±å­—å…¸
        
        Returns:
            total_loss: åŠ æ¬Šç¸½æå¤±
            weights: å¯¦éš›ä½¿ç”¨çš„æ¬Šé‡
        """
        total_loss = 0
        weights = {}
        
        for task, loss in task_losses.items():
            weight = self.task_weights.get(task, 1.0)
            weights[task] = weight
            total_loss += weight * loss
        
        return total_loss, weights
    
    def normalize_task_losses(self, task_losses: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        æ­¸ä¸€åŒ–ä»»å‹™æå¤± (å¯é¸)
        
        Args:
            task_losses: åŸå§‹ä»»å‹™æå¤±
        
        Returns:
            normalized_losses: æ­¸ä¸€åŒ–å¾Œçš„æå¤±
        """
        if not self.normalize_losses:
            return task_losses
        
        # è¨ˆç®—æå¤±çš„å°ºåº¦
        loss_scales = {}
        for task, loss in task_losses.items():
            loss_scales[task] = loss.item()
        
        # æ­¸ä¸€åŒ–åˆ°ç›¸ä¼¼å°ºåº¦
        max_scale = max(loss_scales.values()) if loss_scales else 1.0
        normalized_losses = {}
        
        for task, loss in task_losses.items():
            if max_scale > 0:
                scale_factor = max_scale / (loss_scales[task] + 1e-8)
                normalized_losses[task] = loss * scale_factor
            else:
                normalized_losses[task] = loss
        
        return normalized_losses
    
    def forward(self, 
                predictions: Dict[str, torch.Tensor],
                targets: Dict[str, Any],
                features: Optional[torch.Tensor] = None,
                shared_params: Optional[List[nn.Parameter]] = None) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            predictions: å„ä»»å‹™é æ¸¬å­—å…¸
            targets: å„ä»»å‹™ç›®æ¨™å­—å…¸
            features: å…±äº«ç‰¹å¾µ (ç”¨æ–¼å°æ¯”å­¸ç¿’)
            shared_params: å…±äº«åƒæ•¸ (ç”¨æ–¼ GradNorm)
        
        Returns:
            total_loss: ç¸½æå¤±
            loss_info: è©³ç´°æå¤±ä¿¡æ¯
        """
        task_losses = {}
        detailed_losses = {}
        
        # è¨ˆç®—å„ä»»å‹™æå¤±
        if 'detection' in predictions and 'detection' in targets:
            det_loss, det_dict = self.detection_loss(predictions['detection'], targets['detection'])
            task_losses['detection'] = det_loss
            detailed_losses['detection'] = det_dict
        
        if 'segmentation' in predictions and 'segmentation' in targets:
            seg_loss, seg_dict = self.segmentation_loss(predictions['segmentation'], targets['segmentation'])
            task_losses['segmentation'] = seg_loss
            detailed_losses['segmentation'] = seg_dict
        
        if 'classification' in predictions and 'classification' in targets:
            # è™•ç†åˆ†é¡ä»»å‹™çš„ç‰¹å¾µè¼¸å…¥
            cls_features = features if features is not None else None
            cls_loss, cls_dict = self.classification_loss(
                predictions['classification'], 
                targets['classification'],
                features=cls_features
            )
            task_losses['classification'] = cls_loss
            detailed_losses['classification'] = cls_dict
        
        # æå¤±æ­¸ä¸€åŒ– (å¯é¸)
        if self.normalize_losses:
            task_losses = self.normalize_task_losses(task_losses)
        
        # æ ¹æ“šç­–ç•¥è¨ˆç®—åŠ æ¬Šæå¤±
        if self.weighting_strategy == 'uncertainty':
            total_loss, weights = self.uncertainty_weighting(task_losses)
        elif self.weighting_strategy == 'gradnorm':
            total_loss, weights = self.gradnorm_weighting(task_losses, shared_params or [])
        elif self.weighting_strategy == 'dynamic':
            total_loss, weights = self.dynamic_weighting(task_losses)
        else:  # 'fixed'
            total_loss, weights = self.fixed_weighting(task_losses)
        
        # æ§‹å»ºè¿”å›ä¿¡æ¯
        loss_info = {
            'total_loss': total_loss,
            'task_losses': task_losses,
            'detailed_losses': detailed_losses,
            'task_weights': weights,
            'weighting_strategy': self.weighting_strategy
        }
        
        # æ·»åŠ ä¸ç¢ºå®šæ€§ä¿¡æ¯ (å¦‚æœé©ç”¨)
        if self.weighting_strategy == 'uncertainty':
            uncertainties = {task: torch.exp(0.5 * self.log_vars[task]).item() 
                           for task in self.log_vars}
            loss_info['uncertainties'] = uncertainties
        
        return total_loss, loss_info
    
    def get_task_weights(self) -> Dict[str, float]:
        """
        ç²å–ç•¶å‰ä»»å‹™æ¬Šé‡
        
        Returns:
            weights: ä»»å‹™æ¬Šé‡å­—å…¸
        """
        if self.weighting_strategy == 'uncertainty':
            return {task: torch.exp(-self.log_vars[task]).item() for task in self.log_vars}
        elif self.weighting_strategy == 'gradnorm':
            return {task: torch.abs(param).item() for task, param in self.task_weights_param.items()}
        else:
            return self.task_weights.copy()
    
    def set_task_weights(self, new_weights: Dict[str, float]):
        """
        è¨­ç½®ä»»å‹™æ¬Šé‡
        
        Args:
            new_weights: æ–°çš„ä»»å‹™æ¬Šé‡
        """
        self.task_weights.update(new_weights)
        
        if self.weighting_strategy == 'gradnorm':
            for task, weight in new_weights.items():
                if task in self.task_weights_param:
                    # Use detach() and clone() instead of direct .data manipulation
                    with torch.no_grad():
                        self.task_weights_param[task].copy_(torch.tensor(weight))
    
    def reset_adaptation(self):
        """
        é‡ç½®è‡ªé©æ‡‰åƒæ•¸
        """
        if self.weighting_strategy == 'uncertainty':
            for param in self.log_vars.values():
                # Use detach() and clone() instead of direct .data manipulation
                with torch.no_grad():
                    param.copy_(torch.zeros_like(param))
        
        if self.weighting_strategy == 'gradnorm':
            self.initial_losses = {}
            self.training_step = 0
        
        if self.weighting_strategy == 'dynamic':
            self.loss_history = {task: [] for task in self.tasks}
            self.loss_moving_avg = {task: 0.0 for task in self.tasks}
            self.step_count = 0


def create_multitask_loss(task_weights: Optional[Dict[str, float]] = None,
                         weighting_strategy: str = 'uncertainty',
                         **kwargs) -> MultiTaskLoss:
    """
    å¤šä»»å‹™æå¤±å·¥å» å‡½æ•¸
    
    Args:
        task_weights: ä»»å‹™æ¬Šé‡å­—å…¸
        weighting_strategy: æ¬Šé‡ç­–ç•¥
        **kwargs: å…¶ä»–åƒæ•¸
    
    Returns:
        multitask_loss: å¤šä»»å‹™æå¤±å‡½æ•¸
    """
    return MultiTaskLoss(
        task_weights=task_weights,
        weighting_strategy=weighting_strategy,
        **kwargs
    )


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("âœ… å¤šä»»å‹™æå¤±æ¸¬è©¦é–‹å§‹...")
    
    # å‰µå»ºå¤šä»»å‹™æå¤±
    multitask_loss = create_multitask_loss(
        task_weights={'detection': 1.0, 'segmentation': 1.0, 'classification': 1.0},
        weighting_strategy='uncertainty'
    )
    
    # æ¨¡æ“¬é æ¸¬æ•¸æ“š
    batch_size = 8
    predictions = {
        'detection': torch.randn(batch_size, 100, 6).to(device),  # (B, H*W, 6)
        'segmentation': torch.randn(batch_size, 21, 256, 256).to(device),  # (B, C, H, W)
        'classification': torch.randn(batch_size, 10).to(device)  # (B, C)
    }
    
    # æ¨¡æ“¬ç›®æ¨™æ•¸æ“š
    targets = {
        'detection': [
            {
                'boxes': torch.rand(3, 4).to(device),
                'labels': torch.randint(0, 10, (3,)).to(device)
            } for _ in range(batch_size)
        ],
        'segmentation': torch.randint(0, 21, (batch_size, 256, 256)).to(device),
        'classification': torch.randint(0, 10, (batch_size,)).to(device)
    }
    
    # å…±äº«ç‰¹å¾µ (ç”¨æ–¼åˆ†é¡å°æ¯”å­¸ç¿’)
    features = torch.randn(batch_size, 128).to(device)
    
    print("ğŸ“Š æ¸¬è©¦ä¸åŒæ¬Šé‡ç­–ç•¥:")
    
    strategies = ['fixed', 'uncertainty', 'dynamic']
    
    for strategy in strategies:
        print(f"\nğŸ§ª æ¸¬è©¦ {strategy} ç­–ç•¥:")
        
        test_loss = create_multitask_loss(
            weighting_strategy=strategy,
            task_weights={'detection': 1.0, 'segmentation': 1.0, 'classification': 1.0}
        )
        
        try:
            total_loss, loss_info = test_loss(predictions, targets, features=features)
            
            print(f"  ç¸½æå¤±: {total_loss.item():.4f}")
            print(f"  ä»»å‹™æ¬Šé‡: {loss_info['task_weights']}")
            
            if 'uncertainties' in loss_info:
                print(f"  ä¸ç¢ºå®šæ€§: {loss_info['uncertainties']}")
            
            # æ¸¬è©¦æ¢¯åº¦
            total_loss.backward(retain_graph=True)
            print(f"  âœ… æ¢¯åº¦å›å‚³æ­£å¸¸")
            
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
    
    print("\nğŸ”§ æ¸¬è©¦æ¬Šé‡èª¿æ•´:")
    
    # æ¸¬è©¦æ¬Šé‡è¨­ç½®å’Œç²å–
    current_weights = multitask_loss.get_task_weights()
    print(f"  ç•¶å‰æ¬Šé‡: {current_weights}")
    
    new_weights = {'detection': 2.0, 'segmentation': 0.5, 'classification': 1.5}
    multitask_loss.set_task_weights(new_weights)
    updated_weights = multitask_loss.get_task_weights()
    print(f"  æ›´æ–°å¾Œæ¬Šé‡: {updated_weights}")
    
    print("\nğŸ‰ å¤šä»»å‹™æå¤±æ¸¬è©¦å®Œæˆï¼")
    print("ğŸš€ æ”¯æ´çš„æ¬Šé‡ç­–ç•¥: fixed, uncertainty, gradnorm, dynamic")
    print("âš™ï¸ æ”¯æ´çš„åŠŸèƒ½: æå¤±æ­¸ä¸€åŒ–, å‹•æ…‹æ¬Šé‡èª¿æ•´, ä¸ç¢ºå®šæ€§å»ºæ¨¡")