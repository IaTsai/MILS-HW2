"""
åˆ†å‰²ä»»å‹™æå¤±å‡½æ•¸
åŒ…å«äº¤å‰ç†µæå¤±ã€Diceæå¤±ã€Focalæå¤±ç­‰å¤šç¨®èªç¾©åˆ†å‰²æå¤±å‡½æ•¸
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import numpy as np


class SegmentationLoss(nn.Module):
    """
    èªç¾©åˆ†å‰²æå¤±å‡½æ•¸
    
    çµåˆäº¤å‰ç†µæå¤±å’ŒDiceæå¤±ï¼Œé‡å°èªç¾©åˆ†å‰²ä»»å‹™è¨­è¨ˆã€‚
    æ”¯æ´é¡åˆ¥æ¬Šé‡ã€å¿½ç•¥ç´¢å¼•ã€å¤šå°ºåº¦æå¤±ç­‰ç‰¹æ€§ã€‚
    
    Args:
        num_classes: åˆ†å‰²é¡åˆ¥æ•¸
        loss_type: æå¤±é¡å‹ ('ce', 'dice', 'focal', 'combined')
        class_weights: é¡åˆ¥æ¬Šé‡
        ignore_index: å¿½ç•¥çš„é¡åˆ¥ç´¢å¼•
        focal_alpha: Focal loss alpha åƒæ•¸
        focal_gamma: Focal loss gamma åƒæ•¸
        dice_smooth: Dice loss å¹³æ»‘åƒæ•¸
        ce_weight: äº¤å‰ç†µæå¤±æ¬Šé‡
        dice_weight: Diceæå¤±æ¬Šé‡
    """
    
    def __init__(self,
                 num_classes: int = 21,
                 loss_type: str = 'combined',
                 class_weights: Optional[torch.Tensor] = None,
                 ignore_index: int = 255,
                 focal_alpha: float = 0.25,
                 focal_gamma: float = 2.0,
                 dice_smooth: float = 1e-6,
                 ce_weight: float = 1.0,
                 dice_weight: float = 1.0):
        super().__init__()
        
        self.num_classes = num_classes
        self.loss_type = loss_type
        self.ignore_index = ignore_index
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.dice_smooth = dice_smooth
        self.ce_weight = ce_weight
        self.dice_weight = dice_weight
        
        # è¨­ç½®é¡åˆ¥æ¬Šé‡
        if class_weights is not None:
            self.register_buffer('class_weights', class_weights)
        else:
            self.class_weights = None
        
        # å‰µå»ºäº¤å‰ç†µæå¤±
        self.ce_loss = nn.CrossEntropyLoss(
            weight=self.class_weights,
            ignore_index=ignore_index,
            reduction='mean'
        )
    
    def cross_entropy_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        äº¤å‰ç†µæå¤±
        
        Args:
            pred: é æ¸¬logits (B, C, H, W)
            target: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            ce_loss: äº¤å‰ç†µæå¤±
        """
        return self.ce_loss(pred, target)
    
    def dice_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Diceæå¤±
        
        Args:
            pred: é æ¸¬logits (B, C, H, W)
            target: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            dice_loss: Diceæå¤±
        """
        # è½‰æ›ç‚ºæ¦‚ç‡
        pred_prob = F.softmax(pred, dim=1)
        
        # å‰µå»º one-hot ç·¨ç¢¼
        target_one_hot = F.one_hot(target.long(), num_classes=self.num_classes)
        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()  # (B, C, H, W)
        
        # å¿½ç•¥æŒ‡å®šç´¢å¼•
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float().unsqueeze(1)
            pred_prob = pred_prob * mask
            target_one_hot = target_one_hot * mask
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ Dice ä¿‚æ•¸
        intersection = (pred_prob * target_one_hot).sum(dim=(2, 3))  # (B, C)
        union = pred_prob.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))  # (B, C)
        
        dice_coeff = (2.0 * intersection + self.dice_smooth) / (union + self.dice_smooth)
        
        # è¿”å› 1 - mean(dice_coeff)
        return 1.0 - dice_coeff.mean()
    
    def focal_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Focal Loss (ç”¨æ–¼è™•ç†é¡åˆ¥ä¸å¹³è¡¡)
        
        Args:
            pred: é æ¸¬logits (B, C, H, W)
            target: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            focal_loss: Focalæå¤±
        """
        # è¨ˆç®—äº¤å‰ç†µ
        ce_loss = F.cross_entropy(pred, target, reduction='none', ignore_index=self.ignore_index)
        
        # è¨ˆç®—æ¦‚ç‡
        pt = torch.exp(-ce_loss)
        
        # æ‡‰ç”¨ Focal weight
        focal_weight = (1 - pt) ** self.focal_gamma
        
        # æ‡‰ç”¨ alpha weighting (å¦‚æœæœ‰é¡åˆ¥æ¬Šé‡)
        if self.focal_alpha > 0:
            alpha_weight = self.focal_alpha
            focal_weight = alpha_weight * focal_weight
        
        focal_loss = focal_weight * ce_loss
        
        # è™•ç†å¿½ç•¥ç´¢å¼•
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float()
            focal_loss = focal_loss * mask
            return focal_loss.sum() / (mask.sum() + 1e-8)
        else:
            return focal_loss.mean()
    
    def boundary_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        é‚Šç•Œæå¤± (å¼·èª¿åˆ†å‰²é‚Šç•Œ)
        
        Args:
            pred: é æ¸¬logits (B, C, H, W)
            target: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            boundary_loss: é‚Šç•Œæå¤±
        """
        # è¨ˆç®—æ¢¯åº¦ä¾†æª¢æ¸¬é‚Šç•Œ
        def compute_gradient(tensor):
            # Sobel ç®—å­
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)
            
            sobel_x = sobel_x.to(tensor.device)
            sobel_y = sobel_y.to(tensor.device)
            
            grad_x = F.conv2d(tensor, sobel_x, padding=1)
            grad_y = F.conv2d(tensor, sobel_y, padding=1)
            
            return torch.sqrt(grad_x ** 2 + grad_y ** 2)
        
        # ç²å–é æ¸¬æ¦‚ç‡
        pred_prob = F.softmax(pred, dim=1)
        pred_max = torch.argmax(pred_prob, dim=1, keepdim=True).float()
        
        # è¨ˆç®—ç›®æ¨™çš„é‚Šç•Œ
        target_boundary = compute_gradient(target.unsqueeze(1).float())
        pred_boundary = compute_gradient(pred_max)
        
        # é‚Šç•Œæå¤± (L2è·é›¢)
        boundary_loss = F.mse_loss(pred_boundary, target_boundary)
        
        return boundary_loss
    
    def lovasz_softmax_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        LovÃ¡sz-Softmax æå¤± (é‡å°IoUå„ªåŒ–)
        
        Args:
            pred: é æ¸¬logits (B, C, H, W)
            target: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            lovasz_loss: LovÃ¡szæå¤±
        """
        # ç°¡åŒ–ç‰ˆæœ¬çš„ LovÃ¡sz-Softmax
        # å®Œæ•´å¯¦ç¾è¼ƒè¤‡é›œï¼Œé€™è£¡æä¾›åŸºæœ¬ç‰ˆæœ¬
        
        pred_prob = F.softmax(pred, dim=1)
        
        # å°‡æ¯å€‹é¡åˆ¥åˆ†é–‹è™•ç†
        losses = []
        for c in range(self.num_classes):
            if c == self.ignore_index:
                continue
            
            # ç•¶å‰é¡åˆ¥çš„é æ¸¬å’Œç›®æ¨™
            pred_c = pred_prob[:, c]  # (B, H, W)
            target_c = (target == c).float()  # (B, H, W)
            
            # å¿½ç•¥æŒ‡å®šç´¢å¼•
            if self.ignore_index >= 0:
                mask = (target != self.ignore_index).float()
                pred_c = pred_c * mask
                target_c = target_c * mask
            
            # è¨ˆç®—è©²é¡åˆ¥çš„æå¤± (ç°¡åŒ–ç‰ˆ)
            intersection = (pred_c * target_c).sum()
            union = pred_c.sum() + target_c.sum() - intersection
            iou_loss = 1.0 - (intersection + 1e-8) / (union + 1e-8)
            
            losses.append(iou_loss)
        
        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=pred.device)
    
    def forward(self, 
                predictions: torch.Tensor, 
                targets: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            predictions: æ¨¡å‹é æ¸¬ (B, C, H, W)
            targets: ç›®æ¨™æ¨™ç±¤ (B, H, W)
        
        Returns:
            total_loss: ç¸½æå¤±
            loss_dict: è©³ç´°æå¤±å­—å…¸
        """
        loss_dict = {}
        
        if self.loss_type == 'ce':
            # åªä½¿ç”¨äº¤å‰ç†µæå¤±
            ce_loss = self.cross_entropy_loss(predictions, targets)
            total_loss = ce_loss
            loss_dict = {
                'cross_entropy': ce_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'dice':
            # åªä½¿ç”¨ Dice æå¤±
            dice_loss = self.dice_loss(predictions, targets)
            total_loss = dice_loss
            loss_dict = {
                'dice': dice_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'focal':
            # åªä½¿ç”¨ Focal æå¤±
            focal_loss = self.focal_loss(predictions, targets)
            total_loss = focal_loss
            loss_dict = {
                'focal': focal_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'combined':
            # çµåˆäº¤å‰ç†µå’Œ Dice æå¤±
            ce_loss = self.cross_entropy_loss(predictions, targets)
            dice_loss = self.dice_loss(predictions, targets)
            
            total_loss = self.ce_weight * ce_loss + self.dice_weight * dice_loss
            
            loss_dict = {
                'cross_entropy': ce_loss,
                'dice': dice_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'advanced':
            # é«˜ç´šæå¤±çµ„åˆ
            ce_loss = self.cross_entropy_loss(predictions, targets)
            dice_loss = self.dice_loss(predictions, targets)
            focal_loss = self.focal_loss(predictions, targets)
            boundary_loss = self.boundary_loss(predictions, targets)
            
            total_loss = (0.4 * ce_loss + 
                         0.3 * dice_loss + 
                         0.2 * focal_loss + 
                         0.1 * boundary_loss)
            
            loss_dict = {
                'cross_entropy': ce_loss,
                'dice': dice_loss,
                'focal': focal_loss,
                'boundary': boundary_loss,
                'total': total_loss
            }
        
        else:
            raise ValueError(f"Unsupported loss type: {self.loss_type}")
        
        return total_loss, loss_dict


class DiceLoss(nn.Module):
    """
    ç¨ç«‹çš„ Dice æå¤±å¯¦ç¾
    """
    def __init__(self, num_classes: int = 21, smooth: float = 1e-6, ignore_index: int = 255):
        super().__init__()
        self.num_classes = num_classes
        self.smooth = smooth
        self.ignore_index = ignore_index
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        pred_prob = F.softmax(pred, dim=1)
        
        target_one_hot = F.one_hot(target.long(), num_classes=self.num_classes)
        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()
        
        # è™•ç†å¿½ç•¥ç´¢å¼•
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float().unsqueeze(1)
            pred_prob = pred_prob * mask
            target_one_hot = target_one_hot * mask
        
        intersection = (pred_prob * target_one_hot).sum(dim=(2, 3))
        union = pred_prob.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))
        
        dice_coeff = (2.0 * intersection + self.smooth) / (union + self.smooth)
        
        return 1.0 - dice_coeff.mean()


class IoULoss(nn.Module):
    """
    IoU æå¤±å¯¦ç¾
    """
    def __init__(self, num_classes: int = 21, smooth: float = 1e-6, ignore_index: int = 255):
        super().__init__()
        self.num_classes = num_classes
        self.smooth = smooth
        self.ignore_index = ignore_index
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        pred_prob = F.softmax(pred, dim=1)
        
        target_one_hot = F.one_hot(target.long(), num_classes=self.num_classes)
        target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()
        
        # è™•ç†å¿½ç•¥ç´¢å¼•
        if self.ignore_index >= 0:
            mask = (target != self.ignore_index).float().unsqueeze(1)
            pred_prob = pred_prob * mask
            target_one_hot = target_one_hot * mask
        
        intersection = (pred_prob * target_one_hot).sum(dim=(2, 3))
        union = pred_prob.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3)) - intersection
        
        iou = (intersection + self.smooth) / (union + self.smooth)
        
        return 1.0 - iou.mean()


def create_segmentation_loss(num_classes: int = 21,
                           loss_type: str = 'combined',
                           **kwargs) -> SegmentationLoss:
    """
    åˆ†å‰²æå¤±å·¥å» å‡½æ•¸
    
    Args:
        num_classes: åˆ†å‰²é¡åˆ¥æ•¸
        loss_type: æå¤±é¡å‹
        **kwargs: å…¶ä»–åƒæ•¸
    
    Returns:
        seg_loss: åˆ†å‰²æå¤±å‡½æ•¸
    """
    return SegmentationLoss(num_classes=num_classes, loss_type=loss_type, **kwargs)


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºåˆ†å‰²æå¤±
    seg_loss = create_segmentation_loss(num_classes=21, loss_type='combined')
    
    # æ¨¡æ“¬é æ¸¬å’Œç›®æ¨™
    batch_size = 2
    height, width = 256, 256
    num_classes = 21
    
    # é æ¸¬: (B, C, H, W)
    predictions = torch.randn(batch_size, num_classes, height, width).to(device)
    
    # ç›®æ¨™: (B, H, W)
    targets = torch.randint(0, num_classes, (batch_size, height, width)).to(device)
    
    # è¨ˆç®—æå¤±
    total_loss, loss_dict = seg_loss(predictions, targets)
    
    print("âœ… åˆ†å‰²æå¤±æ¸¬è©¦æˆåŠŸï¼")
    print(f"ğŸ“Š ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"ğŸ” è©³ç´°æå¤±: {loss_dict}")
    print(f"ğŸ“ˆ æå¤±é …: {list(loss_dict.keys())}")
    
    # æ¸¬è©¦ä¸åŒæå¤±é¡å‹
    print("\nğŸ§ª æ¸¬è©¦ä¸åŒæå¤±é¡å‹:")
    loss_types = ['ce', 'dice', 'focal', 'combined']
    
    for loss_type in loss_types:
        test_loss = create_segmentation_loss(num_classes=21, loss_type=loss_type)
        test_total_loss, test_loss_dict = test_loss(predictions, targets)
        print(f"  {loss_type}: {test_total_loss.item():.4f}")