"""
åˆ†é¡ä»»å‹™æå¤±å‡½æ•¸
åŒ…å«äº¤å‰ç†µæå¤±ã€æ¨™ç±¤å¹³æ»‘ã€Focalæå¤±ã€å°æ¯”å­¸ç¿’æå¤±ç­‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import math


class ClassificationLoss(nn.Module):
    """
    åˆ†é¡ä»»å‹™æå¤±å‡½æ•¸
    
    æ”¯æ´å¤šç¨®åˆ†é¡æå¤±å‡½æ•¸çµ„åˆï¼ŒåŒ…æ‹¬ï¼š
    1. äº¤å‰ç†µæå¤± (å«æ¨™ç±¤å¹³æ»‘)
    2. Focal Loss (è™•ç†é¡åˆ¥ä¸å¹³è¡¡)
    3. å°æ¯”å­¸ç¿’æå¤±
    4. æº«åº¦ç¸®æ”¾
    5. Mixup æ•¸æ“šå¢å¼·æ”¯æ´
    
    Args:
        num_classes: åˆ†é¡é¡åˆ¥æ•¸
        loss_type: æå¤±é¡å‹ ('ce', 'focal', 'contrastive', 'combined')
        label_smoothing: æ¨™ç±¤å¹³æ»‘åƒæ•¸
        class_weights: é¡åˆ¥æ¬Šé‡
        focal_alpha: Focal loss alpha åƒæ•¸
        focal_gamma: Focal loss gamma åƒæ•¸
        temperature: æº«åº¦ç¸®æ”¾åƒæ•¸
        contrastive_margin: å°æ¯”å­¸ç¿’é‚Šç•Œ
    """
    
    def __init__(self,
                 num_classes: int = 10,
                 loss_type: str = 'combined',
                 label_smoothing: float = 0.1,
                 class_weights: Optional[torch.Tensor] = None,
                 focal_alpha: Union[float, List[float]] = 0.25,
                 focal_gamma: float = 2.0,
                 temperature: float = 1.0,
                 contrastive_margin: float = 0.5,
                 ce_weight: float = 1.0,
                 focal_weight: float = 1.0,
                 contrastive_weight: float = 0.1):
        super().__init__()
        
        self.num_classes = num_classes
        self.loss_type = loss_type
        self.label_smoothing = label_smoothing
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.temperature = temperature
        self.contrastive_margin = contrastive_margin
        self.ce_weight = ce_weight
        self.focal_weight = focal_weight
        self.contrastive_weight = contrastive_weight
        
        # è¨­ç½®é¡åˆ¥æ¬Šé‡
        if class_weights is not None:
            self.register_buffer('class_weights', class_weights)
        else:
            self.class_weights = None
        
        # å‰µå»ºåŸºç¤æå¤±å‡½æ•¸
        self.ce_loss = nn.CrossEntropyLoss(
            weight=self.class_weights,
            label_smoothing=label_smoothing,
            reduction='mean'
        )
        
        # è¨­ç½® Focal Loss alpha
        if isinstance(focal_alpha, (list, tuple)):
            assert len(focal_alpha) == num_classes, "Alpha é•·åº¦å¿…é ˆç­‰æ–¼é¡åˆ¥æ•¸"
            self.register_buffer('focal_alpha_tensor', torch.tensor(focal_alpha))
        else:
            self.focal_alpha_tensor = focal_alpha
    
    def cross_entropy_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        äº¤å‰ç†µæå¤± (å«æ¨™ç±¤å¹³æ»‘)
        
        Args:
            pred: é æ¸¬logits (B, C)
            target: ç›®æ¨™æ¨™ç±¤ (B,)
        
        Returns:
            ce_loss: äº¤å‰ç†µæå¤±
        """
        return self.ce_loss(pred, target)
    
    def focal_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Focal Loss å¯¦ç¾
        
        Args:
            pred: é æ¸¬logits (B, C)
            target: ç›®æ¨™æ¨™ç±¤ (B,)
        
        Returns:
            focal_loss: Focalæå¤±
        """
        # è¨ˆç®—äº¤å‰ç†µ (ä¸é€²è¡Œ reduction)
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        
        # è¨ˆç®—æ¦‚ç‡
        p_t = torch.exp(-ce_loss)  # p_t = exp(-ce_loss)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** self.focal_gamma
        
        # Alpha weighting
        if isinstance(self.focal_alpha_tensor, torch.Tensor):
            # å¤šé¡åˆ¥ alpha
            if self.focal_alpha_tensor.device != pred.device:
                self.focal_alpha_tensor = self.focal_alpha_tensor.to(pred.device)
            alpha_t = self.focal_alpha_tensor.gather(0, target)
            focal_weight = alpha_t * focal_weight
        elif self.focal_alpha_tensor > 0:
            # å–®ä¸€ alpha å€¼
            focal_weight = self.focal_alpha_tensor * focal_weight
        
        # æ‡‰ç”¨ Focal weight
        focal_loss = focal_weight * ce_loss
        
        return focal_loss.mean()
    
    def contrastive_loss(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        å°æ¯”å­¸ç¿’æå¤± (SimCLR é¢¨æ ¼)
        
        Args:
            features: ç‰¹å¾µå‘é‡ (B, D)
            labels: æ¨™ç±¤ (B,)
        
        Returns:
            contrastive_loss: å°æ¯”å­¸ç¿’æå¤±
        """
        batch_size = features.size(0)
        
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # L2 æ­£è¦åŒ–ç‰¹å¾µ
        features = F.normalize(features, dim=1)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # å‰µå»ºæ¨™ç±¤æ©ç¢¼
        labels = labels.unsqueeze(1)
        mask = (labels == labels.T).float()  # ç›¸åŒæ¨™ç±¤ç‚º 1
        
        # ç§»é™¤å°è§’ç·š (è‡ªå·±å’Œè‡ªå·±)
        mask = mask - torch.eye(batch_size, device=features.device)
        
        # è¨ˆç®—æ­£æ¨£æœ¬å°æ•¸
        positive_pairs = mask.sum()
        
        if positive_pairs == 0:
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # å°æ¯”å­¸ç¿’æå¤±è¨ˆç®—
        exp_sim = torch.exp(similarity_matrix)
        
        # åˆ†å­ï¼šæ­£æ¨£æœ¬ç›¸ä¼¼åº¦
        pos_sim = (exp_sim * mask).sum(dim=1)
        
        # åˆ†æ¯ï¼šæ‰€æœ‰æ¨£æœ¬ç›¸ä¼¼åº¦ (é™¤äº†è‡ªå·±)
        neg_mask = 1 - torch.eye(batch_size, device=features.device)
        all_sim = (exp_sim * neg_mask).sum(dim=1)
        
        # é¿å…é™¤é›¶
        loss = -torch.log((pos_sim + 1e-8) / (all_sim + 1e-8))
        
        # åªè¨ˆç®—æœ‰æ­£æ¨£æœ¬çš„æå¤±
        valid_mask = (mask.sum(dim=1) > 0).float()
        loss = (loss * valid_mask).sum() / (valid_mask.sum() + 1e-8)
        
        return loss
    
    def forward(self, 
                predictions: torch.Tensor, 
                targets: torch.Tensor,
                features: Optional[torch.Tensor] = None,
                teacher_logits: Optional[torch.Tensor] = None,
                mixup_params: Optional[Dict] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            predictions: æ¨¡å‹é æ¸¬ (B, C)
            targets: ç›®æ¨™æ¨™ç±¤ (B,)
            features: ç‰¹å¾µå‘é‡ (B, D) - ç”¨æ–¼å°æ¯”å­¸ç¿’
            teacher_logits: æ•™å¸«æ¨¡å‹logits (B, C) - ç”¨æ–¼çŸ¥è­˜è’¸é¤¾
            mixup_params: Mixupåƒæ•¸å­—å…¸
        
        Returns:
            total_loss: ç¸½æå¤±
            loss_dict: è©³ç´°æå¤±å­—å…¸
        """
        loss_dict = {}
        
        if self.loss_type == 'ce':
            # ç´”äº¤å‰ç†µæå¤±
            ce_loss = self.cross_entropy_loss(predictions, targets)
            total_loss = ce_loss
            loss_dict = {
                'cross_entropy': ce_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'focal':
            # ç´” Focal æå¤±
            focal_loss = self.focal_loss(predictions, targets)
            total_loss = focal_loss
            loss_dict = {
                'focal': focal_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'contrastive':
            # å°æ¯”å­¸ç¿’æå¤±
            if features is None:
                raise ValueError("Contrastive loss requires features")
            
            ce_loss = self.cross_entropy_loss(predictions, targets)
            contrastive_loss = self.contrastive_loss(features, targets)
            
            total_loss = ce_loss + self.contrastive_weight * contrastive_loss
            
            loss_dict = {
                'cross_entropy': ce_loss,
                'contrastive': contrastive_loss,
                'total': total_loss
            }
        
        elif self.loss_type == 'combined':
            # çµ„åˆæå¤±ï¼šäº¤å‰ç†µ + Focal
            ce_loss = self.cross_entropy_loss(predictions, targets)
            focal_loss = self.focal_loss(predictions, targets)
            
            total_loss = self.ce_weight * ce_loss + self.focal_weight * focal_loss
            
            loss_dict = {
                'cross_entropy': ce_loss,
                'focal': focal_loss,
                'total': total_loss
            }
            
            # å¯é¸ï¼šæ·»åŠ å°æ¯”å­¸ç¿’
            if features is not None:
                contrastive_loss = self.contrastive_loss(features, targets)
                total_loss += self.contrastive_weight * contrastive_loss
                loss_dict['contrastive'] = contrastive_loss
                loss_dict['total'] = total_loss
        
        else:
            raise ValueError(f"Unsupported loss type: {self.loss_type}")
        
        return total_loss, loss_dict


def create_classification_loss(num_classes: int = 10,
                             loss_type: str = 'combined',
                             **kwargs) -> ClassificationLoss:
    """
    åˆ†é¡æå¤±å·¥å» å‡½æ•¸
    
    Args:
        num_classes: åˆ†é¡é¡åˆ¥æ•¸
        loss_type: æå¤±é¡å‹
        **kwargs: å…¶ä»–åƒæ•¸
    
    Returns:
        classification_loss: åˆ†é¡æå¤±å‡½æ•¸
    """
    return ClassificationLoss(num_classes=num_classes, loss_type=loss_type, **kwargs)


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å‰µå»ºåˆ†é¡æå¤±
    cls_loss = create_classification_loss(num_classes=10, loss_type='combined')
    
    # æ¨¡æ“¬é æ¸¬å’Œç›®æ¨™
    batch_size = 16
    num_classes = 10
    feature_dim = 128
    
    # é æ¸¬: (B, C)
    predictions = torch.randn(batch_size, num_classes).to(device)
    
    # ç›®æ¨™: (B,)
    targets = torch.randint(0, num_classes, (batch_size,)).to(device)
    
    # ç‰¹å¾µ: (B, D) - ç”¨æ–¼å°æ¯”å­¸ç¿’
    features = torch.randn(batch_size, feature_dim).to(device)
    
    print("âœ… åˆ†é¡æå¤±æ¸¬è©¦é–‹å§‹...")
    
    # 1. æ¸¬è©¦åŸºæœ¬çµ„åˆæå¤±
    total_loss, loss_dict = cls_loss(predictions, targets, features=features)
    
    print(f"ğŸ“Š çµ„åˆæå¤±: {total_loss.item():.4f}")
    print(f"ğŸ” è©³ç´°æå¤±: {loss_dict}")
    
    # 2. æ¸¬è©¦ä¸åŒæå¤±é¡å‹
    print("\nğŸ§ª æ¸¬è©¦ä¸åŒæå¤±é¡å‹:")
    loss_types = ['ce', 'focal', 'contrastive', 'combined']
    
    for loss_type in loss_types:
        test_loss = create_classification_loss(num_classes=10, loss_type=loss_type)
        
        try:
            if loss_type == 'contrastive':
                test_total_loss, test_loss_dict = test_loss(predictions, targets, features=features)
            else:
                test_total_loss, test_loss_dict = test_loss(predictions, targets)
            
            print(f"  {loss_type}: {test_total_loss.item():.4f}")
        except Exception as e:
            print(f"  {loss_type}: éŒ¯èª¤ - {e}")
    
    print("\nğŸ‰ åˆ†é¡æå¤±æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“‹ æ”¯æ´çš„æå¤±é¡å‹: {loss_types}")
    print(f"ğŸ”§ æ”¯æ´çš„ç‰¹æ®ŠåŠŸèƒ½: å°æ¯”å­¸ç¿’, æº«åº¦ç¸®æ”¾")
