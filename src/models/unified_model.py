"""
çµ±ä¸€å¤šä»»å‹™å­¸ç¿’æ¨¡å‹
æ•´åˆéª¨å¹¹ç¶²è·¯ã€é ¸éƒ¨ç¶²è·¯ã€å¤šä»»å‹™é ­éƒ¨çš„å®Œæ•´æ¨¡å‹å¯¦ç¾
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Union, Tuple, Any
import time

from .backbone import create_backbone, BackboneNetwork
from .neck import create_neck, FeaturePyramidNetwork
from .head import create_multitask_head, UnifiedMultiTaskHead


class UnifiedMultiTaskModel(nn.Module):
    """
    çµ±ä¸€å¤šä»»å‹™å­¸ç¿’æ¨¡å‹
    
    æ•´åˆéª¨å¹¹ç¶²è·¯ã€é ¸éƒ¨ç¶²è·¯ã€å¤šä»»å‹™é ­éƒ¨çš„å®Œæ•´æ¶æ§‹ï¼š
    Input -> Backbone -> Neck -> Head -> Multi-task Outputs
    
    æ”¯æ´ï¼š
    - åŒæ™‚é€²è¡Œæª¢æ¸¬ã€åˆ†å‰²ã€åˆ†é¡ä¸‰å€‹ä»»å‹™
    - å–®ä»»å‹™æ¨ç†æ¨¡å¼
    - éˆæ´»çš„åƒæ•¸é…ç½®
    - é«˜æ•ˆçš„ç‰¹å¾µå…±äº«
    
    Args:
        backbone_name: éª¨å¹¹ç¶²è·¯åç¨±
        neck_type: é ¸éƒ¨ç¶²è·¯é¡å‹
        head_type: é ­éƒ¨ç¶²è·¯é¡å‹
        num_det_classes: æª¢æ¸¬é¡åˆ¥æ•¸
        num_seg_classes: åˆ†å‰²é¡åˆ¥æ•¸
        num_cls_classes: åˆ†é¡é¡åˆ¥æ•¸
        pretrained: æ˜¯å¦ä½¿ç”¨é è¨“ç·´æ¬Šé‡
    """
    
    def __init__(self,
                 backbone_name: str = 'mobilenetv3_small',
                 neck_type: str = 'fpn',
                 head_type: str = 'unified',
                 num_det_classes: int = 10,
                 num_seg_classes: int = 21,
                 num_cls_classes: int = 10,
                 pretrained: bool = True,
                 neck_out_channels: int = 128,
                 head_shared_channels: int = 256):
        super().__init__()
        
        self.backbone_name = backbone_name
        self.neck_type = neck_type
        self.head_type = head_type
        self.num_det_classes = num_det_classes
        self.num_seg_classes = num_seg_classes
        self.num_cls_classes = num_cls_classes
        
        # å‰µå»ºéª¨å¹¹ç¶²è·¯
        self.backbone = create_backbone(
            model_name=backbone_name,
            pretrained=pretrained
        )
        
        # ç²å–éª¨å¹¹ç¶²è·¯ç‰¹å¾µé€šé“é…ç½®
        backbone_channels = self.backbone.feature_channels
        in_channels_list = [
            backbone_channels['layer1'],
            backbone_channels['layer2'], 
            backbone_channels['layer3'],
            backbone_channels['layer4']
        ]
        
        # å‰µå»ºé ¸éƒ¨ç¶²è·¯
        if neck_type == 'fpn':
            self.neck = create_neck(
                neck_type='fpn',
                in_channels_list=in_channels_list,
                out_channels=neck_out_channels
            )
        elif neck_type == 'lightweight':
            self.neck = create_neck(
                neck_type='lightweight',
                in_channels_list=in_channels_list,
                out_channels=neck_out_channels
            )
        else:
            raise ValueError(f"Unsupported neck type: {neck_type}")
        
        # å‰µå»ºå¤šä»»å‹™é ­éƒ¨
        if head_type == 'unified':
            self.head = create_multitask_head(
                head_type='unified',
                in_channels=neck_out_channels,
                num_det_classes=num_det_classes,
                num_seg_classes=num_seg_classes,
                num_cls_classes=num_cls_classes,
                shared_channels=head_shared_channels
            )
        else:
            raise ValueError(f"Unsupported head type: {head_type}")
        
        # æ¨¡å‹ä¿¡æ¯
        self.model_info = self._get_model_info()
    
    def forward(self, 
                x: torch.Tensor, 
                task_type: str = 'all') -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥åœ–åƒ (B, 3, H, W)
            task_type: ä»»å‹™é¡å‹ 'all', 'detection', 'segmentation', 'classification'
        
        Returns:
            outputs: ä»»å‹™è¼¸å‡ºå­—å…¸
        """
        # éª¨å¹¹ç¶²è·¯ç‰¹å¾µæå–
        backbone_features = self.backbone(x)
        
        # é ¸éƒ¨ç¶²è·¯ç‰¹å¾µèåˆ
        neck_features = self.neck(backbone_features)
        
        # å¤šä»»å‹™é ­éƒ¨è¼¸å‡º
        head_outputs = self.head(neck_features, task_type=task_type)
        
        return head_outputs
    
    def get_total_parameters(self) -> Dict[str, int]:
        """ç²å–æ¨¡å‹ç¸½åƒæ•¸é‡çµ±è¨ˆ"""
        # éª¨å¹¹ç¶²è·¯åƒæ•¸
        backbone_params = self.backbone.get_parameter_count()
        
        # é ¸éƒ¨ç¶²è·¯åƒæ•¸
        neck_params = self.neck.get_parameter_count()
        
        # é ­éƒ¨ç¶²è·¯åƒæ•¸
        head_params = self.head.get_parameter_count()
        
        # ç¸½åƒæ•¸é‡
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'backbone_parameters': backbone_params['total_parameters'],
            'neck_parameters': neck_params['total_parameters'],
            'head_parameters': head_params['total_parameters'],
            'backbone_detail': backbone_params,
            'neck_detail': neck_params,
            'head_detail': head_params
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹è©³ç´°ä¿¡æ¯"""
        return self.model_info
    
    def _get_model_info(self) -> Dict[str, Any]:
        """è¨ˆç®—æ¨¡å‹ä¿¡æ¯"""
        param_info = self.get_total_parameters()
        
        return {
            'architecture': {
                'backbone': self.backbone_name,
                'neck': self.neck_type,
                'head': self.head_type
            },
            'task_config': {
                'detection_classes': self.num_det_classes,
                'segmentation_classes': self.num_seg_classes,
                'classification_classes': self.num_cls_classes
            },
            'parameters': param_info,
            'parameter_budget': {
                'total_budget': 8_000_000,
                'used_parameters': param_info['total_parameters'],
                'remaining_budget': 8_000_000 - param_info['total_parameters'],
                'utilization_rate': param_info['total_parameters'] / 8_000_000
            }
        }
    
    def inference_benchmark(self, 
                           input_size: Tuple[int, int] = (512, 512),
                           batch_size: int = 1,
                           num_warmup: int = 10,
                           num_runs: int = 50,
                           device: Optional[torch.device] = None) -> Dict[str, float]:
        """
        æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦
        
        Args:
            input_size: è¼¸å…¥åœ–åƒå°ºå¯¸
            batch_size: æ‰¹æ¬¡å¤§å°
            num_warmup: ç†±èº«æ¬¡æ•¸
            num_runs: æ¸¬è©¦æ¬¡æ•¸
            device: è¨ˆç®—è¨­å‚™
        
        Returns:
            benchmark_results: æ€§èƒ½æ¸¬è©¦çµæœ
        """
        if device is None:
            device = next(self.parameters()).device
        
        self.eval()
        
        # å‰µå»ºæ¸¬è©¦è¼¸å…¥
        test_input = torch.randn(batch_size, 3, input_size[0], input_size[1]).to(device)
        
        # ç†±èº«
        with torch.no_grad():
            for _ in range(num_warmup):
                _ = self(test_input, task_type='all')
        
        # æ€§èƒ½æ¸¬è©¦
        times = []
        memory_usage = []
        
        with torch.no_grad():
            for _ in range(num_runs):
                # è¨˜éŒ„èµ·å§‹è¨˜æ†¶é«”
                if device.type == 'cuda':
                    torch.cuda.reset_peak_memory_stats()
                    start_memory = torch.cuda.memory_allocated()
                
                # è¨ˆæ™‚
                start_time = time.time()
                outputs = self(test_input, task_type='all')
                
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                end_time = time.time()
                inference_time = (end_time - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
                times.append(inference_time)
                
                # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨
                if device.type == 'cuda':
                    peak_memory = torch.cuda.max_memory_allocated()
                    memory_usage.append((peak_memory - start_memory) / 1024**2)  # MB
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        import numpy as np
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        
        results = {
            'input_size': input_size,
            'batch_size': batch_size,
            'avg_inference_time_ms': avg_time,
            'std_inference_time_ms': std_time,
            'min_inference_time_ms': min_time,
            'max_inference_time_ms': max_time,
            'fps': 1000 / avg_time,
            'meets_latency_requirement': avg_time < 150.0  # < 150ms è¦æ±‚
        }
        
        if device.type == 'cuda' and memory_usage:
            results.update({
                'avg_memory_usage_mb': np.mean(memory_usage),
                'peak_memory_usage_mb': np.max(memory_usage)
            })
        
        return results
    
    def save_model(self, save_path: str, include_optimizer: bool = False, **kwargs):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_config': {
                'backbone_name': self.backbone_name,
                'neck_type': self.neck_type,
                'head_type': self.head_type,
                'num_det_classes': self.num_det_classes,
                'num_seg_classes': self.num_seg_classes,
                'num_cls_classes': self.num_cls_classes
            },
            'model_info': self.model_info,
            **kwargs
        }
        
        torch.save(save_dict, save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    @classmethod
    def load_model(cls, load_path: str, device: Optional[torch.device] = None):
        """åŠ è¼‰æ¨¡å‹"""
        checkpoint = torch.load(load_path, map_location=device)
        
        # é‡å»ºæ¨¡å‹
        model_config = checkpoint['model_config']
        model = cls(**model_config)
        
        # åŠ è¼‰æ¬Šé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        
        if device is not None:
            model = model.to(device)
        
        print(f"âœ… æ¨¡å‹å·²å¾ {load_path} åŠ è¼‰")
        return model, checkpoint.get('model_info', {})
    
    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹ç¸½çµ"""
        info = self.get_model_info()
        
        print("ğŸ—ï¸ çµ±ä¸€å¤šä»»å‹™å­¸ç¿’æ¨¡å‹ç¸½çµ")
        print("=" * 60)
        
        # æ¶æ§‹ä¿¡æ¯
        arch = info['architecture']
        print(f"ğŸ”§ æ¶æ§‹é…ç½®:")
        print(f"  éª¨å¹¹ç¶²è·¯: {arch['backbone']}")
        print(f"  é ¸éƒ¨ç¶²è·¯: {arch['neck']}")
        print(f"  é ­éƒ¨ç¶²è·¯: {arch['head']}")
        
        # ä»»å‹™é…ç½®
        task = info['task_config']
        print(f"\nğŸ¯ ä»»å‹™é…ç½®:")
        print(f"  æª¢æ¸¬é¡åˆ¥: {task['detection_classes']}")
        print(f"  åˆ†å‰²é¡åˆ¥: {task['segmentation_classes']}")
        print(f"  åˆ†é¡é¡åˆ¥: {task['classification_classes']}")
        
        # åƒæ•¸çµ±è¨ˆ
        params = info['parameters']
        budget = info['parameter_budget']
        
        print(f"\nğŸ“Š åƒæ•¸çµ±è¨ˆ:")
        print(f"  ç¸½åƒæ•¸é‡: {params['total_parameters']:,} ({params['total_parameters']/1e6:.2f}M)")
        print(f"  éª¨å¹¹ç¶²è·¯: {params['backbone_parameters']:,} ({params['backbone_parameters']/1e6:.2f}M)")
        print(f"  é ¸éƒ¨ç¶²è·¯: {params['neck_parameters']:,} ({params['neck_parameters']/1e6:.2f}M)")
        print(f"  é ­éƒ¨ç¶²è·¯: {params['head_parameters']:,} ({params['head_parameters']/1e6:.2f}M)")
        
        print(f"\nğŸ’° åƒæ•¸é ç®—:")
        print(f"  é ç®—é™åˆ¶: {budget['total_budget']:,} (8.0M)")
        print(f"  å·²ä½¿ç”¨: {budget['used_parameters']:,}")
        print(f"  å‰©é¤˜: {budget['remaining_budget']:,}")
        print(f"  ä½¿ç”¨ç‡: {budget['utilization_rate']:.1%}")
        
        # é ç®—æª¢æŸ¥
        if budget['used_parameters'] <= budget['total_budget']:
            print(f"  âœ… åƒæ•¸é‡ç¬¦åˆé ç®—é™åˆ¶")
        else:
            print(f"  âŒ åƒæ•¸é‡è¶…å‡ºé ç®—é™åˆ¶")


def create_unified_model(config_name: str = 'default', **kwargs) -> UnifiedMultiTaskModel:
    """
    å‰µå»ºçµ±ä¸€å¤šä»»å‹™æ¨¡å‹çš„å·¥å» å‡½æ•¸
    
    Args:
        config_name: é…ç½®åç¨±
        **kwargs: è¦†è“‹é…ç½®åƒæ•¸
    
    Returns:
        model: çµ±ä¸€å¤šä»»å‹™æ¨¡å‹å¯¦ä¾‹
    """
    # é å®šç¾©é…ç½®
    configs = {
        'default': {
            'backbone_name': 'mobilenetv3_small',
            'neck_type': 'fpn',
            'head_type': 'unified',
            'num_det_classes': 10,
            'num_seg_classes': 21,
            'num_cls_classes': 10,
            'pretrained': True,
            'neck_out_channels': 128,
            'head_shared_channels': 256
        },
        'lightweight': {
            'backbone_name': 'mobilenetv3_small',
            'neck_type': 'lightweight',
            'head_type': 'unified',
            'num_det_classes': 10,
            'num_seg_classes': 21,
            'num_cls_classes': 10,
            'pretrained': True,
            'neck_out_channels': 64,
            'head_shared_channels': 128
        },
        'high_performance': {
            'backbone_name': 'mobilenetv3_small',
            'neck_type': 'fpn',
            'head_type': 'unified',
            'num_det_classes': 10,
            'num_seg_classes': 21,
            'num_cls_classes': 10,
            'pretrained': True,
            'neck_out_channels': 256,
            'head_shared_channels': 512
        }
    }
    
    if config_name not in configs:
        raise ValueError(f"Unknown config: {config_name}. Available: {list(configs.keys())}")
    
    config = configs[config_name].copy()
    config.update(kwargs)
    
    return UnifiedMultiTaskModel(**config)


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("ğŸš€ æ¸¬è©¦çµ±ä¸€å¤šä»»å‹™å­¸ç¿’æ¨¡å‹")
    print("=" * 60)
    
    # å‰µå»ºæ¨¡å‹
    model = create_unified_model('default')
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ç¸½çµ
    model.print_model_summary()
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    print("\nğŸ§ª æ¸¬è©¦å‰å‘å‚³æ’­...")
    batch_size = 2
    test_input = torch.randn(batch_size, 3, 512, 512).to(device)
    
    with torch.no_grad():
        outputs = model(test_input, task_type='all')
    
    print("âœ… å‰å‘å‚³æ’­æ¸¬è©¦æˆåŠŸï¼")
    print("\nğŸ” è¼¸å‡ºå½¢ç‹€:")
    for task, output in outputs.items():
        print(f"  {task}: {output.shape}")
    
    # æ€§èƒ½åŸºæº–æ¸¬è©¦
    print("\nâ±ï¸ åŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦...")
    benchmark_results = model.inference_benchmark(
        input_size=(512, 512),
        batch_size=1,
        num_warmup=5,
        num_runs=20
    )
    
    print("ğŸ“ˆ æ€§èƒ½æ¸¬è©¦çµæœ:")
    for key, value in benchmark_results.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    print(f"\nğŸ‰ æ¨¡å‹æ¸¬è©¦å®Œæˆï¼")
    if benchmark_results['meets_latency_requirement']:
        print("âœ… æ»¿è¶³æ¨ç†é€Ÿåº¦è¦æ±‚ (<150ms)")
    else:
        print("âŒ ä¸æ»¿è¶³æ¨ç†é€Ÿåº¦è¦æ±‚ (<150ms)")
    
    # æª¢æŸ¥åƒæ•¸é ç®—
    info = model.get_model_info()
    budget = info['parameter_budget']
    if budget['used_parameters'] <= budget['total_budget']:
        print("âœ… åƒæ•¸é‡ç¬¦åˆé ç®—é™åˆ¶")
    else:
        print("âŒ åƒæ•¸é‡è¶…å‡ºé ç®—é™åˆ¶")