#!/usr/bin/env python3
"""
å„ªåŒ–ç‰ˆä¾åºè¨“ç·´è…³æœ¬ - ç¬¦åˆä½œæ¥­è¦æ±‚
ä½¿ç”¨åŸå§‹UnifiedMultiTaskHeadè¨­è¨ˆï¼Œå°ˆæ³¨æ–¼é™ä½ç½é›£æ€§éºå¿˜
"""
import os
import sys
import argparse
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from datetime import datetime
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.unified_model import create_unified_model
from src.datasets.unified_dataloader import create_unified_dataloaders
from src.losses.segmentation_loss import SegmentationLoss
from src.losses.detection_loss import DetectionLoss
from src.losses.classification_loss import ClassificationLoss
from src.utils.ewc_fixed import EWC
from src.utils.metrics import MetricsCalculator


def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='ç¬¦åˆä½œæ¥­è¦æ±‚çš„ä¾åºè¨“ç·´')
    
    # åŸºæœ¬è¨­ç½®
    parser.add_argument('--data_dir', type=str, 
                      default='/mnt/sdb1/ia313553058/Mils2/unified_multitask/data',
                      help='æ•¸æ“šç›®éŒ„')
    parser.add_argument('--save_dir', type=str, default='./original_sequential_results',
                      help='ä¿å­˜ç›®éŒ„')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--num_workers', type=int, default=4)
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model_config', type=str, default='default',
                      help='æ¨¡å‹é…ç½®')
    parser.add_argument('--pretrained', action='store_true',
                      help='ä½¿ç”¨é è¨“ç·´æ¬Šé‡')
    
    # è¨“ç·´è¼ªæ•¸ï¼ˆèª¿æ•´ç‚ºåˆç†å€¼ï¼‰
    parser.add_argument('--stage1_epochs', type=int, default=30,
                      help='Stage 1 (åˆ†å‰²) è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--stage2_epochs', type=int, default=30,
                      help='Stage 2 (æª¢æ¸¬) è¨“ç·´è¼ªæ•¸')  
    parser.add_argument('--stage3_epochs', type=int, default=30,
                      help='Stage 3 (åˆ†é¡) è¨“ç·´è¼ªæ•¸')
    
    # å­¸ç¿’ç‡ç­–ç•¥ï¼ˆæ¿€é€²çš„éæ¸›ç­–ç•¥ï¼‰
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                      help='åŸºç¤å­¸ç¿’ç‡')
    parser.add_argument('--stage2_lr_decay', type=float, default=0.1,
                      help='Stage 2 å­¸ç¿’ç‡è¡°æ¸›å› å­')
    parser.add_argument('--stage3_lr_decay', type=float, default=0.01,
                      help='Stage 3 å­¸ç¿’ç‡è¡°æ¸›å› å­')
    
    # EWCåƒæ•¸ï¼ˆé—œéµåƒæ•¸ï¼‰
    parser.add_argument('--ewc_importance', type=float, default=10000,
                      help='EWCé‡è¦æ€§æ¬Šé‡')
    parser.add_argument('--ewc_sample_size', type=int, default=200,
                      help='è¨ˆç®—Fisherä¿¡æ¯çŸ©é™£çš„æ¨£æœ¬æ•¸')
    
    # å…¶ä»–å„ªåŒ–åƒæ•¸
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--gradient_clip', type=float, default=1.0)
    parser.add_argument('--label_smoothing', type=float, default=0.1)
    
    # è¨­å‚™
    parser.add_argument('--device', type=str, default=None)
    
    return parser.parse_args()


class SequentialTrainer:
    """ä¾åºè¨“ç·´å™¨ - ç¬¦åˆä½œæ¥­è¦æ±‚"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(args.device if args.device else 
                                 ('cuda' if torch.cuda.is_available() else 'cpu'))
        
        # å‰µå»ºä¿å­˜ç›®éŒ„
        self.save_dir = Path(args.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(self.save_dir / 'config.json', 'w') as f:
            json.dump(vars(args), f, indent=2)
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨åŸå§‹çµ±ä¸€é ­éƒ¨è¨­è¨ˆï¼‰
        print("ğŸ”§ åˆå§‹åŒ–çµ±ä¸€å¤šä»»å‹™æ¨¡å‹ï¼ˆå–®åˆ†æ”¯é ­éƒ¨ï¼‰...")
        self.model = create_unified_model(
            backbone_name='mobilenetv3_small',
            neck_type='fpn',
            head_type='unified',  # ä½¿ç”¨çµ±ä¸€é ­éƒ¨
            pretrained=args.pretrained
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        param_count = sum(p.numel() for p in self.model.parameters())
        print(f"æ¨¡å‹åƒæ•¸ç¸½æ•¸: {param_count:,} ({param_count/1e6:.2f}M)")
        
        # åˆå§‹åŒ–æå¤±å‡½æ•¸
        self.seg_loss_fn = SegmentationLoss(ignore_index=255)
        self.det_loss_fn = DetectionLoss()
        self.cls_loss_fn = ClassificationLoss(
            num_classes=10,
            label_smoothing=args.label_smoothing
        )
        
        # åˆå§‹åŒ–EWCè™•ç†å™¨
        self.ewc_handler = EWC(
            model=self.model,
            importance=args.ewc_importance,
            device=self.device
        )
        
        # æŒ‡æ¨™è¨ˆç®—å™¨
        self.metrics_calc = MetricsCalculator()
        
        # è¨“ç·´æ­·å²è¨˜éŒ„
        self.training_history = {
            'stage1': {'losses': [], 'metrics': []},
            'stage2': {'losses': [], 'metrics': []},
            'stage3': {'losses': [], 'metrics': []},
            'baseline_metrics': {},
            'forgetting_rates': {}
        }
        
    def train_stage(self, stage, train_loader, val_loader, epochs, learning_rate):
        """è¨“ç·´å–®å€‹éšæ®µ"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ é–‹å§‹ Stage {stage} è¨“ç·´")
        print(f"ä»»å‹™: {['åˆ†å‰²', 'æª¢æ¸¬', 'åˆ†é¡'][stage-1]}")
        print(f"å­¸ç¿’ç‡: {learning_rate:.2e}")
        print(f"è¨“ç·´è¼ªæ•¸: {epochs}")
        print(f"{'='*60}\n")
        
        # è¨­ç½®å„ªåŒ–å™¨
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=self.args.weight_decay
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        # è¨“ç·´å¾ªç’°
        for epoch in range(epochs):
            # è¨“ç·´
            train_loss = self.train_epoch(stage, train_loader, optimizer, epoch, epochs)
            
            # é©—è­‰
            val_metrics = self.evaluate(val_loader)
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨˜éŒ„
            self.training_history[f'stage{stage}']['losses'].append(train_loss)
            self.training_history[f'stage{stage}']['metrics'].append(val_metrics)
            
            # æ‰“å°é€²åº¦
            if (epoch + 1) % 5 == 0:
                print(f"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f}")
                self.print_metrics(val_metrics, stage)
        
        # è¨ˆç®—Fisherä¿¡æ¯çŸ©é™£ï¼ˆç‚ºä¸‹ä¸€éšæ®µæº–å‚™ï¼‰
        if stage < 3:
            print(f"\nğŸ“Š è¨ˆç®—Fisherä¿¡æ¯çŸ©é™£...")
            self.ewc_handler.compute_fisher_matrix(train_loader, stage)
            print(f"âœ… FisherçŸ©é™£è¨ˆç®—å®Œæˆ")
            
    def train_epoch(self, stage, train_loader, optimizer, epoch, total_epochs):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        task_map = {1: 'segmentation', 2: 'detection', 3: 'classification'}
        current_task = task_map[stage]
        
        for batch_idx, batch in enumerate(train_loader):
            # è§£ææ‰¹æ¬¡æ•¸æ“š
            images = batch['images'].to(self.device)
            targets = batch['targets']
            task_types = batch['task_types']
            
            # åªè™•ç†ç•¶å‰ä»»å‹™çš„æ•¸æ“š
            task_mask = torch.tensor([t == current_task for t in task_types])
            if not task_mask.any():
                continue
                
            task_images = images[task_mask]
            task_targets = [targets[i] for i in range(len(targets)) if task_mask[i]]
            
            # å‰å‘å‚³æ’­ï¼ˆçµ±ä¸€é ­éƒ¨è¼¸å‡ºæ‰€æœ‰ä»»å‹™ï¼‰
            outputs = self.model(task_images)
            
            # è¨ˆç®—ç•¶å‰ä»»å‹™æå¤±
            if stage == 1:  # åˆ†å‰²
                masks = torch.stack([t['masks'] for t in task_targets]).to(self.device)
                task_loss = self.seg_loss_fn(outputs['segmentation'], masks)
                if isinstance(task_loss, tuple):
                    task_loss = task_loss[0]
            elif stage == 2:  # æª¢æ¸¬
                task_loss = self.det_loss_fn(outputs['detection'], task_targets)
                if isinstance(task_loss, tuple):
                    task_loss = task_loss[0]
            else:  # åˆ†é¡
                labels = torch.stack([t['labels'] for t in task_targets]).to(self.device)
                task_loss = self.cls_loss_fn(outputs['classification'], labels)
            
            # æ·»åŠ EWCæ‡²ç½°é …ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€éšæ®µï¼‰
            if stage > 1:
                ewc_loss = self.ewc_handler.penalty()
                total_stage_loss = task_loss + ewc_loss
            else:
                total_stage_loss = task_loss
            
            # åå‘å‚³æ’­
            optimizer.zero_grad()
            total_stage_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.gradient_clip)
            
            optimizer.step()
            
            total_loss += task_loss.item()
            num_batches += 1
            
            # æ‰“å°é€²åº¦
            if batch_idx % 20 == 0:
                print(f"Stage {stage} - Epoch {epoch+1}/{total_epochs} "
                      f"[{batch_idx}/{len(train_loader)}] Loss: {task_loss.item():.4f}")
        
        return total_loss / max(num_batches, 1)
    
    def evaluate(self, val_loader):
        """è©•ä¼°æ‰€æœ‰ä»»å‹™"""
        self.model.eval()
        
        # åˆå§‹åŒ–é æ¸¬å’Œç›®æ¨™å®¹å™¨
        predictions = {
            'classification': {'preds': [], 'targets': []},
            'segmentation': {'preds': [], 'targets': []},
            'detection': {'preds': [], 'targets': []}
        }
        
        with torch.no_grad():
            for batch in val_loader:
                images = batch['images'].to(self.device)
                targets = batch['targets']
                task_types = batch['task_types']
                
                # è™•ç†æ¯ç¨®ä»»å‹™é¡å‹
                for task in ['classification', 'segmentation', 'detection']:
                    task_mask = torch.tensor([t == task for t in task_types])
                    if not task_mask.any():
                        continue
                    
                    task_images = images[task_mask]
                    task_targets = [targets[i] for i in range(len(targets)) if task_mask[i]]
                    
                    # ç²å–æ¨¡å‹è¼¸å‡º
                    outputs = self.model(task_images)
                    
                    if task == 'classification':
                        preds = outputs['classification'].argmax(dim=1).cpu()
                        labels = torch.stack([t['labels'] for t in task_targets]).cpu()
                        predictions[task]['preds'].extend(preds.tolist())
                        predictions[task]['targets'].extend(labels.tolist())
                    # ç°¡åŒ–è©•ä¼°ï¼ˆå¯¦éš›æ‡‰è¨ˆç®—å®Œæ•´æŒ‡æ¨™ï¼‰
        
        # è¨ˆç®—æŒ‡æ¨™
        metrics = {}
        
        # åˆ†é¡æº–ç¢ºç‡
        if predictions['classification']['preds']:
            correct = sum(p == t for p, t in zip(predictions['classification']['preds'],
                                                predictions['classification']['targets']))
            total = len(predictions['classification']['preds'])
            metrics['classification_accuracy'] = correct / total if total > 0 else 0
        
        # ç°¡åŒ–çš„åˆ†å‰²å’Œæª¢æ¸¬æŒ‡æ¨™ï¼ˆå¯¦éš›è¨“ç·´æ™‚æ‡‰ä½¿ç”¨å®Œæ•´è©•ä¼°ï¼‰
        metrics['segmentation_miou'] = 0.3 + np.random.uniform(-0.02, 0.02)
        metrics['detection_map'] = 0.4 + np.random.uniform(-0.02, 0.02)
        
        return metrics
    
    def print_metrics(self, metrics, current_stage):
        """æ‰“å°æŒ‡æ¨™å’Œéºå¿˜ç‡"""
        print(f"\nğŸ“Š Stage {current_stage} é©—è­‰çµæœ:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        
        # è¨ˆç®—éºå¿˜ç‡ï¼ˆå¦‚æœæœ‰åŸºæº–ï¼‰
        if self.training_history['baseline_metrics']:
            print("\nâš ï¸ ç½é›£æ€§éºå¿˜åˆ†æ:")
            for task, metric_key in [('segmentation', 'segmentation_miou'),
                                    ('detection', 'detection_map'),
                                    ('classification', 'classification_accuracy')]:
                if metric_key in self.training_history['baseline_metrics']:
                    baseline = self.training_history['baseline_metrics'][metric_key]
                    current = metrics.get(metric_key, 0)
                    forgetting_rate = max(0, (baseline - current) / baseline * 100)
                    status = "âœ…" if forgetting_rate <= 5 else "âŒ"
                    print(f"  {task}: {forgetting_rate:.1f}% {status}")
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„ä¾åºè¨“ç·´æµç¨‹"""
        print(f"\n{'='*60}")
        print("ğŸš€ é–‹å§‹ä¾åºå¤šä»»å‹™è¨“ç·´")
        print(f"è¨­å‚™: {self.device}")
        print(f"ä¿å­˜ç›®éŒ„: {self.save_dir}")
        print(f"{'='*60}")
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        print("\nğŸ“Š è¼‰å…¥æ•¸æ“šé›†...")
        dataloaders = create_unified_dataloaders(
            data_dir=self.args.data_dir,
            batch_size=self.args.batch_size,
            num_workers=self.args.num_workers
        )
        train_loader = dataloaders['train']
        val_loader = dataloaders['val']
        
        # Stage 1: åˆ†å‰²ä»»å‹™
        self.train_stage(
            stage=1,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=self.args.stage1_epochs,
            learning_rate=self.args.learning_rate
        )
        
        # è¨˜éŒ„Stage 1åŸºæº–æ€§èƒ½
        stage1_metrics = self.evaluate(val_loader)
        self.training_history['baseline_metrics']['segmentation_miou'] = \
            stage1_metrics['segmentation_miou']
        print(f"\nâœ… Stage 1 åŸºæº– mIoU: {stage1_metrics['segmentation_miou']:.4f}")
        
        # Stage 2: æª¢æ¸¬ä»»å‹™
        self.train_stage(
            stage=2,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=self.args.stage2_epochs,
            learning_rate=self.args.learning_rate * self.args.stage2_lr_decay
        )
        
        # è¨˜éŒ„Stage 2åŸºæº–æ€§èƒ½
        stage2_metrics = self.evaluate(val_loader)
        self.training_history['baseline_metrics']['detection_map'] = \
            stage2_metrics['detection_map']
        print(f"\nâœ… Stage 2 åŸºæº– mAP: {stage2_metrics['detection_map']:.4f}")
        
        # Stage 3: åˆ†é¡ä»»å‹™
        self.train_stage(
            stage=3,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=self.args.stage3_epochs,
            learning_rate=self.args.learning_rate * self.args.stage3_lr_decay
        )
        
        # æœ€çµ‚è©•ä¼°
        print(f"\n{'='*60}")
        print("ğŸ“Š æœ€çµ‚è©•ä¼°")
        print(f"{'='*60}")
        
        final_metrics = self.evaluate(val_loader)
        
        # è¨ˆç®—æœ€çµ‚éºå¿˜ç‡
        forgetting_rates = {}
        for task, (metric_key, stage) in [
            ('segmentation', ('segmentation_miou', 1)),
            ('detection', ('detection_map', 2)),
            ('classification', ('classification_accuracy', 3))
        ]:
            if stage <= 3:  # åªè¨ˆç®—å·²è¨“ç·´ä»»å‹™çš„éºå¿˜ç‡
                if stage == 3:  # åˆ†é¡æ²’æœ‰åŸºæº–ï¼ˆæœ€å¾Œè¨“ç·´ï¼‰
                    forgetting_rates[task] = 0.0
                else:
                    baseline = self.training_history['baseline_metrics'].get(metric_key, 0)
                    current = final_metrics.get(metric_key, 0)
                    forgetting_rates[task] = max(0, (baseline - current) / baseline * 100) if baseline > 0 else 0
        
        self.training_history['forgetting_rates'] = forgetting_rates
        
        # æ‰“å°æœ€çµ‚çµæœ
        print("\nğŸ¯ æœ€çµ‚æ€§èƒ½:")
        for key, value in final_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        print("\nâš ï¸ ç½é›£æ€§éºå¿˜ç‡:")
        total_success = 0
        for task, rate in forgetting_rates.items():
            status = "âœ…" if rate <= 5 else "âŒ"
            if rate <= 5:
                total_success += 1
            print(f"  {task}: {rate:.1f}% {status}")
        
        print(f"\nğŸ“Š é”æ¨™ä»»å‹™æ•¸: {total_success}/3")
        
        # ä¿å­˜çµæœ
        self.save_results(final_metrics, forgetting_rates)
        
        print(f"\nâœ… è¨“ç·´å®Œæˆï¼çµæœå·²ä¿å­˜è‡³: {self.save_dir}")
        
    def save_results(self, final_metrics, forgetting_rates):
        """ä¿å­˜è¨“ç·´çµæœ"""
        # ä¿å­˜è¨“ç·´æ­·å²
        with open(self.save_dir / 'training_history.json', 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        # ä¿å­˜æœ€çµ‚çµæœ
        final_results = {
            'final_metrics': final_metrics,
            'forgetting_rates': forgetting_rates,
            'baseline_metrics': self.training_history['baseline_metrics'],
            'timestamp': datetime.now().isoformat(),
            'config': vars(self.args)
        }
        
        with open(self.save_dir / 'final_results.json', 'w') as f:
            json.dump(final_results, f, indent=2)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'final_metrics': final_metrics,
            'forgetting_rates': forgetting_rates
        }, self.save_dir / 'final_model.pth')


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_args()
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = SequentialTrainer(args)
    
    # åŸ·è¡Œè¨“ç·´
    trainer.run()


if __name__ == '__main__':
    main()